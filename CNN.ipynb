{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# above cells did not work for me so I used that to parse the data and added the gz file to the git repo\n",
    "\n",
    "# parse data downloaded from http://jmcauley.ucsd.edu/data/amazon/\n",
    "\n",
    "import pandas as pd \n",
    "import gzip \n",
    "\n",
    "def parse(path): \n",
    "    g = gzip.open(path, 'rb') \n",
    "    for l in g: \n",
    "        yield eval(l) \n",
    "        \n",
    "def getDF(path): \n",
    "    i = 0 \n",
    "    df = {}\n",
    "    for d in parse(path): \n",
    "        df[i] = d \n",
    "        i += 1 \n",
    "    return pd.DataFrame.from_dict(df, orient='index') \n",
    "\n",
    "df = getDF('reviews_Cell_Phones_and_Accessories_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "def prepare_df(df):\n",
    "    df_pos = df[df['overall'] == 5]\n",
    "    df_neg = df[df['overall'] == 1]\n",
    "    df_pos = [x.split(\".\") for x in df_pos['reviewText']]\n",
    "    pos_text = []\n",
    "    for x in df_pos:\n",
    "        pos_text += [s for s in x]\n",
    "    df_neg = [x.split(\".\") for x in df_neg['reviewText']]\n",
    "    neg_text = []\n",
    "    for x in df_neg:\n",
    "        neg_text += [s for s in x]\n",
    "    return (pos_text, neg_text)\n",
    "\n",
    "train_pos, train_neg = prepare_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_pos_sample = np.random.choice(train_pos, size = 5000)\n",
    "train_neg_sample = np.random.choice(train_neg, size = 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The mic is great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                               text\n",
       "0       0  So there is no way for me to plug it in here i...\n",
       "1       1                        Good case, Excellent value.\n",
       "2       1                             Great for the jawbone.\n",
       "3       0  Tied to charger for conversations lasting more...\n",
       "4       1                                  The mic is great."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = open(\"amazon_cells_labelled.txt\", 'r')\n",
    "text_file = text_file.read()\n",
    "text_file = re.split(\"\\t|\\n\", text_file)\n",
    "text_file = text_file[:len(text_file)-1]\n",
    "labeled = pd.DataFrame({\"text\":text_file[:len(text_file):2], \"rating\":[int(x) for x in text_file[1:len(text_file):2]]})\n",
    "labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pos = labeled[labeled[\"rating\"] == 1]['text']\n",
    "test_neg = labeled[labeled[\"rating\"] == 0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(positive_data_file)#list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(negative_data_file)#list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 7471\n",
      "Train/Dev split: 9000/1000\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n",
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n",
      "Writing to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\n",
      "\n",
      "2017-03-19T17:14:52.705611: step 1, loss 3.34634, acc 0.46875\n",
      "2017-03-19T17:14:53.508601: step 2, loss 1.94474, acc 0.53125\n",
      "2017-03-19T17:14:54.307689: step 3, loss 1.50472, acc 0.5625\n",
      "2017-03-19T17:14:55.094205: step 4, loss 1.68381, acc 0.4375\n",
      "2017-03-19T17:14:55.887271: step 5, loss 1.6262, acc 0.453125\n",
      "2017-03-19T17:14:56.672775: step 6, loss 1.70062, acc 0.546875\n",
      "2017-03-19T17:14:57.452331: step 7, loss 1.66072, acc 0.4375\n",
      "2017-03-19T17:14:58.256303: step 8, loss 1.67919, acc 0.484375\n",
      "2017-03-19T17:14:59.076339: step 9, loss 1.58823, acc 0.53125\n",
      "2017-03-19T17:15:00.061593: step 10, loss 1.59691, acc 0.46875\n",
      "2017-03-19T17:15:00.912148: step 11, loss 1.64049, acc 0.484375\n",
      "2017-03-19T17:15:01.746190: step 12, loss 1.36944, acc 0.546875\n",
      "2017-03-19T17:15:02.590742: step 13, loss 1.50707, acc 0.484375\n",
      "2017-03-19T17:15:03.457839: step 14, loss 2.15551, acc 0.421875\n",
      "2017-03-19T17:15:04.285878: step 15, loss 1.87871, acc 0.46875\n",
      "2017-03-19T17:15:05.130067: step 16, loss 1.31307, acc 0.53125\n",
      "2017-03-19T17:15:05.939592: step 17, loss 1.61205, acc 0.53125\n",
      "2017-03-19T17:15:06.793648: step 18, loss 1.30512, acc 0.578125\n",
      "2017-03-19T17:15:07.772278: step 19, loss 1.9541, acc 0.453125\n",
      "2017-03-19T17:15:08.579300: step 20, loss 1.99634, acc 0.390625\n",
      "2017-03-19T17:15:09.392777: step 21, loss 1.66661, acc 0.484375\n",
      "2017-03-19T17:15:10.246283: step 22, loss 2.1747, acc 0.34375\n",
      "2017-03-19T17:15:11.109823: step 23, loss 1.85017, acc 0.390625\n",
      "2017-03-19T17:15:12.060501: step 24, loss 1.72011, acc 0.421875\n",
      "2017-03-19T17:15:12.896567: step 25, loss 1.33314, acc 0.53125\n",
      "2017-03-19T17:15:13.732111: step 26, loss 1.72848, acc 0.390625\n",
      "2017-03-19T17:15:14.553475: step 27, loss 1.88018, acc 0.421875\n",
      "2017-03-19T17:15:15.395538: step 28, loss 1.38616, acc 0.5625\n",
      "2017-03-19T17:15:16.224577: step 29, loss 1.2635, acc 0.515625\n",
      "2017-03-19T17:15:17.255733: step 30, loss 1.30905, acc 0.46875\n",
      "2017-03-19T17:15:18.156803: step 31, loss 1.93186, acc 0.546875\n",
      "2017-03-19T17:15:19.036378: step 32, loss 1.11683, acc 0.65625\n",
      "2017-03-19T17:15:19.907897: step 33, loss 1.50516, acc 0.46875\n",
      "2017-03-19T17:15:20.764906: step 34, loss 1.9054, acc 0.375\n",
      "2017-03-19T17:15:21.765198: step 35, loss 1.43996, acc 0.46875\n",
      "2017-03-19T17:15:22.572168: step 36, loss 1.34298, acc 0.546875\n",
      "2017-03-19T17:15:23.382746: step 37, loss 2.20466, acc 0.453125\n",
      "2017-03-19T17:15:24.223345: step 38, loss 1.09869, acc 0.59375\n",
      "2017-03-19T17:15:25.065405: step 39, loss 1.2286, acc 0.546875\n",
      "2017-03-19T17:15:25.898950: step 40, loss 1.08794, acc 0.578125\n",
      "2017-03-19T17:15:26.723986: step 41, loss 1.59932, acc 0.453125\n",
      "2017-03-19T17:15:27.543072: step 42, loss 1.37471, acc 0.515625\n",
      "2017-03-19T17:15:28.372661: step 43, loss 1.55347, acc 0.5625\n",
      "2017-03-19T17:15:29.198883: step 44, loss 1.27221, acc 0.515625\n",
      "2017-03-19T17:15:30.039930: step 45, loss 1.4517, acc 0.46875\n",
      "2017-03-19T17:15:30.998807: step 46, loss 1.43168, acc 0.5625\n",
      "2017-03-19T17:15:31.855898: step 47, loss 1.44275, acc 0.5625\n",
      "2017-03-19T17:15:32.701000: step 48, loss 1.49563, acc 0.53125\n",
      "2017-03-19T17:15:33.538992: step 49, loss 1.40435, acc 0.515625\n",
      "2017-03-19T17:15:34.369485: step 50, loss 1.29212, acc 0.4375\n",
      "2017-03-19T17:15:35.187008: step 51, loss 1.38008, acc 0.578125\n",
      "2017-03-19T17:15:36.004539: step 52, loss 1.42018, acc 0.515625\n",
      "2017-03-19T17:15:36.899072: step 53, loss 0.932825, acc 0.609375\n",
      "2017-03-19T17:15:37.729611: step 54, loss 1.73751, acc 0.453125\n",
      "2017-03-19T17:15:38.556770: step 55, loss 1.37366, acc 0.53125\n",
      "2017-03-19T17:15:39.401269: step 56, loss 1.50125, acc 0.4375\n",
      "2017-03-19T17:15:40.239817: step 57, loss 1.51703, acc 0.546875\n",
      "2017-03-19T17:15:41.079311: step 58, loss 0.980323, acc 0.625\n",
      "2017-03-19T17:15:41.945759: step 59, loss 1.63455, acc 0.4375\n",
      "2017-03-19T17:15:42.763247: step 60, loss 1.27052, acc 0.640625\n",
      "2017-03-19T17:15:43.593337: step 61, loss 1.16669, acc 0.59375\n",
      "2017-03-19T17:15:44.415370: step 62, loss 1.45084, acc 0.40625\n",
      "2017-03-19T17:15:45.234954: step 63, loss 1.40714, acc 0.46875\n",
      "2017-03-19T17:15:46.062939: step 64, loss 1.59789, acc 0.484375\n",
      "2017-03-19T17:15:46.896541: step 65, loss 1.27827, acc 0.5625\n",
      "2017-03-19T17:15:47.857726: step 66, loss 1.27982, acc 0.515625\n",
      "2017-03-19T17:15:48.693421: step 67, loss 1.49748, acc 0.453125\n",
      "2017-03-19T17:15:49.722165: step 68, loss 1.27398, acc 0.578125\n",
      "2017-03-19T17:15:50.566715: step 69, loss 1.48185, acc 0.46875\n",
      "2017-03-19T17:15:51.519930: step 70, loss 0.909685, acc 0.65625\n",
      "2017-03-19T17:15:52.568678: step 71, loss 1.12869, acc 0.640625\n",
      "2017-03-19T17:15:53.612735: step 72, loss 1.22425, acc 0.515625\n",
      "2017-03-19T17:15:54.601856: step 73, loss 1.59493, acc 0.421875\n",
      "2017-03-19T17:15:55.575550: step 74, loss 1.33737, acc 0.46875\n",
      "2017-03-19T17:15:56.550641: step 75, loss 1.15175, acc 0.5\n",
      "2017-03-19T17:15:57.490735: step 76, loss 1.37291, acc 0.5625\n",
      "2017-03-19T17:15:58.499900: step 77, loss 0.914142, acc 0.65625\n",
      "2017-03-19T17:15:59.457982: step 78, loss 1.45061, acc 0.40625\n",
      "2017-03-19T17:16:00.437179: step 79, loss 1.31164, acc 0.5625\n",
      "2017-03-19T17:16:01.320342: step 80, loss 1.04001, acc 0.640625\n",
      "2017-03-19T17:16:02.231464: step 81, loss 1.0861, acc 0.59375\n",
      "2017-03-19T17:16:03.293201: step 82, loss 1.29681, acc 0.484375\n",
      "2017-03-19T17:16:04.265214: step 83, loss 1.09684, acc 0.578125\n",
      "2017-03-19T17:16:05.223910: step 84, loss 1.51522, acc 0.46875\n",
      "2017-03-19T17:16:06.202025: step 85, loss 1.31525, acc 0.5625\n",
      "2017-03-19T17:16:07.150699: step 86, loss 1.27616, acc 0.5\n",
      "2017-03-19T17:16:08.060826: step 87, loss 0.968635, acc 0.609375\n",
      "2017-03-19T17:16:09.012003: step 88, loss 1.26932, acc 0.421875\n",
      "2017-03-19T17:16:09.850121: step 89, loss 1.32567, acc 0.484375\n",
      "2017-03-19T17:16:10.801104: step 90, loss 1.45856, acc 0.515625\n",
      "2017-03-19T17:16:11.791663: step 91, loss 1.68571, acc 0.359375\n",
      "2017-03-19T17:16:12.730831: step 92, loss 0.953249, acc 0.625\n",
      "2017-03-19T17:16:13.558345: step 93, loss 0.930473, acc 0.59375\n",
      "2017-03-19T17:16:14.555555: step 94, loss 1.27089, acc 0.5\n",
      "2017-03-19T17:16:15.562167: step 95, loss 1.37729, acc 0.546875\n",
      "2017-03-19T17:16:16.561108: step 96, loss 1.01305, acc 0.5625\n",
      "2017-03-19T17:16:17.531561: step 97, loss 1.0083, acc 0.515625\n",
      "2017-03-19T17:16:18.535275: step 98, loss 1.21625, acc 0.5625\n",
      "2017-03-19T17:16:19.549472: step 99, loss 1.07911, acc 0.515625\n",
      "2017-03-19T17:16:20.520059: step 100, loss 1.20078, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:16:24.895145: step 100, loss 0.615072, acc 0.672\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-100\n",
      "\n",
      "2017-03-19T17:16:27.029670: step 101, loss 1.28727, acc 0.640625\n",
      "2017-03-19T17:16:27.854872: step 102, loss 1.22473, acc 0.5625\n",
      "2017-03-19T17:16:28.788267: step 103, loss 0.974025, acc 0.65625\n",
      "2017-03-19T17:16:29.585240: step 104, loss 1.28671, acc 0.53125\n",
      "2017-03-19T17:16:30.370043: step 105, loss 1.36969, acc 0.53125\n",
      "2017-03-19T17:16:31.164560: step 106, loss 0.860918, acc 0.65625\n",
      "2017-03-19T17:16:31.946609: step 107, loss 1.07221, acc 0.5\n",
      "2017-03-19T17:16:32.726164: step 108, loss 1.00978, acc 0.546875\n",
      "2017-03-19T17:16:33.509638: step 109, loss 1.10067, acc 0.546875\n",
      "2017-03-19T17:16:34.314612: step 110, loss 0.942992, acc 0.546875\n",
      "2017-03-19T17:16:35.315826: step 111, loss 1.07762, acc 0.53125\n",
      "2017-03-19T17:16:36.265366: step 112, loss 1.19992, acc 0.546875\n",
      "2017-03-19T17:16:37.283540: step 113, loss 1.03693, acc 0.609375\n",
      "2017-03-19T17:16:38.292258: step 114, loss 1.04911, acc 0.5625\n",
      "2017-03-19T17:16:39.152858: step 115, loss 0.883923, acc 0.609375\n",
      "2017-03-19T17:16:39.993404: step 116, loss 0.94153, acc 0.53125\n",
      "2017-03-19T17:16:40.818169: step 117, loss 0.910362, acc 0.671875\n",
      "2017-03-19T17:16:41.670776: step 118, loss 0.893411, acc 0.53125\n",
      "2017-03-19T17:16:42.510269: step 119, loss 1.15812, acc 0.5625\n",
      "2017-03-19T17:16:43.333751: step 120, loss 0.908525, acc 0.5625\n",
      "2017-03-19T17:16:44.139784: step 121, loss 1.37264, acc 0.390625\n",
      "2017-03-19T17:16:45.002429: step 122, loss 1.01052, acc 0.53125\n",
      "2017-03-19T17:16:45.836986: step 123, loss 0.877771, acc 0.515625\n",
      "2017-03-19T17:16:46.673758: step 124, loss 1.30134, acc 0.46875\n",
      "2017-03-19T17:16:47.512773: step 125, loss 1.20172, acc 0.453125\n",
      "2017-03-19T17:16:48.527649: step 126, loss 1.29249, acc 0.453125\n",
      "2017-03-19T17:16:49.558284: step 127, loss 1.00766, acc 0.5625\n",
      "2017-03-19T17:16:50.630943: step 128, loss 1.06172, acc 0.5625\n",
      "2017-03-19T17:16:51.639152: step 129, loss 1.08705, acc 0.546875\n",
      "2017-03-19T17:16:52.619771: step 130, loss 1.12118, acc 0.59375\n",
      "2017-03-19T17:16:53.585018: step 131, loss 0.842229, acc 0.65625\n",
      "2017-03-19T17:16:54.591406: step 132, loss 1.08449, acc 0.5\n",
      "2017-03-19T17:16:55.435640: step 133, loss 1.09268, acc 0.484375\n",
      "2017-03-19T17:16:56.249856: step 134, loss 1.01661, acc 0.59375\n",
      "2017-03-19T17:16:57.070005: step 135, loss 1.31642, acc 0.46875\n",
      "2017-03-19T17:16:58.039144: step 136, loss 1.52684, acc 0.375\n",
      "2017-03-19T17:16:58.887297: step 137, loss 1.29627, acc 0.5\n",
      "2017-03-19T17:16:59.731732: step 138, loss 1.36881, acc 0.484375\n",
      "2017-03-19T17:17:00.651364: step 139, loss 0.936035, acc 0.609375\n",
      "2017-03-19T17:17:01.541008: step 140, loss 0.989897, acc 0.546875\n",
      "2017-03-19T17:17:02.134691: step 141, loss 0.875111, acc 0.625\n",
      "2017-03-19T17:17:02.942297: step 142, loss 0.881605, acc 0.609375\n",
      "2017-03-19T17:17:03.764279: step 143, loss 1.10962, acc 0.578125\n",
      "2017-03-19T17:17:04.577858: step 144, loss 0.937195, acc 0.453125\n",
      "2017-03-19T17:17:05.412364: step 145, loss 1.24657, acc 0.453125\n",
      "2017-03-19T17:17:06.406466: step 146, loss 0.898718, acc 0.640625\n",
      "2017-03-19T17:17:07.357628: step 147, loss 0.878759, acc 0.625\n",
      "2017-03-19T17:17:08.352751: step 148, loss 1.013, acc 0.46875\n",
      "2017-03-19T17:17:09.361181: step 149, loss 1.24913, acc 0.390625\n",
      "2017-03-19T17:17:10.350262: step 150, loss 0.901482, acc 0.484375\n",
      "2017-03-19T17:17:11.341659: step 151, loss 0.917613, acc 0.640625\n",
      "2017-03-19T17:17:12.328761: step 152, loss 1.18782, acc 0.546875\n",
      "2017-03-19T17:17:13.266929: step 153, loss 0.764218, acc 0.65625\n",
      "2017-03-19T17:17:14.275147: step 154, loss 0.693075, acc 0.65625\n",
      "2017-03-19T17:17:15.176291: step 155, loss 0.677788, acc 0.671875\n",
      "2017-03-19T17:17:16.195014: step 156, loss 0.767215, acc 0.578125\n",
      "2017-03-19T17:17:17.171711: step 157, loss 0.814747, acc 0.671875\n",
      "2017-03-19T17:17:18.108879: step 158, loss 0.928254, acc 0.59375\n",
      "2017-03-19T17:17:19.142614: step 159, loss 0.72879, acc 0.625\n",
      "2017-03-19T17:17:20.064239: step 160, loss 0.965344, acc 0.53125\n",
      "2017-03-19T17:17:21.094503: step 161, loss 1.13136, acc 0.625\n",
      "2017-03-19T17:17:21.907032: step 162, loss 1.0765, acc 0.546875\n",
      "2017-03-19T17:17:22.853619: step 163, loss 0.910103, acc 0.625\n",
      "2017-03-19T17:17:23.817753: step 164, loss 0.795097, acc 0.609375\n",
      "2017-03-19T17:17:24.763877: step 165, loss 0.87876, acc 0.609375\n",
      "2017-03-19T17:17:25.616668: step 166, loss 1.0731, acc 0.515625\n",
      "2017-03-19T17:17:26.639863: step 167, loss 0.810088, acc 0.59375\n",
      "2017-03-19T17:17:27.462268: step 168, loss 1.00999, acc 0.5625\n",
      "2017-03-19T17:17:28.446384: step 169, loss 0.919087, acc 0.578125\n",
      "2017-03-19T17:17:29.403066: step 170, loss 0.85987, acc 0.515625\n",
      "2017-03-19T17:17:30.410233: step 171, loss 0.761626, acc 0.6875\n",
      "2017-03-19T17:17:31.393934: step 172, loss 0.849924, acc 0.609375\n",
      "2017-03-19T17:17:32.240296: step 173, loss 1.0093, acc 0.546875\n",
      "2017-03-19T17:17:33.053469: step 174, loss 1.07526, acc 0.5\n",
      "2017-03-19T17:17:33.925091: step 175, loss 0.921814, acc 0.625\n",
      "2017-03-19T17:17:34.745086: step 176, loss 0.656294, acc 0.671875\n",
      "2017-03-19T17:17:35.934880: step 177, loss 0.846643, acc 0.65625\n",
      "2017-03-19T17:17:36.898067: step 178, loss 1.0766, acc 0.5625\n",
      "2017-03-19T17:17:37.888789: step 179, loss 1.11481, acc 0.640625\n",
      "2017-03-19T17:17:38.855480: step 180, loss 0.847788, acc 0.609375\n",
      "2017-03-19T17:17:39.840312: step 181, loss 1.29733, acc 0.46875\n",
      "2017-03-19T17:17:40.666330: step 182, loss 1.23078, acc 0.515625\n",
      "2017-03-19T17:17:41.486463: step 183, loss 0.814119, acc 0.578125\n",
      "2017-03-19T17:17:42.352553: step 184, loss 0.738201, acc 0.625\n",
      "2017-03-19T17:17:43.194630: step 185, loss 1.29446, acc 0.46875\n",
      "2017-03-19T17:17:44.021666: step 186, loss 1.03378, acc 0.609375\n",
      "2017-03-19T17:17:44.839646: step 187, loss 0.832664, acc 0.65625\n",
      "2017-03-19T17:17:45.783213: step 188, loss 0.543225, acc 0.6875\n",
      "2017-03-19T17:17:46.774449: step 189, loss 0.641569, acc 0.671875\n",
      "2017-03-19T17:17:47.764637: step 190, loss 0.728746, acc 0.671875\n",
      "2017-03-19T17:17:48.720110: step 191, loss 0.76551, acc 0.609375\n",
      "2017-03-19T17:17:49.675738: step 192, loss 0.893957, acc 0.515625\n",
      "2017-03-19T17:17:50.649494: step 193, loss 0.55087, acc 0.71875\n",
      "2017-03-19T17:17:51.632706: step 194, loss 0.923526, acc 0.59375\n",
      "2017-03-19T17:17:52.607867: step 195, loss 0.585854, acc 0.71875\n",
      "2017-03-19T17:17:53.581264: step 196, loss 0.671819, acc 0.671875\n",
      "2017-03-19T17:17:54.476300: step 197, loss 0.623255, acc 0.640625\n",
      "2017-03-19T17:17:55.554568: step 198, loss 0.707172, acc 0.640625\n",
      "2017-03-19T17:17:56.505243: step 199, loss 1.04039, acc 0.578125\n",
      "2017-03-19T17:17:57.323434: step 200, loss 0.68543, acc 0.609375\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:18:01.578940: step 200, loss 0.560302, acc 0.702\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-200\n",
      "\n",
      "2017-03-19T17:18:03.765663: step 201, loss 0.652687, acc 0.71875\n",
      "2017-03-19T17:18:04.567136: step 202, loss 0.700341, acc 0.640625\n",
      "2017-03-19T17:18:05.430149: step 203, loss 0.613021, acc 0.625\n",
      "2017-03-19T17:18:06.303178: step 204, loss 0.879184, acc 0.578125\n",
      "2017-03-19T17:18:07.109752: step 205, loss 0.675166, acc 0.640625\n",
      "2017-03-19T17:18:08.071454: step 206, loss 0.736212, acc 0.59375\n",
      "2017-03-19T17:18:09.016366: step 207, loss 0.987372, acc 0.5\n",
      "2017-03-19T17:18:09.959486: step 208, loss 0.725998, acc 0.75\n",
      "2017-03-19T17:18:10.899350: step 209, loss 0.810903, acc 0.578125\n",
      "2017-03-19T17:18:11.695788: step 210, loss 0.90128, acc 0.53125\n",
      "2017-03-19T17:18:12.482952: step 211, loss 0.723461, acc 0.625\n",
      "2017-03-19T17:18:13.363483: step 212, loss 0.981285, acc 0.59375\n",
      "2017-03-19T17:18:14.342681: step 213, loss 0.684991, acc 0.59375\n",
      "2017-03-19T17:18:15.221640: step 214, loss 0.817279, acc 0.703125\n",
      "2017-03-19T17:18:16.286349: step 215, loss 0.689182, acc 0.578125\n",
      "2017-03-19T17:18:17.277601: step 216, loss 0.723152, acc 0.65625\n",
      "2017-03-19T17:18:18.148206: step 217, loss 0.855032, acc 0.625\n",
      "2017-03-19T17:18:18.980360: step 218, loss 0.907902, acc 0.5\n",
      "2017-03-19T17:18:19.989094: step 219, loss 0.90665, acc 0.546875\n",
      "2017-03-19T17:18:20.801182: step 220, loss 0.740446, acc 0.640625\n",
      "2017-03-19T17:18:21.640674: step 221, loss 0.708383, acc 0.671875\n",
      "2017-03-19T17:18:22.450198: step 222, loss 0.870225, acc 0.578125\n",
      "2017-03-19T17:18:23.457952: step 223, loss 0.71091, acc 0.671875\n",
      "2017-03-19T17:18:24.433062: step 224, loss 0.824758, acc 0.59375\n",
      "2017-03-19T17:18:25.480049: step 225, loss 0.762381, acc 0.609375\n",
      "2017-03-19T17:18:26.464647: step 226, loss 0.779666, acc 0.640625\n",
      "2017-03-19T17:18:27.510798: step 227, loss 1.07457, acc 0.5\n",
      "2017-03-19T17:18:28.462394: step 228, loss 0.864432, acc 0.59375\n",
      "2017-03-19T17:18:29.292383: step 229, loss 0.832381, acc 0.59375\n",
      "2017-03-19T17:18:30.293596: step 230, loss 0.908768, acc 0.625\n",
      "2017-03-19T17:18:31.155725: step 231, loss 0.856144, acc 0.546875\n",
      "2017-03-19T17:18:31.970703: step 232, loss 0.80412, acc 0.59375\n",
      "2017-03-19T17:18:32.795832: step 233, loss 0.720965, acc 0.59375\n",
      "2017-03-19T17:18:33.830963: step 234, loss 0.907726, acc 0.546875\n",
      "2017-03-19T17:18:34.657568: step 235, loss 0.716666, acc 0.703125\n",
      "2017-03-19T17:18:35.481624: step 236, loss 0.790035, acc 0.5625\n",
      "2017-03-19T17:18:36.315217: step 237, loss 0.779496, acc 0.65625\n",
      "2017-03-19T17:18:37.328496: step 238, loss 0.828705, acc 0.578125\n",
      "2017-03-19T17:18:38.386231: step 239, loss 0.609959, acc 0.65625\n",
      "2017-03-19T17:18:39.451596: step 240, loss 0.830546, acc 0.5625\n",
      "2017-03-19T17:18:40.430741: step 241, loss 0.951409, acc 0.59375\n",
      "2017-03-19T17:18:41.251249: step 242, loss 0.882951, acc 0.5\n",
      "2017-03-19T17:18:42.066924: step 243, loss 0.603295, acc 0.71875\n",
      "2017-03-19T17:18:42.883564: step 244, loss 0.855683, acc 0.59375\n",
      "2017-03-19T17:18:43.701557: step 245, loss 0.726865, acc 0.625\n",
      "2017-03-19T17:18:44.724750: step 246, loss 0.735378, acc 0.671875\n",
      "2017-03-19T17:18:45.554740: step 247, loss 0.784596, acc 0.53125\n",
      "2017-03-19T17:18:46.417371: step 248, loss 0.71077, acc 0.625\n",
      "2017-03-19T17:18:47.240928: step 249, loss 0.683342, acc 0.640625\n",
      "2017-03-19T17:18:48.296597: step 250, loss 0.640133, acc 0.71875\n",
      "2017-03-19T17:18:49.117632: step 251, loss 0.999285, acc 0.515625\n",
      "2017-03-19T17:18:49.940743: step 252, loss 0.791522, acc 0.578125\n",
      "2017-03-19T17:18:50.835798: step 253, loss 0.599323, acc 0.65625\n",
      "2017-03-19T17:18:51.643278: step 254, loss 0.716429, acc 0.609375\n",
      "2017-03-19T17:18:52.468333: step 255, loss 0.887898, acc 0.546875\n",
      "2017-03-19T17:18:53.288864: step 256, loss 0.75843, acc 0.625\n",
      "2017-03-19T17:18:54.355573: step 257, loss 0.776778, acc 0.625\n",
      "2017-03-19T17:18:55.336898: step 258, loss 0.765802, acc 0.703125\n",
      "2017-03-19T17:18:56.195974: step 259, loss 0.869401, acc 0.5\n",
      "2017-03-19T17:18:57.159672: step 260, loss 0.84109, acc 0.5\n",
      "2017-03-19T17:18:57.982207: step 261, loss 0.787248, acc 0.5625\n",
      "2017-03-19T17:18:58.802684: step 262, loss 0.77072, acc 0.609375\n",
      "2017-03-19T17:18:59.809351: step 263, loss 0.790766, acc 0.5625\n",
      "2017-03-19T17:19:00.643958: step 264, loss 0.723979, acc 0.59375\n",
      "2017-03-19T17:19:01.664167: step 265, loss 0.662064, acc 0.640625\n",
      "2017-03-19T17:19:02.539219: step 266, loss 0.745366, acc 0.609375\n",
      "2017-03-19T17:19:03.366809: step 267, loss 0.870024, acc 0.546875\n",
      "2017-03-19T17:19:04.383533: step 268, loss 0.834042, acc 0.53125\n",
      "2017-03-19T17:19:05.210531: step 269, loss 0.815434, acc 0.640625\n",
      "2017-03-19T17:19:06.039570: step 270, loss 0.745121, acc 0.53125\n",
      "2017-03-19T17:19:06.865156: step 271, loss 0.796968, acc 0.53125\n",
      "2017-03-19T17:19:07.684179: step 272, loss 0.818635, acc 0.5625\n",
      "2017-03-19T17:19:08.744382: step 273, loss 0.701261, acc 0.65625\n",
      "2017-03-19T17:19:09.711701: step 274, loss 0.701004, acc 0.6875\n",
      "2017-03-19T17:19:10.557701: step 275, loss 0.729445, acc 0.625\n",
      "2017-03-19T17:19:11.542006: step 276, loss 0.972938, acc 0.609375\n",
      "2017-03-19T17:19:12.544649: step 277, loss 0.820217, acc 0.546875\n",
      "2017-03-19T17:19:13.384254: step 278, loss 0.820116, acc 0.671875\n",
      "2017-03-19T17:19:14.364126: step 279, loss 0.79622, acc 0.65625\n",
      "2017-03-19T17:19:15.362337: step 280, loss 0.894934, acc 0.578125\n",
      "2017-03-19T17:19:16.399198: step 281, loss 0.804087, acc 0.5625\n",
      "2017-03-19T17:19:17.171386: step 282, loss 0.759003, acc 0.575\n",
      "2017-03-19T17:19:18.283181: step 283, loss 0.588621, acc 0.671875\n",
      "2017-03-19T17:19:19.160328: step 284, loss 0.660101, acc 0.671875\n",
      "2017-03-19T17:19:20.041108: step 285, loss 0.631463, acc 0.71875\n",
      "2017-03-19T17:19:20.899719: step 286, loss 0.608077, acc 0.671875\n",
      "2017-03-19T17:19:21.756303: step 287, loss 0.644727, acc 0.65625\n",
      "2017-03-19T17:19:22.649855: step 288, loss 0.852602, acc 0.546875\n",
      "2017-03-19T17:19:23.636530: step 289, loss 0.886486, acc 0.5625\n",
      "2017-03-19T17:19:24.506154: step 290, loss 0.603011, acc 0.671875\n",
      "2017-03-19T17:19:25.426995: step 291, loss 0.492368, acc 0.71875\n",
      "2017-03-19T17:19:26.262973: step 292, loss 0.600965, acc 0.65625\n",
      "2017-03-19T17:19:27.452613: step 293, loss 0.632344, acc 0.65625\n",
      "2017-03-19T17:19:28.425713: step 294, loss 0.838738, acc 0.5625\n",
      "2017-03-19T17:19:29.277303: step 295, loss 0.555549, acc 0.671875\n",
      "2017-03-19T17:19:30.344440: step 296, loss 0.596257, acc 0.671875\n",
      "2017-03-19T17:19:31.288971: step 297, loss 0.519763, acc 0.765625\n",
      "2017-03-19T17:19:32.267273: step 298, loss 0.722477, acc 0.59375\n",
      "2017-03-19T17:19:33.115467: step 299, loss 0.64276, acc 0.6875\n",
      "2017-03-19T17:19:33.954550: step 300, loss 0.614174, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:19:38.447569: step 300, loss 0.542549, acc 0.712\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-300\n",
      "\n",
      "2017-03-19T17:19:40.790758: step 301, loss 0.664443, acc 0.6875\n",
      "2017-03-19T17:19:41.613083: step 302, loss 0.606022, acc 0.6875\n",
      "2017-03-19T17:19:42.553637: step 303, loss 0.614251, acc 0.671875\n",
      "2017-03-19T17:19:43.373543: step 304, loss 0.640075, acc 0.671875\n",
      "2017-03-19T17:19:44.160157: step 305, loss 0.766446, acc 0.609375\n",
      "2017-03-19T17:19:44.945171: step 306, loss 0.658839, acc 0.65625\n",
      "2017-03-19T17:19:45.755487: step 307, loss 0.577055, acc 0.734375\n",
      "2017-03-19T17:19:46.567565: step 308, loss 0.830862, acc 0.609375\n",
      "2017-03-19T17:19:47.402333: step 309, loss 0.638996, acc 0.6875\n",
      "2017-03-19T17:19:48.306978: step 310, loss 0.649704, acc 0.578125\n",
      "2017-03-19T17:19:49.171593: step 311, loss 0.725501, acc 0.625\n",
      "2017-03-19T17:19:49.989267: step 312, loss 0.582097, acc 0.734375\n",
      "2017-03-19T17:19:50.860388: step 313, loss 0.841832, acc 0.625\n",
      "2017-03-19T17:19:51.700430: step 314, loss 0.582396, acc 0.625\n",
      "2017-03-19T17:19:52.536530: step 315, loss 0.826914, acc 0.59375\n",
      "2017-03-19T17:19:53.391718: step 316, loss 0.516113, acc 0.734375\n",
      "2017-03-19T17:19:54.253275: step 317, loss 0.738156, acc 0.59375\n",
      "2017-03-19T17:19:55.136946: step 318, loss 0.631562, acc 0.6875\n",
      "2017-03-19T17:19:55.982675: step 319, loss 0.691638, acc 0.625\n",
      "2017-03-19T17:19:56.836147: step 320, loss 0.579588, acc 0.640625\n",
      "2017-03-19T17:19:57.671683: step 321, loss 0.559056, acc 0.71875\n",
      "2017-03-19T17:19:58.505824: step 322, loss 0.637229, acc 0.640625\n",
      "2017-03-19T17:19:59.344215: step 323, loss 0.78052, acc 0.625\n",
      "2017-03-19T17:20:00.226843: step 324, loss 0.538474, acc 0.671875\n",
      "2017-03-19T17:20:01.077605: step 325, loss 0.637587, acc 0.65625\n",
      "2017-03-19T17:20:01.947034: step 326, loss 0.795492, acc 0.5625\n",
      "2017-03-19T17:20:02.756378: step 327, loss 0.722112, acc 0.640625\n",
      "2017-03-19T17:20:03.579197: step 328, loss 0.657516, acc 0.65625\n",
      "2017-03-19T17:20:04.579410: step 329, loss 0.78314, acc 0.5625\n",
      "2017-03-19T17:20:05.596108: step 330, loss 0.597093, acc 0.71875\n",
      "2017-03-19T17:20:06.636350: step 331, loss 0.641351, acc 0.671875\n",
      "2017-03-19T17:20:07.683762: step 332, loss 0.81771, acc 0.5625\n",
      "2017-03-19T17:20:08.751039: step 333, loss 0.658826, acc 0.6875\n",
      "2017-03-19T17:20:09.778864: step 334, loss 0.540789, acc 0.703125\n",
      "2017-03-19T17:20:10.833114: step 335, loss 0.767396, acc 0.609375\n",
      "2017-03-19T17:20:11.827323: step 336, loss 0.581842, acc 0.734375\n",
      "2017-03-19T17:20:12.743976: step 337, loss 0.786782, acc 0.578125\n",
      "2017-03-19T17:20:13.863774: step 338, loss 0.509397, acc 0.75\n",
      "2017-03-19T17:20:14.914086: step 339, loss 0.587483, acc 0.609375\n",
      "2017-03-19T17:20:15.945321: step 340, loss 0.64192, acc 0.703125\n",
      "2017-03-19T17:20:16.968763: step 341, loss 0.61589, acc 0.625\n",
      "2017-03-19T17:20:17.970477: step 342, loss 0.599636, acc 0.640625\n",
      "2017-03-19T17:20:19.013220: step 343, loss 0.751728, acc 0.640625\n",
      "2017-03-19T17:20:20.039950: step 344, loss 0.596777, acc 0.671875\n",
      "2017-03-19T17:20:21.002136: step 345, loss 0.728026, acc 0.65625\n",
      "2017-03-19T17:20:22.018360: step 346, loss 0.601941, acc 0.65625\n",
      "2017-03-19T17:20:22.872442: step 347, loss 0.663479, acc 0.625\n",
      "2017-03-19T17:20:23.718356: step 348, loss 0.567161, acc 0.734375\n",
      "2017-03-19T17:20:24.618002: step 349, loss 0.578519, acc 0.6875\n",
      "2017-03-19T17:20:25.475367: step 350, loss 0.568585, acc 0.734375\n",
      "2017-03-19T17:20:26.312984: step 351, loss 0.564858, acc 0.703125\n",
      "2017-03-19T17:20:27.240134: step 352, loss 0.66882, acc 0.609375\n",
      "2017-03-19T17:20:28.312398: step 353, loss 0.653359, acc 0.6875\n",
      "2017-03-19T17:20:29.317062: step 354, loss 0.689637, acc 0.5625\n",
      "2017-03-19T17:20:30.160212: step 355, loss 0.733012, acc 0.59375\n",
      "2017-03-19T17:20:30.989576: step 356, loss 0.556643, acc 0.703125\n",
      "2017-03-19T17:20:31.810611: step 357, loss 0.63344, acc 0.640625\n",
      "2017-03-19T17:20:32.647770: step 358, loss 0.609662, acc 0.640625\n",
      "2017-03-19T17:20:33.513859: step 359, loss 0.744595, acc 0.640625\n",
      "2017-03-19T17:20:34.349965: step 360, loss 0.716725, acc 0.578125\n",
      "2017-03-19T17:20:35.191801: step 361, loss 0.646888, acc 0.703125\n",
      "2017-03-19T17:20:36.029555: step 362, loss 0.799468, acc 0.53125\n",
      "2017-03-19T17:20:36.851750: step 363, loss 0.67831, acc 0.609375\n",
      "2017-03-19T17:20:37.820126: step 364, loss 0.709221, acc 0.65625\n",
      "2017-03-19T17:20:38.831640: step 365, loss 0.672862, acc 0.59375\n",
      "2017-03-19T17:20:39.838472: step 366, loss 0.660689, acc 0.65625\n",
      "2017-03-19T17:20:40.826635: step 367, loss 0.649174, acc 0.640625\n",
      "2017-03-19T17:20:41.801742: step 368, loss 0.574498, acc 0.671875\n",
      "2017-03-19T17:20:42.625278: step 369, loss 0.574124, acc 0.78125\n",
      "2017-03-19T17:20:43.718476: step 370, loss 0.800278, acc 0.625\n",
      "2017-03-19T17:20:44.669778: step 371, loss 0.758918, acc 0.609375\n",
      "2017-03-19T17:20:45.693161: step 372, loss 0.745351, acc 0.53125\n",
      "2017-03-19T17:20:46.533870: step 373, loss 0.677169, acc 0.609375\n",
      "2017-03-19T17:20:47.396631: step 374, loss 0.535135, acc 0.78125\n",
      "2017-03-19T17:20:48.243266: step 375, loss 0.502409, acc 0.71875\n",
      "2017-03-19T17:20:49.092530: step 376, loss 0.647442, acc 0.640625\n",
      "2017-03-19T17:20:49.922181: step 377, loss 0.555344, acc 0.71875\n",
      "2017-03-19T17:20:50.784080: step 378, loss 0.67602, acc 0.609375\n",
      "2017-03-19T17:20:51.790161: step 379, loss 0.675594, acc 0.59375\n",
      "2017-03-19T17:20:52.751835: step 380, loss 0.546208, acc 0.671875\n",
      "2017-03-19T17:20:53.600479: step 381, loss 0.59163, acc 0.671875\n",
      "2017-03-19T17:20:54.686253: step 382, loss 0.689416, acc 0.609375\n",
      "2017-03-19T17:20:55.545365: step 383, loss 0.605892, acc 0.640625\n",
      "2017-03-19T17:20:56.384492: step 384, loss 0.763471, acc 0.5625\n",
      "2017-03-19T17:20:57.458648: step 385, loss 0.607588, acc 0.65625\n",
      "2017-03-19T17:20:58.480795: step 386, loss 0.744337, acc 0.625\n",
      "2017-03-19T17:20:59.361085: step 387, loss 0.656109, acc 0.65625\n",
      "2017-03-19T17:21:00.454361: step 388, loss 0.633689, acc 0.625\n",
      "2017-03-19T17:21:01.495526: step 389, loss 0.569784, acc 0.6875\n",
      "2017-03-19T17:21:02.553872: step 390, loss 0.636184, acc 0.625\n",
      "2017-03-19T17:21:03.389816: step 391, loss 0.709615, acc 0.59375\n",
      "2017-03-19T17:21:04.388468: step 392, loss 0.487379, acc 0.71875\n",
      "2017-03-19T17:21:05.418123: step 393, loss 0.668652, acc 0.671875\n",
      "2017-03-19T17:21:06.258168: step 394, loss 0.586701, acc 0.734375\n",
      "2017-03-19T17:21:07.214850: step 395, loss 0.773918, acc 0.65625\n",
      "2017-03-19T17:21:08.132504: step 396, loss 0.638968, acc 0.6875\n",
      "2017-03-19T17:21:08.988228: step 397, loss 0.521334, acc 0.78125\n",
      "2017-03-19T17:21:09.834829: step 398, loss 0.599563, acc 0.78125\n",
      "2017-03-19T17:21:10.853050: step 399, loss 0.623396, acc 0.640625\n",
      "2017-03-19T17:21:11.892470: step 400, loss 0.723474, acc 0.5625\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:21:16.545124: step 400, loss 0.556418, acc 0.715\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-400\n",
      "\n",
      "2017-03-19T17:21:18.656407: step 401, loss 0.587754, acc 0.671875\n",
      "2017-03-19T17:21:19.455009: step 402, loss 0.622955, acc 0.71875\n",
      "2017-03-19T17:21:20.246583: step 403, loss 0.676068, acc 0.53125\n",
      "2017-03-19T17:21:21.036146: step 404, loss 0.919033, acc 0.484375\n",
      "2017-03-19T17:21:21.847730: step 405, loss 0.786526, acc 0.515625\n",
      "2017-03-19T17:21:22.649301: step 406, loss 0.539903, acc 0.6875\n",
      "2017-03-19T17:21:23.462405: step 407, loss 0.691277, acc 0.640625\n",
      "2017-03-19T17:21:24.268138: step 408, loss 0.841406, acc 0.671875\n",
      "2017-03-19T17:21:25.074008: step 409, loss 0.624729, acc 0.609375\n",
      "2017-03-19T17:21:25.862227: step 410, loss 0.650097, acc 0.609375\n",
      "2017-03-19T17:21:26.672194: step 411, loss 0.710957, acc 0.671875\n",
      "2017-03-19T17:21:27.480331: step 412, loss 0.622075, acc 0.65625\n",
      "2017-03-19T17:21:28.310163: step 413, loss 0.766969, acc 0.578125\n",
      "2017-03-19T17:21:29.169525: step 414, loss 0.605777, acc 0.640625\n",
      "2017-03-19T17:21:30.022626: step 415, loss 0.669143, acc 0.609375\n",
      "2017-03-19T17:21:30.905157: step 416, loss 0.759688, acc 0.546875\n",
      "2017-03-19T17:21:31.776302: step 417, loss 0.586485, acc 0.671875\n",
      "2017-03-19T17:21:32.648581: step 418, loss 0.671939, acc 0.65625\n",
      "2017-03-19T17:21:33.509566: step 419, loss 0.639081, acc 0.640625\n",
      "2017-03-19T17:21:34.518580: step 420, loss 0.732555, acc 0.59375\n",
      "2017-03-19T17:21:35.369686: step 421, loss 0.72236, acc 0.625\n",
      "2017-03-19T17:21:36.574773: step 422, loss 0.593162, acc 0.65625\n",
      "2017-03-19T17:21:37.150739: step 423, loss 0.533761, acc 0.75\n",
      "2017-03-19T17:21:38.055593: step 424, loss 0.704592, acc 0.625\n",
      "2017-03-19T17:21:38.980751: step 425, loss 0.614647, acc 0.609375\n",
      "2017-03-19T17:21:40.025215: step 426, loss 0.528635, acc 0.625\n",
      "2017-03-19T17:21:40.850804: step 427, loss 0.676637, acc 0.75\n",
      "2017-03-19T17:21:41.699525: step 428, loss 0.469264, acc 0.8125\n",
      "2017-03-19T17:21:42.812204: step 429, loss 0.666828, acc 0.5625\n",
      "2017-03-19T17:21:43.910489: step 430, loss 0.570037, acc 0.703125\n",
      "2017-03-19T17:21:44.905311: step 431, loss 0.624476, acc 0.640625\n",
      "2017-03-19T17:21:45.880690: step 432, loss 0.501469, acc 0.71875\n",
      "2017-03-19T17:21:46.998986: step 433, loss 0.684152, acc 0.734375\n",
      "2017-03-19T17:21:48.079254: step 434, loss 0.527973, acc 0.71875\n",
      "2017-03-19T17:21:49.107599: step 435, loss 0.539208, acc 0.6875\n",
      "2017-03-19T17:21:50.190180: step 436, loss 0.552216, acc 0.75\n",
      "2017-03-19T17:21:51.328221: step 437, loss 0.617688, acc 0.6875\n",
      "2017-03-19T17:21:52.395548: step 438, loss 0.548821, acc 0.625\n",
      "2017-03-19T17:21:53.425725: step 439, loss 0.686129, acc 0.609375\n",
      "2017-03-19T17:21:54.398751: step 440, loss 0.634967, acc 0.703125\n",
      "2017-03-19T17:21:55.444005: step 441, loss 0.496327, acc 0.796875\n",
      "2017-03-19T17:21:56.449169: step 442, loss 0.635674, acc 0.734375\n",
      "2017-03-19T17:21:57.482746: step 443, loss 0.574483, acc 0.75\n",
      "2017-03-19T17:21:58.523118: step 444, loss 0.503648, acc 0.65625\n",
      "2017-03-19T17:21:59.376137: step 445, loss 0.504436, acc 0.671875\n",
      "2017-03-19T17:22:00.241879: step 446, loss 0.626461, acc 0.6875\n",
      "2017-03-19T17:22:01.085461: step 447, loss 0.576274, acc 0.703125\n",
      "2017-03-19T17:22:02.056609: step 448, loss 0.588993, acc 0.640625\n",
      "2017-03-19T17:22:02.881192: step 449, loss 0.594924, acc 0.6875\n",
      "2017-03-19T17:22:03.711231: step 450, loss 0.544547, acc 0.71875\n",
      "2017-03-19T17:22:04.557549: step 451, loss 0.56272, acc 0.703125\n",
      "2017-03-19T17:22:05.422674: step 452, loss 0.575699, acc 0.6875\n",
      "2017-03-19T17:22:06.350991: step 453, loss 0.601354, acc 0.671875\n",
      "2017-03-19T17:22:07.355707: step 454, loss 0.688003, acc 0.625\n",
      "2017-03-19T17:22:08.205570: step 455, loss 0.668409, acc 0.6875\n",
      "2017-03-19T17:22:09.053650: step 456, loss 0.614066, acc 0.625\n",
      "2017-03-19T17:22:10.125501: step 457, loss 0.647368, acc 0.609375\n",
      "2017-03-19T17:22:11.136477: step 458, loss 0.526866, acc 0.703125\n",
      "2017-03-19T17:22:11.978753: step 459, loss 0.560415, acc 0.65625\n",
      "2017-03-19T17:22:13.033486: step 460, loss 0.607775, acc 0.625\n",
      "2017-03-19T17:22:13.858788: step 461, loss 0.56416, acc 0.703125\n",
      "2017-03-19T17:22:14.697781: step 462, loss 0.492039, acc 0.765625\n",
      "2017-03-19T17:22:15.531345: step 463, loss 0.6355, acc 0.640625\n",
      "2017-03-19T17:22:16.579591: step 464, loss 0.705565, acc 0.65625\n",
      "2017-03-19T17:22:17.420819: step 465, loss 0.500089, acc 0.703125\n",
      "2017-03-19T17:22:18.529075: step 466, loss 0.538313, acc 0.703125\n",
      "2017-03-19T17:22:19.416707: step 467, loss 0.531473, acc 0.71875\n",
      "2017-03-19T17:22:20.591043: step 468, loss 0.509155, acc 0.671875\n",
      "2017-03-19T17:22:21.691327: step 469, loss 0.616241, acc 0.671875\n",
      "2017-03-19T17:22:22.563449: step 470, loss 0.506775, acc 0.75\n",
      "2017-03-19T17:22:23.445577: step 471, loss 0.492296, acc 0.6875\n",
      "2017-03-19T17:22:24.313696: step 472, loss 0.689148, acc 0.65625\n",
      "2017-03-19T17:22:25.205413: step 473, loss 0.703324, acc 0.671875\n",
      "2017-03-19T17:22:26.255161: step 474, loss 0.695964, acc 0.65625\n",
      "2017-03-19T17:22:27.165323: step 475, loss 0.676214, acc 0.65625\n",
      "2017-03-19T17:22:28.013928: step 476, loss 0.54234, acc 0.6875\n",
      "2017-03-19T17:22:28.914069: step 477, loss 0.614646, acc 0.609375\n",
      "2017-03-19T17:22:29.765676: step 478, loss 0.603466, acc 0.765625\n",
      "2017-03-19T17:22:30.664901: step 479, loss 0.611819, acc 0.6875\n",
      "2017-03-19T17:22:31.698042: step 480, loss 0.786413, acc 0.609375\n",
      "2017-03-19T17:22:32.722783: step 481, loss 0.605243, acc 0.6875\n",
      "2017-03-19T17:22:33.735560: step 482, loss 0.577296, acc 0.59375\n",
      "2017-03-19T17:22:34.835845: step 483, loss 0.699216, acc 0.671875\n",
      "2017-03-19T17:22:35.821235: step 484, loss 0.523119, acc 0.78125\n",
      "2017-03-19T17:22:36.713207: step 485, loss 0.715075, acc 0.5625\n",
      "2017-03-19T17:22:37.625846: step 486, loss 0.627937, acc 0.609375\n",
      "2017-03-19T17:22:38.527775: step 487, loss 0.452143, acc 0.796875\n",
      "2017-03-19T17:22:39.435495: step 488, loss 0.571714, acc 0.609375\n",
      "2017-03-19T17:22:40.311266: step 489, loss 0.604589, acc 0.671875\n",
      "2017-03-19T17:22:41.184387: step 490, loss 0.597454, acc 0.703125\n",
      "2017-03-19T17:22:42.058521: step 491, loss 0.551511, acc 0.640625\n",
      "2017-03-19T17:22:43.024238: step 492, loss 0.437303, acc 0.8125\n",
      "2017-03-19T17:22:43.868807: step 493, loss 0.618332, acc 0.59375\n",
      "2017-03-19T17:22:44.725418: step 494, loss 0.382082, acc 0.84375\n",
      "2017-03-19T17:22:45.583529: step 495, loss 0.534127, acc 0.734375\n",
      "2017-03-19T17:22:46.440617: step 496, loss 0.529839, acc 0.640625\n",
      "2017-03-19T17:22:47.326787: step 497, loss 0.784067, acc 0.546875\n",
      "2017-03-19T17:22:48.232432: step 498, loss 0.511126, acc 0.71875\n",
      "2017-03-19T17:22:49.127970: step 499, loss 0.596315, acc 0.65625\n",
      "2017-03-19T17:22:49.988581: step 500, loss 0.536388, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:22:54.431747: step 500, loss 0.524148, acc 0.736\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-500\n",
      "\n",
      "2017-03-19T17:22:56.422426: step 501, loss 0.530431, acc 0.671875\n",
      "2017-03-19T17:22:57.300039: step 502, loss 0.570462, acc 0.703125\n",
      "2017-03-19T17:22:58.132182: step 503, loss 0.554071, acc 0.671875\n",
      "2017-03-19T17:22:58.986290: step 504, loss 0.489347, acc 0.75\n",
      "2017-03-19T17:22:59.895443: step 505, loss 0.604979, acc 0.625\n",
      "2017-03-19T17:23:00.856622: step 506, loss 0.684924, acc 0.609375\n",
      "2017-03-19T17:23:01.782599: step 507, loss 0.530581, acc 0.71875\n",
      "2017-03-19T17:23:02.799339: step 508, loss 0.637135, acc 0.609375\n",
      "2017-03-19T17:23:03.807057: step 509, loss 0.644839, acc 0.65625\n",
      "2017-03-19T17:23:04.841294: step 510, loss 0.550189, acc 0.640625\n",
      "2017-03-19T17:23:05.839752: step 511, loss 0.583352, acc 0.703125\n",
      "2017-03-19T17:23:06.861291: step 512, loss 0.618859, acc 0.671875\n",
      "2017-03-19T17:23:07.849756: step 513, loss 0.526207, acc 0.765625\n",
      "2017-03-19T17:23:08.789009: step 514, loss 0.47255, acc 0.703125\n",
      "2017-03-19T17:23:09.705162: step 515, loss 0.50772, acc 0.671875\n",
      "2017-03-19T17:23:10.579431: step 516, loss 0.544119, acc 0.75\n",
      "2017-03-19T17:23:11.463561: step 517, loss 0.61365, acc 0.703125\n",
      "2017-03-19T17:23:12.352212: step 518, loss 0.628649, acc 0.59375\n",
      "2017-03-19T17:23:13.203246: step 519, loss 0.57905, acc 0.609375\n",
      "2017-03-19T17:23:14.049229: step 520, loss 0.58659, acc 0.6875\n",
      "2017-03-19T17:23:14.887042: step 521, loss 0.577867, acc 0.75\n",
      "2017-03-19T17:23:15.724641: step 522, loss 0.591095, acc 0.65625\n",
      "2017-03-19T17:23:16.566576: step 523, loss 0.470364, acc 0.71875\n",
      "2017-03-19T17:23:17.432963: step 524, loss 0.557003, acc 0.734375\n",
      "2017-03-19T17:23:18.307731: step 525, loss 0.608419, acc 0.6875\n",
      "2017-03-19T17:23:19.169757: step 526, loss 0.592677, acc 0.71875\n",
      "2017-03-19T17:23:20.164119: step 527, loss 0.507755, acc 0.734375\n",
      "2017-03-19T17:23:21.161395: step 528, loss 0.579281, acc 0.765625\n",
      "2017-03-19T17:23:22.128697: step 529, loss 0.701411, acc 0.625\n",
      "2017-03-19T17:23:23.098553: step 530, loss 0.541375, acc 0.78125\n",
      "2017-03-19T17:23:24.117519: step 531, loss 0.531462, acc 0.765625\n",
      "2017-03-19T17:23:25.126846: step 532, loss 0.500721, acc 0.765625\n",
      "2017-03-19T17:23:26.127114: step 533, loss 0.644948, acc 0.53125\n",
      "2017-03-19T17:23:27.006379: step 534, loss 0.557991, acc 0.71875\n",
      "2017-03-19T17:23:27.835462: step 535, loss 0.564275, acc 0.65625\n",
      "2017-03-19T17:23:28.685198: step 536, loss 0.513148, acc 0.71875\n",
      "2017-03-19T17:23:29.533500: step 537, loss 0.572609, acc 0.734375\n",
      "2017-03-19T17:23:30.393112: step 538, loss 0.550675, acc 0.6875\n",
      "2017-03-19T17:23:31.385411: step 539, loss 0.5944, acc 0.703125\n",
      "2017-03-19T17:23:32.527873: step 540, loss 0.555364, acc 0.671875\n",
      "2017-03-19T17:23:33.615021: step 541, loss 0.480604, acc 0.78125\n",
      "2017-03-19T17:23:34.666757: step 542, loss 0.630898, acc 0.65625\n",
      "2017-03-19T17:23:35.792560: step 543, loss 0.569216, acc 0.71875\n",
      "2017-03-19T17:23:36.895845: step 544, loss 0.557837, acc 0.703125\n",
      "2017-03-19T17:23:37.775104: step 545, loss 0.611268, acc 0.640625\n",
      "2017-03-19T17:23:38.894098: step 546, loss 0.445611, acc 0.8125\n",
      "2017-03-19T17:23:39.888023: step 547, loss 0.634701, acc 0.640625\n",
      "2017-03-19T17:23:40.915330: step 548, loss 0.539347, acc 0.734375\n",
      "2017-03-19T17:23:41.766402: step 549, loss 0.649738, acc 0.734375\n",
      "2017-03-19T17:23:42.616611: step 550, loss 0.697231, acc 0.5625\n",
      "2017-03-19T17:23:43.648934: step 551, loss 0.667022, acc 0.65625\n",
      "2017-03-19T17:23:44.642634: step 552, loss 0.608332, acc 0.640625\n",
      "2017-03-19T17:23:45.496102: step 553, loss 0.609097, acc 0.671875\n",
      "2017-03-19T17:23:46.484450: step 554, loss 0.563897, acc 0.6875\n",
      "2017-03-19T17:23:47.508802: step 555, loss 0.564881, acc 0.6875\n",
      "2017-03-19T17:23:48.492997: step 556, loss 0.533399, acc 0.671875\n",
      "2017-03-19T17:23:49.354516: step 557, loss 0.491633, acc 0.6875\n",
      "2017-03-19T17:23:50.214249: step 558, loss 0.644885, acc 0.578125\n",
      "2017-03-19T17:23:51.354642: step 559, loss 0.597015, acc 0.625\n",
      "2017-03-19T17:23:52.421852: step 560, loss 0.490083, acc 0.75\n",
      "2017-03-19T17:23:53.462463: step 561, loss 0.590575, acc 0.65625\n",
      "2017-03-19T17:23:54.507253: step 562, loss 0.619599, acc 0.625\n",
      "2017-03-19T17:23:55.339338: step 563, loss 0.567587, acc 0.6875\n",
      "2017-03-19T17:23:55.920834: step 564, loss 0.615846, acc 0.65\n",
      "2017-03-19T17:23:56.768711: step 565, loss 0.554698, acc 0.609375\n",
      "2017-03-19T17:23:57.630325: step 566, loss 0.577772, acc 0.65625\n",
      "2017-03-19T17:23:58.664945: step 567, loss 0.468057, acc 0.765625\n",
      "2017-03-19T17:23:59.667584: step 568, loss 0.62061, acc 0.65625\n",
      "2017-03-19T17:24:00.684237: step 569, loss 0.66675, acc 0.703125\n",
      "2017-03-19T17:24:01.692744: step 570, loss 0.59705, acc 0.625\n",
      "2017-03-19T17:24:02.725887: step 571, loss 0.437134, acc 0.78125\n",
      "2017-03-19T17:24:03.576126: step 572, loss 0.445252, acc 0.75\n",
      "2017-03-19T17:24:04.424329: step 573, loss 0.4923, acc 0.703125\n",
      "2017-03-19T17:24:05.292917: step 574, loss 0.468398, acc 0.734375\n",
      "2017-03-19T17:24:06.115672: step 575, loss 0.511441, acc 0.6875\n",
      "2017-03-19T17:24:06.969433: step 576, loss 0.584111, acc 0.65625\n",
      "2017-03-19T17:24:07.814964: step 577, loss 0.51756, acc 0.671875\n",
      "2017-03-19T17:24:08.674614: step 578, loss 0.618729, acc 0.71875\n",
      "2017-03-19T17:24:09.521631: step 579, loss 0.523336, acc 0.71875\n",
      "2017-03-19T17:24:10.381059: step 580, loss 0.465302, acc 0.796875\n",
      "2017-03-19T17:24:11.253861: step 581, loss 0.41197, acc 0.78125\n",
      "2017-03-19T17:24:12.134430: step 582, loss 0.524675, acc 0.734375\n",
      "2017-03-19T17:24:12.993609: step 583, loss 0.528402, acc 0.765625\n",
      "2017-03-19T17:24:13.822709: step 584, loss 0.56269, acc 0.71875\n",
      "2017-03-19T17:24:14.692961: step 585, loss 0.439089, acc 0.8125\n",
      "2017-03-19T17:24:15.669656: step 586, loss 0.408894, acc 0.78125\n",
      "2017-03-19T17:24:16.625042: step 587, loss 0.623769, acc 0.65625\n",
      "2017-03-19T17:24:17.485154: step 588, loss 0.623095, acc 0.671875\n",
      "2017-03-19T17:24:18.515059: step 589, loss 0.485587, acc 0.75\n",
      "2017-03-19T17:24:19.350602: step 590, loss 0.458965, acc 0.765625\n",
      "2017-03-19T17:24:20.299486: step 591, loss 0.411724, acc 0.8125\n",
      "2017-03-19T17:24:21.310603: step 592, loss 0.453669, acc 0.78125\n",
      "2017-03-19T17:24:22.300464: step 593, loss 0.525804, acc 0.671875\n",
      "2017-03-19T17:24:23.279162: step 594, loss 0.470905, acc 0.796875\n",
      "2017-03-19T17:24:24.292970: step 595, loss 0.534743, acc 0.6875\n",
      "2017-03-19T17:24:25.323283: step 596, loss 0.52729, acc 0.734375\n",
      "2017-03-19T17:24:26.313408: step 597, loss 0.505586, acc 0.65625\n",
      "2017-03-19T17:24:27.189985: step 598, loss 0.555477, acc 0.71875\n",
      "2017-03-19T17:24:28.049097: step 599, loss 0.647809, acc 0.671875\n",
      "2017-03-19T17:24:28.894295: step 600, loss 0.593979, acc 0.6875\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:24:33.297235: step 600, loss 0.493044, acc 0.756\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-600\n",
      "\n",
      "2017-03-19T17:24:34.941962: step 601, loss 0.506894, acc 0.71875\n",
      "2017-03-19T17:24:35.732525: step 602, loss 0.522782, acc 0.65625\n",
      "2017-03-19T17:24:36.539717: step 603, loss 0.531162, acc 0.734375\n",
      "2017-03-19T17:24:37.336703: step 604, loss 0.528719, acc 0.671875\n",
      "2017-03-19T17:24:38.192264: step 605, loss 0.623125, acc 0.703125\n",
      "2017-03-19T17:24:39.212491: step 606, loss 0.578512, acc 0.703125\n",
      "2017-03-19T17:24:40.092119: step 607, loss 0.726497, acc 0.640625\n",
      "2017-03-19T17:24:41.209427: step 608, loss 0.398168, acc 0.828125\n",
      "2017-03-19T17:24:42.199639: step 609, loss 0.626047, acc 0.6875\n",
      "2017-03-19T17:24:43.018659: step 610, loss 0.505516, acc 0.765625\n",
      "2017-03-19T17:24:43.834691: step 611, loss 0.552939, acc 0.671875\n",
      "2017-03-19T17:24:44.672206: step 612, loss 0.606799, acc 0.671875\n",
      "2017-03-19T17:24:45.686380: step 613, loss 0.531456, acc 0.703125\n",
      "2017-03-19T17:24:46.529100: step 614, loss 0.469695, acc 0.796875\n",
      "2017-03-19T17:24:47.585649: step 615, loss 0.578274, acc 0.765625\n",
      "2017-03-19T17:24:48.697441: step 616, loss 0.593254, acc 0.640625\n",
      "2017-03-19T17:24:49.637110: step 617, loss 0.518821, acc 0.71875\n",
      "2017-03-19T17:24:50.533734: step 618, loss 0.696869, acc 0.640625\n",
      "2017-03-19T17:24:51.737626: step 619, loss 0.529223, acc 0.75\n",
      "2017-03-19T17:24:52.763358: step 620, loss 0.551051, acc 0.765625\n",
      "2017-03-19T17:24:53.820611: step 621, loss 0.478699, acc 0.71875\n",
      "2017-03-19T17:24:54.927918: step 622, loss 0.498197, acc 0.75\n",
      "2017-03-19T17:24:55.951553: step 623, loss 0.623724, acc 0.734375\n",
      "2017-03-19T17:24:56.956407: step 624, loss 0.636079, acc 0.609375\n",
      "2017-03-19T17:24:57.811251: step 625, loss 0.511558, acc 0.6875\n",
      "2017-03-19T17:24:58.641254: step 626, loss 0.613756, acc 0.65625\n",
      "2017-03-19T17:24:59.470241: step 627, loss 0.621114, acc 0.671875\n",
      "2017-03-19T17:25:00.303336: step 628, loss 0.500999, acc 0.765625\n",
      "2017-03-19T17:25:01.128319: step 629, loss 0.429586, acc 0.796875\n",
      "2017-03-19T17:25:01.954932: step 630, loss 0.592702, acc 0.734375\n",
      "2017-03-19T17:25:02.928077: step 631, loss 0.627831, acc 0.703125\n",
      "2017-03-19T17:25:03.798109: step 632, loss 0.525949, acc 0.6875\n",
      "2017-03-19T17:25:04.768250: step 633, loss 0.500691, acc 0.796875\n",
      "2017-03-19T17:25:05.770462: step 634, loss 0.566277, acc 0.6875\n",
      "2017-03-19T17:25:06.665559: step 635, loss 0.440305, acc 0.8125\n",
      "2017-03-19T17:25:07.599724: step 636, loss 0.521409, acc 0.6875\n",
      "2017-03-19T17:25:08.590978: step 637, loss 0.43199, acc 0.828125\n",
      "2017-03-19T17:25:09.450011: step 638, loss 0.574419, acc 0.65625\n",
      "2017-03-19T17:25:10.471705: step 639, loss 0.511718, acc 0.765625\n",
      "2017-03-19T17:25:11.501359: step 640, loss 0.548273, acc 0.71875\n",
      "2017-03-19T17:25:12.550502: step 641, loss 0.615852, acc 0.625\n",
      "2017-03-19T17:25:13.663794: step 642, loss 0.542243, acc 0.6875\n",
      "2017-03-19T17:25:14.744597: step 643, loss 0.557036, acc 0.703125\n",
      "2017-03-19T17:25:15.896917: step 644, loss 0.480775, acc 0.71875\n",
      "2017-03-19T17:25:16.885121: step 645, loss 0.539094, acc 0.6875\n",
      "2017-03-19T17:25:17.808279: step 646, loss 0.483844, acc 0.71875\n",
      "2017-03-19T17:25:18.668806: step 647, loss 0.616047, acc 0.609375\n",
      "2017-03-19T17:25:19.611479: step 648, loss 0.612308, acc 0.640625\n",
      "2017-03-19T17:25:20.519644: step 649, loss 0.513634, acc 0.75\n",
      "2017-03-19T17:25:21.378664: step 650, loss 0.496992, acc 0.6875\n",
      "2017-03-19T17:25:22.213707: step 651, loss 0.524709, acc 0.78125\n",
      "2017-03-19T17:25:23.070870: step 652, loss 0.531092, acc 0.71875\n",
      "2017-03-19T17:25:23.895062: step 653, loss 0.664628, acc 0.640625\n",
      "2017-03-19T17:25:24.735564: step 654, loss 0.46593, acc 0.75\n",
      "2017-03-19T17:25:25.598597: step 655, loss 0.419414, acc 0.734375\n",
      "2017-03-19T17:25:26.444598: step 656, loss 0.501856, acc 0.6875\n",
      "2017-03-19T17:25:27.291201: step 657, loss 0.559904, acc 0.703125\n",
      "2017-03-19T17:25:28.126271: step 658, loss 0.505525, acc 0.6875\n",
      "2017-03-19T17:25:29.136937: step 659, loss 0.532608, acc 0.765625\n",
      "2017-03-19T17:25:30.018657: step 660, loss 0.462874, acc 0.796875\n",
      "2017-03-19T17:25:30.846141: step 661, loss 0.468728, acc 0.71875\n",
      "2017-03-19T17:25:32.000383: step 662, loss 0.508242, acc 0.828125\n",
      "2017-03-19T17:25:33.029012: step 663, loss 0.522464, acc 0.703125\n",
      "2017-03-19T17:25:34.036643: step 664, loss 0.566307, acc 0.65625\n",
      "2017-03-19T17:25:35.033823: step 665, loss 0.51002, acc 0.75\n",
      "2017-03-19T17:25:36.047440: step 666, loss 0.498242, acc 0.703125\n",
      "2017-03-19T17:25:37.048551: step 667, loss 0.50058, acc 0.71875\n",
      "2017-03-19T17:25:38.019586: step 668, loss 0.51539, acc 0.703125\n",
      "2017-03-19T17:25:39.030759: step 669, loss 0.518297, acc 0.75\n",
      "2017-03-19T17:25:39.871746: step 670, loss 0.698185, acc 0.59375\n",
      "2017-03-19T17:25:40.719769: step 671, loss 0.625878, acc 0.640625\n",
      "2017-03-19T17:25:41.776169: step 672, loss 0.507831, acc 0.765625\n",
      "2017-03-19T17:25:42.775984: step 673, loss 0.513692, acc 0.671875\n",
      "2017-03-19T17:25:43.782094: step 674, loss 0.552804, acc 0.75\n",
      "2017-03-19T17:25:44.792463: step 675, loss 0.517995, acc 0.75\n",
      "2017-03-19T17:25:45.656598: step 676, loss 0.592877, acc 0.609375\n",
      "2017-03-19T17:25:46.531117: step 677, loss 0.526431, acc 0.671875\n",
      "2017-03-19T17:25:47.590872: step 678, loss 0.555807, acc 0.71875\n",
      "2017-03-19T17:25:48.620572: step 679, loss 0.592687, acc 0.71875\n",
      "2017-03-19T17:25:49.651451: step 680, loss 0.547876, acc 0.703125\n",
      "2017-03-19T17:25:50.626646: step 681, loss 0.558256, acc 0.734375\n",
      "2017-03-19T17:25:51.659826: step 682, loss 0.564592, acc 0.671875\n",
      "2017-03-19T17:25:52.504219: step 683, loss 0.51151, acc 0.734375\n",
      "2017-03-19T17:25:53.330374: step 684, loss 0.543668, acc 0.6875\n",
      "2017-03-19T17:25:54.178375: step 685, loss 0.727168, acc 0.625\n",
      "2017-03-19T17:25:55.052893: step 686, loss 0.509463, acc 0.734375\n",
      "2017-03-19T17:25:55.909912: step 687, loss 0.613293, acc 0.75\n",
      "2017-03-19T17:25:56.768472: step 688, loss 0.67417, acc 0.5625\n",
      "2017-03-19T17:25:57.739111: step 689, loss 0.503853, acc 0.6875\n",
      "2017-03-19T17:25:58.759165: step 690, loss 0.639137, acc 0.640625\n",
      "2017-03-19T17:25:59.774967: step 691, loss 0.58072, acc 0.75\n",
      "2017-03-19T17:26:00.761670: step 692, loss 0.470555, acc 0.828125\n",
      "2017-03-19T17:26:01.752855: step 693, loss 0.478843, acc 0.765625\n",
      "2017-03-19T17:26:02.766577: step 694, loss 0.547895, acc 0.6875\n",
      "2017-03-19T17:26:03.767706: step 695, loss 0.512832, acc 0.765625\n",
      "2017-03-19T17:26:04.759797: step 696, loss 0.515606, acc 0.734375\n",
      "2017-03-19T17:26:05.786487: step 697, loss 0.619339, acc 0.734375\n",
      "2017-03-19T17:26:06.798672: step 698, loss 0.547939, acc 0.75\n",
      "2017-03-19T17:26:07.804235: step 699, loss 0.574806, acc 0.71875\n",
      "2017-03-19T17:26:08.676826: step 700, loss 0.564359, acc 0.671875\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:26:12.856774: step 700, loss 0.488425, acc 0.769\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-700\n",
      "\n",
      "2017-03-19T17:26:14.809683: step 701, loss 0.525275, acc 0.671875\n",
      "2017-03-19T17:26:15.760773: step 702, loss 0.502264, acc 0.71875\n",
      "2017-03-19T17:26:16.723410: step 703, loss 0.57328, acc 0.71875\n",
      "2017-03-19T17:26:17.680828: step 704, loss 0.49851, acc 0.75\n",
      "2017-03-19T17:26:18.324187: step 705, loss 0.363622, acc 0.825\n",
      "2017-03-19T17:26:19.313392: step 706, loss 0.493933, acc 0.703125\n",
      "2017-03-19T17:26:20.184011: step 707, loss 0.591779, acc 0.65625\n",
      "2017-03-19T17:26:21.010889: step 708, loss 0.537204, acc 0.796875\n",
      "2017-03-19T17:26:22.142148: step 709, loss 0.462118, acc 0.703125\n",
      "2017-03-19T17:26:23.120187: step 710, loss 0.574238, acc 0.65625\n",
      "2017-03-19T17:26:24.120099: step 711, loss 0.492856, acc 0.78125\n",
      "2017-03-19T17:26:25.043380: step 712, loss 0.577427, acc 0.703125\n",
      "2017-03-19T17:26:25.893986: step 713, loss 0.583591, acc 0.703125\n",
      "2017-03-19T17:26:26.787204: step 714, loss 0.502918, acc 0.78125\n",
      "2017-03-19T17:26:27.731276: step 715, loss 0.476946, acc 0.734375\n",
      "2017-03-19T17:26:28.627914: step 716, loss 0.522615, acc 0.671875\n",
      "2017-03-19T17:26:29.517048: step 717, loss 0.402023, acc 0.78125\n",
      "2017-03-19T17:26:30.374109: step 718, loss 0.488076, acc 0.71875\n",
      "2017-03-19T17:26:31.225730: step 719, loss 0.460035, acc 0.8125\n",
      "2017-03-19T17:26:32.103758: step 720, loss 0.466613, acc 0.703125\n",
      "2017-03-19T17:26:32.974408: step 721, loss 0.489906, acc 0.75\n",
      "2017-03-19T17:26:33.823512: step 722, loss 0.455701, acc 0.78125\n",
      "2017-03-19T17:26:34.674012: step 723, loss 0.563323, acc 0.6875\n",
      "2017-03-19T17:26:35.569650: step 724, loss 0.419382, acc 0.8125\n",
      "2017-03-19T17:26:36.419074: step 725, loss 0.499059, acc 0.71875\n",
      "2017-03-19T17:26:37.274131: step 726, loss 0.442006, acc 0.828125\n",
      "2017-03-19T17:26:38.131802: step 727, loss 0.460789, acc 0.75\n",
      "2017-03-19T17:26:39.015067: step 728, loss 0.452328, acc 0.796875\n",
      "2017-03-19T17:26:40.044302: step 729, loss 0.399244, acc 0.859375\n",
      "2017-03-19T17:26:41.079038: step 730, loss 0.466922, acc 0.734375\n",
      "2017-03-19T17:26:42.069096: step 731, loss 0.427213, acc 0.765625\n",
      "2017-03-19T17:26:43.069885: step 732, loss 0.548548, acc 0.703125\n",
      "2017-03-19T17:26:44.091562: step 733, loss 0.554154, acc 0.71875\n",
      "2017-03-19T17:26:44.964684: step 734, loss 0.51767, acc 0.6875\n",
      "2017-03-19T17:26:45.801599: step 735, loss 0.497786, acc 0.671875\n",
      "2017-03-19T17:26:46.650151: step 736, loss 0.442131, acc 0.828125\n",
      "2017-03-19T17:26:47.528267: step 737, loss 0.380569, acc 0.84375\n",
      "2017-03-19T17:26:48.393279: step 738, loss 0.493479, acc 0.734375\n",
      "2017-03-19T17:26:49.240335: step 739, loss 0.48202, acc 0.8125\n",
      "2017-03-19T17:26:50.090868: step 740, loss 0.428584, acc 0.765625\n",
      "2017-03-19T17:26:51.016476: step 741, loss 0.449012, acc 0.78125\n",
      "2017-03-19T17:26:51.907043: step 742, loss 0.467021, acc 0.734375\n",
      "2017-03-19T17:26:52.777560: step 743, loss 0.416862, acc 0.78125\n",
      "2017-03-19T17:26:53.622666: step 744, loss 0.439275, acc 0.734375\n",
      "2017-03-19T17:26:54.463760: step 745, loss 0.527797, acc 0.765625\n",
      "2017-03-19T17:26:55.317803: step 746, loss 0.45984, acc 0.671875\n",
      "2017-03-19T17:26:56.331026: step 747, loss 0.414478, acc 0.75\n",
      "2017-03-19T17:26:57.368263: step 748, loss 0.5264, acc 0.6875\n",
      "2017-03-19T17:26:58.358846: step 749, loss 0.550786, acc 0.71875\n",
      "2017-03-19T17:26:59.200041: step 750, loss 0.482097, acc 0.71875\n",
      "2017-03-19T17:27:00.043053: step 751, loss 0.518537, acc 0.6875\n",
      "2017-03-19T17:27:00.890557: step 752, loss 0.620116, acc 0.6875\n",
      "2017-03-19T17:27:01.771083: step 753, loss 0.374663, acc 0.8125\n",
      "2017-03-19T17:27:02.802361: step 754, loss 0.574414, acc 0.734375\n",
      "2017-03-19T17:27:03.667582: step 755, loss 0.428522, acc 0.78125\n",
      "2017-03-19T17:27:04.494603: step 756, loss 0.534092, acc 0.71875\n",
      "2017-03-19T17:27:05.349124: step 757, loss 0.399035, acc 0.84375\n",
      "2017-03-19T17:27:06.183167: step 758, loss 0.509088, acc 0.71875\n",
      "2017-03-19T17:27:07.033787: step 759, loss 0.467474, acc 0.734375\n",
      "2017-03-19T17:27:07.888834: step 760, loss 0.393584, acc 0.875\n",
      "2017-03-19T17:27:08.721430: step 761, loss 0.453528, acc 0.703125\n",
      "2017-03-19T17:27:09.608011: step 762, loss 0.551191, acc 0.671875\n",
      "2017-03-19T17:27:10.460574: step 763, loss 0.476296, acc 0.609375\n",
      "2017-03-19T17:27:11.321134: step 764, loss 0.509956, acc 0.6875\n",
      "2017-03-19T17:27:12.191642: step 765, loss 0.47252, acc 0.71875\n",
      "2017-03-19T17:27:13.036150: step 766, loss 0.472322, acc 0.75\n",
      "2017-03-19T17:27:13.863186: step 767, loss 0.41492, acc 0.8125\n",
      "2017-03-19T17:27:14.707710: step 768, loss 0.504588, acc 0.71875\n",
      "2017-03-19T17:27:15.564355: step 769, loss 0.498584, acc 0.796875\n",
      "2017-03-19T17:27:16.397367: step 770, loss 0.636693, acc 0.71875\n",
      "2017-03-19T17:27:17.241917: step 771, loss 0.532068, acc 0.671875\n",
      "2017-03-19T17:27:18.119043: step 772, loss 0.511086, acc 0.75\n",
      "2017-03-19T17:27:18.960595: step 773, loss 0.571685, acc 0.71875\n",
      "2017-03-19T17:27:19.914227: step 774, loss 0.45344, acc 0.78125\n",
      "2017-03-19T17:27:20.955968: step 775, loss 0.529981, acc 0.71875\n",
      "2017-03-19T17:27:21.941671: step 776, loss 0.392588, acc 0.8125\n",
      "2017-03-19T17:27:22.935879: step 777, loss 0.495447, acc 0.765625\n",
      "2017-03-19T17:27:23.954606: step 778, loss 0.404295, acc 0.75\n",
      "2017-03-19T17:27:24.963324: step 779, loss 0.508781, acc 0.75\n",
      "2017-03-19T17:27:25.965037: step 780, loss 0.500261, acc 0.75\n",
      "2017-03-19T17:27:26.952236: step 781, loss 0.561993, acc 0.6875\n",
      "2017-03-19T17:27:27.958985: step 782, loss 0.524884, acc 0.703125\n",
      "2017-03-19T17:27:28.959919: step 783, loss 0.445568, acc 0.75\n",
      "2017-03-19T17:27:29.944547: step 784, loss 0.45717, acc 0.78125\n",
      "2017-03-19T17:27:30.927858: step 785, loss 0.582512, acc 0.75\n",
      "2017-03-19T17:27:31.929488: step 786, loss 0.405615, acc 0.796875\n",
      "2017-03-19T17:27:32.970200: step 787, loss 0.511858, acc 0.703125\n",
      "2017-03-19T17:27:33.985786: step 788, loss 0.523885, acc 0.765625\n",
      "2017-03-19T17:27:34.976262: step 789, loss 0.473218, acc 0.78125\n",
      "2017-03-19T17:27:35.973529: step 790, loss 0.435196, acc 0.765625\n",
      "2017-03-19T17:27:36.963650: step 791, loss 0.490133, acc 0.6875\n",
      "2017-03-19T17:27:37.989278: step 792, loss 0.497868, acc 0.765625\n",
      "2017-03-19T17:27:38.990390: step 793, loss 0.544119, acc 0.640625\n",
      "2017-03-19T17:27:39.973022: step 794, loss 0.4883, acc 0.71875\n",
      "2017-03-19T17:27:40.852300: step 795, loss 0.450846, acc 0.8125\n",
      "2017-03-19T17:27:41.920966: step 796, loss 0.416316, acc 0.734375\n",
      "2017-03-19T17:27:42.938692: step 797, loss 0.561973, acc 0.765625\n",
      "2017-03-19T17:27:43.787739: step 798, loss 0.494681, acc 0.765625\n",
      "2017-03-19T17:27:44.770939: step 799, loss 0.526791, acc 0.6875\n",
      "2017-03-19T17:27:45.773910: step 800, loss 0.540077, acc 0.71875\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:27:50.134924: step 800, loss 0.474976, acc 0.776\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-800\n",
      "\n",
      "2017-03-19T17:27:52.106416: step 801, loss 0.48707, acc 0.8125\n",
      "2017-03-19T17:27:52.924907: step 802, loss 0.614066, acc 0.703125\n",
      "2017-03-19T17:27:53.730444: step 803, loss 0.464877, acc 0.828125\n",
      "2017-03-19T17:27:54.668569: step 804, loss 0.5562, acc 0.671875\n",
      "2017-03-19T17:27:55.490688: step 805, loss 0.447913, acc 0.78125\n",
      "2017-03-19T17:27:56.303169: step 806, loss 0.493504, acc 0.703125\n",
      "2017-03-19T17:27:57.247860: step 807, loss 0.509292, acc 0.78125\n",
      "2017-03-19T17:27:58.079922: step 808, loss 0.454803, acc 0.78125\n",
      "2017-03-19T17:27:58.904830: step 809, loss 0.535037, acc 0.703125\n",
      "2017-03-19T17:27:59.730105: step 810, loss 0.423737, acc 0.796875\n",
      "2017-03-19T17:28:00.557262: step 811, loss 0.441772, acc 0.75\n",
      "2017-03-19T17:28:01.395859: step 812, loss 0.643188, acc 0.65625\n",
      "2017-03-19T17:28:02.241861: step 813, loss 0.470606, acc 0.703125\n",
      "2017-03-19T17:28:03.083008: step 814, loss 0.412952, acc 0.78125\n",
      "2017-03-19T17:28:03.931007: step 815, loss 0.545073, acc 0.65625\n",
      "2017-03-19T17:28:04.847060: step 816, loss 0.480468, acc 0.765625\n",
      "2017-03-19T17:28:05.729688: step 817, loss 0.490606, acc 0.75\n",
      "2017-03-19T17:28:06.635783: step 818, loss 0.566394, acc 0.734375\n",
      "2017-03-19T17:28:07.688533: step 819, loss 0.500882, acc 0.75\n",
      "2017-03-19T17:28:08.825302: step 820, loss 0.547381, acc 0.765625\n",
      "2017-03-19T17:28:09.695938: step 821, loss 0.627736, acc 0.625\n",
      "2017-03-19T17:28:10.538004: step 822, loss 0.705458, acc 0.546875\n",
      "2017-03-19T17:28:11.388069: step 823, loss 0.462885, acc 0.765625\n",
      "2017-03-19T17:28:12.229698: step 824, loss 0.43022, acc 0.796875\n",
      "2017-03-19T17:28:13.154271: step 825, loss 0.471683, acc 0.78125\n",
      "2017-03-19T17:28:14.132965: step 826, loss 0.371467, acc 0.828125\n",
      "2017-03-19T17:28:15.134090: step 827, loss 0.383306, acc 0.796875\n",
      "2017-03-19T17:28:16.081922: step 828, loss 0.396525, acc 0.765625\n",
      "2017-03-19T17:28:16.950491: step 829, loss 0.474314, acc 0.765625\n",
      "2017-03-19T17:28:17.798026: step 830, loss 0.423319, acc 0.75\n",
      "2017-03-19T17:28:18.681583: step 831, loss 0.435988, acc 0.8125\n",
      "2017-03-19T17:28:19.535137: step 832, loss 0.486954, acc 0.75\n",
      "2017-03-19T17:28:20.385653: step 833, loss 0.5241, acc 0.734375\n",
      "2017-03-19T17:28:21.238181: step 834, loss 0.50088, acc 0.703125\n",
      "2017-03-19T17:28:22.106055: step 835, loss 0.515762, acc 0.75\n",
      "2017-03-19T17:28:23.161557: step 836, loss 0.672103, acc 0.640625\n",
      "2017-03-19T17:28:24.053119: step 837, loss 0.49376, acc 0.75\n",
      "2017-03-19T17:28:25.116376: step 838, loss 0.511914, acc 0.765625\n",
      "2017-03-19T17:28:26.022030: step 839, loss 0.467506, acc 0.734375\n",
      "2017-03-19T17:28:26.966992: step 840, loss 0.497101, acc 0.6875\n",
      "2017-03-19T17:28:28.086771: step 841, loss 0.560874, acc 0.78125\n",
      "2017-03-19T17:28:29.075394: step 842, loss 0.426408, acc 0.765625\n",
      "2017-03-19T17:28:30.138652: step 843, loss 0.510715, acc 0.734375\n",
      "2017-03-19T17:28:30.993764: step 844, loss 0.530719, acc 0.734375\n",
      "2017-03-19T17:28:31.860773: step 845, loss 0.445233, acc 0.78125\n",
      "2017-03-19T17:28:32.549676: step 846, loss 0.431668, acc 0.8\n",
      "2017-03-19T17:28:33.628444: step 847, loss 0.439035, acc 0.875\n",
      "2017-03-19T17:28:34.623217: step 848, loss 0.392694, acc 0.796875\n",
      "2017-03-19T17:28:35.604915: step 849, loss 0.571764, acc 0.8125\n",
      "2017-03-19T17:28:36.602076: step 850, loss 0.4233, acc 0.78125\n",
      "2017-03-19T17:28:37.565976: step 851, loss 0.589386, acc 0.625\n",
      "2017-03-19T17:28:38.412885: step 852, loss 0.454018, acc 0.765625\n",
      "2017-03-19T17:28:39.252971: step 853, loss 0.461192, acc 0.796875\n",
      "2017-03-19T17:28:40.344017: step 854, loss 0.467466, acc 0.796875\n",
      "2017-03-19T17:28:41.357740: step 855, loss 0.475361, acc 0.78125\n",
      "2017-03-19T17:28:42.356463: step 856, loss 0.401957, acc 0.8125\n",
      "2017-03-19T17:28:43.345564: step 857, loss 0.474393, acc 0.765625\n",
      "2017-03-19T17:28:44.229707: step 858, loss 0.338413, acc 0.828125\n",
      "2017-03-19T17:28:45.362222: step 859, loss 0.522435, acc 0.671875\n",
      "2017-03-19T17:28:46.377718: step 860, loss 0.437187, acc 0.78125\n",
      "2017-03-19T17:28:47.361368: step 861, loss 0.411047, acc 0.765625\n",
      "2017-03-19T17:28:48.421623: step 862, loss 0.381215, acc 0.84375\n",
      "2017-03-19T17:28:49.405749: step 863, loss 0.440255, acc 0.84375\n",
      "2017-03-19T17:28:50.383402: step 864, loss 0.42435, acc 0.734375\n",
      "2017-03-19T17:28:51.435119: step 865, loss 0.375966, acc 0.859375\n",
      "2017-03-19T17:28:52.449054: step 866, loss 0.446281, acc 0.84375\n",
      "2017-03-19T17:28:53.438693: step 867, loss 0.492757, acc 0.78125\n",
      "2017-03-19T17:28:54.454157: step 868, loss 0.412033, acc 0.8125\n",
      "2017-03-19T17:28:55.479343: step 869, loss 0.387521, acc 0.78125\n",
      "2017-03-19T17:28:56.479181: step 870, loss 0.391422, acc 0.8125\n",
      "2017-03-19T17:28:57.562505: step 871, loss 0.484093, acc 0.796875\n",
      "2017-03-19T17:28:58.586872: step 872, loss 0.486416, acc 0.765625\n",
      "2017-03-19T17:28:59.456001: step 873, loss 0.498959, acc 0.734375\n",
      "2017-03-19T17:29:00.313061: step 874, loss 0.421253, acc 0.90625\n",
      "2017-03-19T17:29:01.186082: step 875, loss 0.406803, acc 0.78125\n",
      "2017-03-19T17:29:02.073664: step 876, loss 0.418761, acc 0.78125\n",
      "2017-03-19T17:29:02.961332: step 877, loss 0.389373, acc 0.859375\n",
      "2017-03-19T17:29:03.848034: step 878, loss 0.466421, acc 0.78125\n",
      "2017-03-19T17:29:04.717738: step 879, loss 0.398037, acc 0.875\n",
      "2017-03-19T17:29:05.565342: step 880, loss 0.555574, acc 0.65625\n",
      "2017-03-19T17:29:06.651464: step 881, loss 0.444438, acc 0.71875\n",
      "2017-03-19T17:29:07.672199: step 882, loss 0.254844, acc 0.921875\n",
      "2017-03-19T17:29:08.557681: step 883, loss 0.486583, acc 0.734375\n",
      "2017-03-19T17:29:09.427356: step 884, loss 0.646764, acc 0.734375\n",
      "2017-03-19T17:29:10.285483: step 885, loss 0.581076, acc 0.671875\n",
      "2017-03-19T17:29:11.130979: step 886, loss 0.438904, acc 0.796875\n",
      "2017-03-19T17:29:12.012089: step 887, loss 0.503501, acc 0.734375\n",
      "2017-03-19T17:29:12.889161: step 888, loss 0.391797, acc 0.78125\n",
      "2017-03-19T17:29:13.869307: step 889, loss 0.255119, acc 0.890625\n",
      "2017-03-19T17:29:14.899519: step 890, loss 0.367926, acc 0.8125\n",
      "2017-03-19T17:29:15.911740: step 891, loss 0.397581, acc 0.828125\n",
      "2017-03-19T17:29:16.772616: step 892, loss 0.55294, acc 0.65625\n",
      "2017-03-19T17:29:17.618667: step 893, loss 0.496102, acc 0.765625\n",
      "2017-03-19T17:29:18.492200: step 894, loss 0.401064, acc 0.8125\n",
      "2017-03-19T17:29:19.320738: step 895, loss 0.47489, acc 0.734375\n",
      "2017-03-19T17:29:20.163302: step 896, loss 0.388162, acc 0.8125\n",
      "2017-03-19T17:29:21.186485: step 897, loss 0.343805, acc 0.765625\n",
      "2017-03-19T17:29:22.172602: step 898, loss 0.424154, acc 0.765625\n",
      "2017-03-19T17:29:23.292902: step 899, loss 0.351925, acc 0.90625\n",
      "2017-03-19T17:29:24.309632: step 900, loss 0.445505, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:29:28.556509: step 900, loss 0.467281, acc 0.78\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-900\n",
      "\n",
      "2017-03-19T17:29:30.713811: step 901, loss 0.513296, acc 0.6875\n",
      "2017-03-19T17:29:31.680400: step 902, loss 0.41394, acc 0.828125\n",
      "2017-03-19T17:29:32.635530: step 903, loss 0.50524, acc 0.703125\n",
      "2017-03-19T17:29:33.582705: step 904, loss 0.343504, acc 0.828125\n",
      "2017-03-19T17:29:34.427307: step 905, loss 0.407301, acc 0.71875\n",
      "2017-03-19T17:29:35.390727: step 906, loss 0.514375, acc 0.734375\n",
      "2017-03-19T17:29:36.203257: step 907, loss 0.403415, acc 0.75\n",
      "2017-03-19T17:29:37.036813: step 908, loss 0.460589, acc 0.765625\n",
      "2017-03-19T17:29:38.041479: step 909, loss 0.446138, acc 0.78125\n",
      "2017-03-19T17:29:39.031193: step 910, loss 0.444111, acc 0.75\n",
      "2017-03-19T17:29:39.992889: step 911, loss 0.421454, acc 0.796875\n",
      "2017-03-19T17:29:41.002537: step 912, loss 0.488482, acc 0.765625\n",
      "2017-03-19T17:29:42.013412: step 913, loss 0.354873, acc 0.8125\n",
      "2017-03-19T17:29:42.897697: step 914, loss 0.562355, acc 0.78125\n",
      "2017-03-19T17:29:43.784329: step 915, loss 0.43414, acc 0.765625\n",
      "2017-03-19T17:29:44.662955: step 916, loss 0.576208, acc 0.6875\n",
      "2017-03-19T17:29:45.554283: step 917, loss 0.584221, acc 0.671875\n",
      "2017-03-19T17:29:46.422882: step 918, loss 0.498228, acc 0.765625\n",
      "2017-03-19T17:29:47.300507: step 919, loss 0.41262, acc 0.734375\n",
      "2017-03-19T17:29:48.279705: step 920, loss 0.364424, acc 0.828125\n",
      "2017-03-19T17:29:49.219374: step 921, loss 0.371588, acc 0.828125\n",
      "2017-03-19T17:29:50.082940: step 922, loss 0.599381, acc 0.671875\n",
      "2017-03-19T17:29:50.963076: step 923, loss 0.409908, acc 0.75\n",
      "2017-03-19T17:29:51.827191: step 924, loss 0.51085, acc 0.75\n",
      "2017-03-19T17:29:52.685251: step 925, loss 0.59313, acc 0.703125\n",
      "2017-03-19T17:29:53.535448: step 926, loss 0.458853, acc 0.75\n",
      "2017-03-19T17:29:54.388518: step 927, loss 0.470187, acc 0.78125\n",
      "2017-03-19T17:29:55.245021: step 928, loss 0.492306, acc 0.75\n",
      "2017-03-19T17:29:56.155118: step 929, loss 0.499514, acc 0.796875\n",
      "2017-03-19T17:29:57.021235: step 930, loss 0.415637, acc 0.75\n",
      "2017-03-19T17:29:57.883818: step 931, loss 0.36371, acc 0.765625\n",
      "2017-03-19T17:29:58.759950: step 932, loss 0.344477, acc 0.90625\n",
      "2017-03-19T17:29:59.632571: step 933, loss 0.541054, acc 0.78125\n",
      "2017-03-19T17:30:00.493633: step 934, loss 0.40443, acc 0.78125\n",
      "2017-03-19T17:30:01.352790: step 935, loss 0.469959, acc 0.75\n",
      "2017-03-19T17:30:02.200343: step 936, loss 0.411954, acc 0.75\n",
      "2017-03-19T17:30:03.037387: step 937, loss 0.545093, acc 0.671875\n",
      "2017-03-19T17:30:03.911020: step 938, loss 0.413376, acc 0.765625\n",
      "2017-03-19T17:30:04.800050: step 939, loss 0.510018, acc 0.796875\n",
      "2017-03-19T17:30:05.699798: step 940, loss 0.444104, acc 0.78125\n",
      "2017-03-19T17:30:06.563928: step 941, loss 0.406283, acc 0.8125\n",
      "2017-03-19T17:30:07.635098: step 942, loss 0.422961, acc 0.765625\n",
      "2017-03-19T17:30:08.634788: step 943, loss 0.305817, acc 0.828125\n",
      "2017-03-19T17:30:09.798708: step 944, loss 0.543731, acc 0.734375\n",
      "2017-03-19T17:30:10.953031: step 945, loss 0.482653, acc 0.78125\n",
      "2017-03-19T17:30:11.952928: step 946, loss 0.490285, acc 0.765625\n",
      "2017-03-19T17:30:12.967100: step 947, loss 0.506534, acc 0.78125\n",
      "2017-03-19T17:30:13.981266: step 948, loss 0.519769, acc 0.75\n",
      "2017-03-19T17:30:15.001108: step 949, loss 0.476484, acc 0.78125\n",
      "2017-03-19T17:30:16.002158: step 950, loss 0.495371, acc 0.75\n",
      "2017-03-19T17:30:17.023213: step 951, loss 0.319902, acc 0.859375\n",
      "2017-03-19T17:30:18.020924: step 952, loss 0.39035, acc 0.78125\n",
      "2017-03-19T17:30:19.067667: step 953, loss 0.492167, acc 0.703125\n",
      "2017-03-19T17:30:20.100833: step 954, loss 0.460484, acc 0.734375\n",
      "2017-03-19T17:30:21.101044: step 955, loss 0.395799, acc 0.796875\n",
      "2017-03-19T17:30:21.971675: step 956, loss 0.473991, acc 0.75\n",
      "2017-03-19T17:30:23.133905: step 957, loss 0.465387, acc 0.75\n",
      "2017-03-19T17:30:24.120161: step 958, loss 0.419516, acc 0.8125\n",
      "2017-03-19T17:30:25.182832: step 959, loss 0.597377, acc 0.703125\n",
      "2017-03-19T17:30:26.282132: step 960, loss 0.402256, acc 0.75\n",
      "2017-03-19T17:30:27.293751: step 961, loss 0.45214, acc 0.75\n",
      "2017-03-19T17:30:28.167833: step 962, loss 0.474386, acc 0.78125\n",
      "2017-03-19T17:30:29.031448: step 963, loss 0.576869, acc 0.671875\n",
      "2017-03-19T17:30:29.914578: step 964, loss 0.455581, acc 0.6875\n",
      "2017-03-19T17:30:30.764303: step 965, loss 0.502809, acc 0.75\n",
      "2017-03-19T17:30:31.651602: step 966, loss 0.425203, acc 0.796875\n",
      "2017-03-19T17:30:32.507607: step 967, loss 0.439113, acc 0.71875\n",
      "2017-03-19T17:30:33.367166: step 968, loss 0.423426, acc 0.84375\n",
      "2017-03-19T17:30:34.222776: step 969, loss 0.399923, acc 0.78125\n",
      "2017-03-19T17:30:35.125369: step 970, loss 0.506425, acc 0.734375\n",
      "2017-03-19T17:30:36.069542: step 971, loss 0.510764, acc 0.734375\n",
      "2017-03-19T17:30:37.167324: step 972, loss 0.511785, acc 0.796875\n",
      "2017-03-19T17:30:38.211178: step 973, loss 0.453614, acc 0.734375\n",
      "2017-03-19T17:30:39.224914: step 974, loss 0.54435, acc 0.640625\n",
      "2017-03-19T17:30:40.271998: step 975, loss 0.430471, acc 0.8125\n",
      "2017-03-19T17:30:41.290669: step 976, loss 0.422609, acc 0.78125\n",
      "2017-03-19T17:30:42.304369: step 977, loss 0.499504, acc 0.71875\n",
      "2017-03-19T17:30:43.322354: step 978, loss 0.492659, acc 0.765625\n",
      "2017-03-19T17:30:44.200062: step 979, loss 0.50589, acc 0.765625\n",
      "2017-03-19T17:30:45.164163: step 980, loss 0.466498, acc 0.75\n",
      "2017-03-19T17:30:46.180834: step 981, loss 0.452042, acc 0.75\n",
      "2017-03-19T17:30:47.180549: step 982, loss 0.429274, acc 0.796875\n",
      "2017-03-19T17:30:48.203769: step 983, loss 0.44633, acc 0.71875\n",
      "2017-03-19T17:30:49.065350: step 984, loss 0.482838, acc 0.703125\n",
      "2017-03-19T17:30:49.917993: step 985, loss 0.471811, acc 0.765625\n",
      "2017-03-19T17:30:50.834146: step 986, loss 0.364824, acc 0.8125\n",
      "2017-03-19T17:30:51.558297: step 987, loss 0.447766, acc 0.825\n",
      "2017-03-19T17:30:52.722054: step 988, loss 0.362261, acc 0.890625\n",
      "2017-03-19T17:30:53.674189: step 989, loss 0.444873, acc 0.75\n",
      "2017-03-19T17:30:54.712829: step 990, loss 0.315855, acc 0.8125\n",
      "2017-03-19T17:30:55.717494: step 991, loss 0.408474, acc 0.828125\n",
      "2017-03-19T17:30:56.718173: step 992, loss 0.283895, acc 0.875\n",
      "2017-03-19T17:30:57.732690: step 993, loss 0.407271, acc 0.75\n",
      "2017-03-19T17:30:58.741476: step 994, loss 0.356317, acc 0.828125\n",
      "2017-03-19T17:30:59.751213: step 995, loss 0.454978, acc 0.75\n",
      "2017-03-19T17:31:00.767836: step 996, loss 0.429963, acc 0.734375\n",
      "2017-03-19T17:31:01.788782: step 997, loss 0.295098, acc 0.921875\n",
      "2017-03-19T17:31:02.814828: step 998, loss 0.427889, acc 0.734375\n",
      "2017-03-19T17:31:03.822882: step 999, loss 0.476565, acc 0.71875\n",
      "2017-03-19T17:31:04.821066: step 1000, loss 0.426882, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:31:09.496317: step 1000, loss 0.482921, acc 0.767\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1000\n",
      "\n",
      "2017-03-19T17:31:11.775999: step 1001, loss 0.38513, acc 0.8125\n",
      "2017-03-19T17:31:12.706161: step 1002, loss 0.417931, acc 0.78125\n",
      "2017-03-19T17:31:13.789933: step 1003, loss 0.313844, acc 0.796875\n",
      "2017-03-19T17:31:14.670856: step 1004, loss 0.345213, acc 0.859375\n",
      "2017-03-19T17:31:15.847198: step 1005, loss 0.324268, acc 0.859375\n",
      "2017-03-19T17:31:16.823908: step 1006, loss 0.317897, acc 0.890625\n",
      "2017-03-19T17:31:17.802071: step 1007, loss 0.408519, acc 0.78125\n",
      "2017-03-19T17:31:18.661698: step 1008, loss 0.419075, acc 0.796875\n",
      "2017-03-19T17:31:19.774604: step 1009, loss 0.360946, acc 0.84375\n",
      "2017-03-19T17:31:20.906114: step 1010, loss 0.434407, acc 0.75\n",
      "2017-03-19T17:31:22.008399: step 1011, loss 0.332318, acc 0.796875\n",
      "2017-03-19T17:31:23.050642: step 1012, loss 0.36061, acc 0.859375\n",
      "2017-03-19T17:31:23.948740: step 1013, loss 0.422321, acc 0.765625\n",
      "2017-03-19T17:31:24.807283: step 1014, loss 0.417137, acc 0.71875\n",
      "2017-03-19T17:31:25.660856: step 1015, loss 0.342969, acc 0.8125\n",
      "2017-03-19T17:31:26.493801: step 1016, loss 0.352356, acc 0.828125\n",
      "2017-03-19T17:31:27.347546: step 1017, loss 0.482409, acc 0.796875\n",
      "2017-03-19T17:31:28.212110: step 1018, loss 0.32906, acc 0.859375\n",
      "2017-03-19T17:31:29.061674: step 1019, loss 0.428971, acc 0.78125\n",
      "2017-03-19T17:31:29.941996: step 1020, loss 0.357221, acc 0.84375\n",
      "2017-03-19T17:31:30.815568: step 1021, loss 0.372372, acc 0.796875\n",
      "2017-03-19T17:31:31.680131: step 1022, loss 0.471965, acc 0.734375\n",
      "2017-03-19T17:31:32.678295: step 1023, loss 0.327635, acc 0.828125\n",
      "2017-03-19T17:31:33.718599: step 1024, loss 0.34916, acc 0.8125\n",
      "2017-03-19T17:31:34.727273: step 1025, loss 0.319883, acc 0.828125\n",
      "2017-03-19T17:31:35.729822: step 1026, loss 0.415354, acc 0.71875\n",
      "2017-03-19T17:31:36.881642: step 1027, loss 0.414385, acc 0.765625\n",
      "2017-03-19T17:31:37.802332: step 1028, loss 0.444663, acc 0.765625\n",
      "2017-03-19T17:31:38.978369: step 1029, loss 0.435507, acc 0.78125\n",
      "2017-03-19T17:31:40.155920: step 1030, loss 0.406647, acc 0.828125\n",
      "2017-03-19T17:31:41.259706: step 1031, loss 0.382037, acc 0.796875\n",
      "2017-03-19T17:31:42.277499: step 1032, loss 0.3307, acc 0.8125\n",
      "2017-03-19T17:31:43.400299: step 1033, loss 0.308417, acc 0.859375\n",
      "2017-03-19T17:31:44.470783: step 1034, loss 0.30878, acc 0.796875\n",
      "2017-03-19T17:31:45.556569: step 1035, loss 0.335753, acc 0.8125\n",
      "2017-03-19T17:31:46.597528: step 1036, loss 0.487268, acc 0.75\n",
      "2017-03-19T17:31:47.654280: step 1037, loss 0.352899, acc 0.8125\n",
      "2017-03-19T17:31:48.660983: step 1038, loss 0.339924, acc 0.828125\n",
      "2017-03-19T17:31:49.737751: step 1039, loss 0.418676, acc 0.796875\n",
      "2017-03-19T17:31:50.733919: step 1040, loss 0.400844, acc 0.796875\n",
      "2017-03-19T17:31:51.782513: step 1041, loss 0.521033, acc 0.734375\n",
      "2017-03-19T17:31:52.752704: step 1042, loss 0.456368, acc 0.78125\n",
      "2017-03-19T17:31:53.749999: step 1043, loss 0.385763, acc 0.828125\n",
      "2017-03-19T17:31:54.751677: step 1044, loss 0.35402, acc 0.828125\n",
      "2017-03-19T17:31:55.866466: step 1045, loss 0.424988, acc 0.765625\n",
      "2017-03-19T17:31:56.882117: step 1046, loss 0.343844, acc 0.84375\n",
      "2017-03-19T17:31:57.861930: step 1047, loss 0.47635, acc 0.71875\n",
      "2017-03-19T17:31:58.742323: step 1048, loss 0.377354, acc 0.78125\n",
      "2017-03-19T17:31:59.875036: step 1049, loss 0.297552, acc 0.90625\n",
      "2017-03-19T17:32:00.881309: step 1050, loss 0.552981, acc 0.671875\n",
      "2017-03-19T17:32:01.892956: step 1051, loss 0.306751, acc 0.828125\n",
      "2017-03-19T17:32:02.787425: step 1052, loss 0.36493, acc 0.859375\n",
      "2017-03-19T17:32:03.748136: step 1053, loss 0.419036, acc 0.8125\n",
      "2017-03-19T17:32:04.756856: step 1054, loss 0.335678, acc 0.859375\n",
      "2017-03-19T17:32:05.650885: step 1055, loss 0.36293, acc 0.78125\n",
      "2017-03-19T17:32:06.514015: step 1056, loss 0.387293, acc 0.765625\n",
      "2017-03-19T17:32:07.372671: step 1057, loss 0.401634, acc 0.84375\n",
      "2017-03-19T17:32:08.213934: step 1058, loss 0.470696, acc 0.703125\n",
      "2017-03-19T17:32:09.063955: step 1059, loss 0.568352, acc 0.734375\n",
      "2017-03-19T17:32:09.913010: step 1060, loss 0.323526, acc 0.828125\n",
      "2017-03-19T17:32:10.762062: step 1061, loss 0.405375, acc 0.78125\n",
      "2017-03-19T17:32:11.619144: step 1062, loss 0.410472, acc 0.75\n",
      "2017-03-19T17:32:12.651871: step 1063, loss 0.411859, acc 0.828125\n",
      "2017-03-19T17:32:13.689016: step 1064, loss 0.328678, acc 0.84375\n",
      "2017-03-19T17:32:14.693944: step 1065, loss 0.32272, acc 0.78125\n",
      "2017-03-19T17:32:15.746333: step 1066, loss 0.474801, acc 0.734375\n",
      "2017-03-19T17:32:16.771555: step 1067, loss 0.412748, acc 0.734375\n",
      "2017-03-19T17:32:17.791277: step 1068, loss 0.403998, acc 0.828125\n",
      "2017-03-19T17:32:18.815487: step 1069, loss 0.415025, acc 0.796875\n",
      "2017-03-19T17:32:19.842187: step 1070, loss 0.342604, acc 0.828125\n",
      "2017-03-19T17:32:20.880432: step 1071, loss 0.395988, acc 0.8125\n",
      "2017-03-19T17:32:21.903897: step 1072, loss 0.431337, acc 0.796875\n",
      "2017-03-19T17:32:22.890996: step 1073, loss 0.367802, acc 0.8125\n",
      "2017-03-19T17:32:23.794666: step 1074, loss 0.408438, acc 0.765625\n",
      "2017-03-19T17:32:24.996465: step 1075, loss 0.341337, acc 0.78125\n",
      "2017-03-19T17:32:26.034657: step 1076, loss 0.304847, acc 0.875\n",
      "2017-03-19T17:32:27.050978: step 1077, loss 0.438217, acc 0.796875\n",
      "2017-03-19T17:32:28.060724: step 1078, loss 0.49504, acc 0.828125\n",
      "2017-03-19T17:32:29.085640: step 1079, loss 0.318853, acc 0.875\n",
      "2017-03-19T17:32:30.075241: step 1080, loss 0.354972, acc 0.84375\n",
      "2017-03-19T17:32:31.089631: step 1081, loss 0.559167, acc 0.734375\n",
      "2017-03-19T17:32:32.093793: step 1082, loss 0.314606, acc 0.890625\n",
      "2017-03-19T17:32:33.121949: step 1083, loss 0.435705, acc 0.75\n",
      "2017-03-19T17:32:34.192712: step 1084, loss 0.411593, acc 0.84375\n",
      "2017-03-19T17:32:35.286470: step 1085, loss 0.407894, acc 0.78125\n",
      "2017-03-19T17:32:36.307196: step 1086, loss 0.47476, acc 0.703125\n",
      "2017-03-19T17:32:37.346887: step 1087, loss 0.476249, acc 0.75\n",
      "2017-03-19T17:32:38.328553: step 1088, loss 0.44443, acc 0.71875\n",
      "2017-03-19T17:32:39.357352: step 1089, loss 0.451855, acc 0.734375\n",
      "2017-03-19T17:32:40.441155: step 1090, loss 0.439138, acc 0.78125\n",
      "2017-03-19T17:32:41.509417: step 1091, loss 0.388046, acc 0.8125\n",
      "2017-03-19T17:32:42.576177: step 1092, loss 0.390211, acc 0.828125\n",
      "2017-03-19T17:32:43.596205: step 1093, loss 0.399338, acc 0.8125\n",
      "2017-03-19T17:32:44.605953: step 1094, loss 0.426024, acc 0.78125\n",
      "2017-03-19T17:32:45.624790: step 1095, loss 0.409187, acc 0.796875\n",
      "2017-03-19T17:32:46.611509: step 1096, loss 0.414864, acc 0.828125\n",
      "2017-03-19T17:32:47.500727: step 1097, loss 0.540665, acc 0.703125\n",
      "2017-03-19T17:32:48.380344: step 1098, loss 0.317559, acc 0.828125\n",
      "2017-03-19T17:32:49.242890: step 1099, loss 0.321857, acc 0.84375\n",
      "2017-03-19T17:32:50.222539: step 1100, loss 0.457985, acc 0.75\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:32:54.646376: step 1100, loss 0.484975, acc 0.759\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1100\n",
      "\n",
      "2017-03-19T17:32:57.151098: step 1101, loss 0.360847, acc 0.8125\n",
      "2017-03-19T17:32:58.261388: step 1102, loss 0.429788, acc 0.765625\n",
      "2017-03-19T17:32:59.267176: step 1103, loss 0.360748, acc 0.78125\n",
      "2017-03-19T17:33:00.245731: step 1104, loss 0.505114, acc 0.671875\n",
      "2017-03-19T17:33:01.213871: step 1105, loss 0.455346, acc 0.796875\n",
      "2017-03-19T17:33:02.190567: step 1106, loss 0.464651, acc 0.734375\n",
      "2017-03-19T17:33:03.039352: step 1107, loss 0.408955, acc 0.84375\n",
      "2017-03-19T17:33:03.967685: step 1108, loss 0.37043, acc 0.84375\n",
      "2017-03-19T17:33:05.006926: step 1109, loss 0.469275, acc 0.734375\n",
      "2017-03-19T17:33:06.010665: step 1110, loss 0.52985, acc 0.703125\n",
      "2017-03-19T17:33:07.014803: step 1111, loss 0.453877, acc 0.765625\n",
      "2017-03-19T17:33:08.065213: step 1112, loss 0.506607, acc 0.796875\n",
      "2017-03-19T17:33:09.116919: step 1113, loss 0.401058, acc 0.8125\n",
      "2017-03-19T17:33:10.170281: step 1114, loss 0.459211, acc 0.765625\n",
      "2017-03-19T17:33:11.205526: step 1115, loss 0.383837, acc 0.796875\n",
      "2017-03-19T17:33:12.086261: step 1116, loss 0.321927, acc 0.84375\n",
      "2017-03-19T17:33:13.084929: step 1117, loss 0.402109, acc 0.8125\n",
      "2017-03-19T17:33:14.121536: step 1118, loss 0.505266, acc 0.8125\n",
      "2017-03-19T17:33:15.002582: step 1119, loss 0.536955, acc 0.75\n",
      "2017-03-19T17:33:16.130877: step 1120, loss 0.302783, acc 0.875\n",
      "2017-03-19T17:33:17.149123: step 1121, loss 0.415462, acc 0.796875\n",
      "2017-03-19T17:33:18.206099: step 1122, loss 0.46204, acc 0.765625\n",
      "2017-03-19T17:33:19.252344: step 1123, loss 0.400234, acc 0.796875\n",
      "2017-03-19T17:33:20.288581: step 1124, loss 0.462111, acc 0.765625\n",
      "2017-03-19T17:33:21.274179: step 1125, loss 0.507721, acc 0.765625\n",
      "2017-03-19T17:33:22.253629: step 1126, loss 0.371581, acc 0.78125\n",
      "2017-03-19T17:33:23.099202: step 1127, loss 0.380651, acc 0.796875\n",
      "2017-03-19T17:33:23.712541: step 1128, loss 0.502301, acc 0.725\n",
      "2017-03-19T17:33:24.556590: step 1129, loss 0.312062, acc 0.875\n",
      "2017-03-19T17:33:25.424157: step 1130, loss 0.38071, acc 0.78125\n",
      "2017-03-19T17:33:26.299675: step 1131, loss 0.403656, acc 0.8125\n",
      "2017-03-19T17:33:27.152346: step 1132, loss 0.48595, acc 0.75\n",
      "2017-03-19T17:33:28.017426: step 1133, loss 0.317357, acc 0.8125\n",
      "2017-03-19T17:33:28.878987: step 1134, loss 0.321805, acc 0.84375\n",
      "2017-03-19T17:33:29.748193: step 1135, loss 0.315994, acc 0.8125\n",
      "2017-03-19T17:33:30.618712: step 1136, loss 0.288333, acc 0.90625\n",
      "2017-03-19T17:33:31.514797: step 1137, loss 0.592708, acc 0.703125\n",
      "2017-03-19T17:33:32.396907: step 1138, loss 0.352561, acc 0.765625\n",
      "2017-03-19T17:33:33.245511: step 1139, loss 0.289501, acc 0.921875\n",
      "2017-03-19T17:33:34.151337: step 1140, loss 0.354354, acc 0.71875\n",
      "2017-03-19T17:33:35.244617: step 1141, loss 0.446993, acc 0.8125\n",
      "2017-03-19T17:33:36.272949: step 1142, loss 0.308041, acc 0.859375\n",
      "2017-03-19T17:33:37.304288: step 1143, loss 0.36114, acc 0.796875\n",
      "2017-03-19T17:33:38.314808: step 1144, loss 0.452398, acc 0.75\n",
      "2017-03-19T17:33:39.346816: step 1145, loss 0.531089, acc 0.734375\n",
      "2017-03-19T17:33:40.399565: step 1146, loss 0.36982, acc 0.8125\n",
      "2017-03-19T17:33:41.409786: step 1147, loss 0.367947, acc 0.828125\n",
      "2017-03-19T17:33:42.419990: step 1148, loss 0.314071, acc 0.84375\n",
      "2017-03-19T17:33:43.430709: step 1149, loss 0.288165, acc 0.84375\n",
      "2017-03-19T17:33:44.467747: step 1150, loss 0.402909, acc 0.796875\n",
      "2017-03-19T17:33:45.480223: step 1151, loss 0.471565, acc 0.75\n",
      "2017-03-19T17:33:46.494062: step 1152, loss 0.36734, acc 0.859375\n",
      "2017-03-19T17:33:47.371543: step 1153, loss 0.462861, acc 0.75\n",
      "2017-03-19T17:33:48.259136: step 1154, loss 0.388001, acc 0.796875\n",
      "2017-03-19T17:33:49.161227: step 1155, loss 0.317149, acc 0.828125\n",
      "2017-03-19T17:33:50.332060: step 1156, loss 0.27884, acc 0.828125\n",
      "2017-03-19T17:33:51.381817: step 1157, loss 0.413138, acc 0.71875\n",
      "2017-03-19T17:33:52.272853: step 1158, loss 0.376824, acc 0.859375\n",
      "2017-03-19T17:33:53.238239: step 1159, loss 0.282472, acc 0.84375\n",
      "2017-03-19T17:33:54.195905: step 1160, loss 0.358651, acc 0.828125\n",
      "2017-03-19T17:33:55.066975: step 1161, loss 0.326804, acc 0.796875\n",
      "2017-03-19T17:33:55.933491: step 1162, loss 0.389955, acc 0.734375\n",
      "2017-03-19T17:33:56.810628: step 1163, loss 0.356128, acc 0.84375\n",
      "2017-03-19T17:33:57.675193: step 1164, loss 0.497399, acc 0.734375\n",
      "2017-03-19T17:33:58.713932: step 1165, loss 0.370647, acc 0.8125\n",
      "2017-03-19T17:33:59.702094: step 1166, loss 0.352392, acc 0.859375\n",
      "2017-03-19T17:34:00.803878: step 1167, loss 0.361443, acc 0.828125\n",
      "2017-03-19T17:34:01.953782: step 1168, loss 0.499877, acc 0.734375\n",
      "2017-03-19T17:34:03.102109: step 1169, loss 0.297096, acc 0.875\n",
      "2017-03-19T17:34:04.142255: step 1170, loss 0.414515, acc 0.78125\n",
      "2017-03-19T17:34:05.182470: step 1171, loss 0.468645, acc 0.78125\n",
      "2017-03-19T17:34:06.066997: step 1172, loss 0.271389, acc 0.859375\n",
      "2017-03-19T17:34:06.941568: step 1173, loss 0.357362, acc 0.859375\n",
      "2017-03-19T17:34:07.796176: step 1174, loss 0.394664, acc 0.78125\n",
      "2017-03-19T17:34:08.665476: step 1175, loss 0.258907, acc 0.8125\n",
      "2017-03-19T17:34:09.534690: step 1176, loss 0.328307, acc 0.890625\n",
      "2017-03-19T17:34:10.421783: step 1177, loss 0.431169, acc 0.796875\n",
      "2017-03-19T17:34:11.331896: step 1178, loss 0.409896, acc 0.75\n",
      "2017-03-19T17:34:12.233487: step 1179, loss 0.274309, acc 0.890625\n",
      "2017-03-19T17:34:13.342342: step 1180, loss 0.461082, acc 0.796875\n",
      "2017-03-19T17:34:14.364988: step 1181, loss 0.466556, acc 0.78125\n",
      "2017-03-19T17:34:15.219049: step 1182, loss 0.346779, acc 0.828125\n",
      "2017-03-19T17:34:16.077691: step 1183, loss 0.400789, acc 0.796875\n",
      "2017-03-19T17:34:16.946959: step 1184, loss 0.323373, acc 0.8125\n",
      "2017-03-19T17:34:17.823046: step 1185, loss 0.437322, acc 0.703125\n",
      "2017-03-19T17:34:18.707625: step 1186, loss 0.422903, acc 0.765625\n",
      "2017-03-19T17:34:19.577730: step 1187, loss 0.333972, acc 0.875\n",
      "2017-03-19T17:34:20.426258: step 1188, loss 0.335833, acc 0.84375\n",
      "2017-03-19T17:34:21.325912: step 1189, loss 0.249242, acc 0.875\n",
      "2017-03-19T17:34:22.318621: step 1190, loss 0.492281, acc 0.671875\n",
      "2017-03-19T17:34:23.356360: step 1191, loss 0.223249, acc 0.875\n",
      "2017-03-19T17:34:24.240167: step 1192, loss 0.372561, acc 0.75\n",
      "2017-03-19T17:34:25.265294: step 1193, loss 0.23457, acc 0.96875\n",
      "2017-03-19T17:34:26.338743: step 1194, loss 0.408128, acc 0.734375\n",
      "2017-03-19T17:34:27.398887: step 1195, loss 0.356044, acc 0.796875\n",
      "2017-03-19T17:34:28.438367: step 1196, loss 0.445963, acc 0.796875\n",
      "2017-03-19T17:34:29.344391: step 1197, loss 0.366441, acc 0.859375\n",
      "2017-03-19T17:34:30.275109: step 1198, loss 0.280328, acc 0.890625\n",
      "2017-03-19T17:34:31.328825: step 1199, loss 0.461087, acc 0.734375\n",
      "2017-03-19T17:34:32.345049: step 1200, loss 0.351285, acc 0.8125\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:34:36.775230: step 1200, loss 0.472298, acc 0.789\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1200\n",
      "\n",
      "2017-03-19T17:34:38.635954: step 1201, loss 0.334408, acc 0.859375\n",
      "2017-03-19T17:34:39.474551: step 1202, loss 0.392809, acc 0.828125\n",
      "2017-03-19T17:34:40.300090: step 1203, loss 0.470969, acc 0.84375\n",
      "2017-03-19T17:34:41.207237: step 1204, loss 0.310189, acc 0.828125\n",
      "2017-03-19T17:34:42.075807: step 1205, loss 0.424401, acc 0.90625\n",
      "2017-03-19T17:34:42.924412: step 1206, loss 0.37848, acc 0.796875\n",
      "2017-03-19T17:34:44.036910: step 1207, loss 0.404595, acc 0.78125\n",
      "2017-03-19T17:34:45.152705: step 1208, loss 0.405882, acc 0.796875\n",
      "2017-03-19T17:34:46.052600: step 1209, loss 0.365937, acc 0.8125\n",
      "2017-03-19T17:34:46.894839: step 1210, loss 0.292968, acc 0.890625\n",
      "2017-03-19T17:34:47.920979: step 1211, loss 0.463736, acc 0.75\n",
      "2017-03-19T17:34:48.923193: step 1212, loss 0.378011, acc 0.75\n",
      "2017-03-19T17:34:49.822835: step 1213, loss 0.387806, acc 0.71875\n",
      "2017-03-19T17:34:50.707413: step 1214, loss 0.380408, acc 0.796875\n",
      "2017-03-19T17:34:51.651984: step 1215, loss 0.34009, acc 0.875\n",
      "2017-03-19T17:34:52.552625: step 1216, loss 0.363495, acc 0.84375\n",
      "2017-03-19T17:34:53.539224: step 1217, loss 0.304142, acc 0.859375\n",
      "2017-03-19T17:34:54.548727: step 1218, loss 0.371318, acc 0.734375\n",
      "2017-03-19T17:34:55.444797: step 1219, loss 0.370089, acc 0.765625\n",
      "2017-03-19T17:34:56.360897: step 1220, loss 0.377103, acc 0.796875\n",
      "2017-03-19T17:34:57.234256: step 1221, loss 0.247743, acc 0.875\n",
      "2017-03-19T17:34:58.117401: step 1222, loss 0.407877, acc 0.75\n",
      "2017-03-19T17:34:58.994958: step 1223, loss 0.307288, acc 0.828125\n",
      "2017-03-19T17:34:59.883505: step 1224, loss 0.413214, acc 0.796875\n",
      "2017-03-19T17:35:00.761579: step 1225, loss 0.385262, acc 0.75\n",
      "2017-03-19T17:35:01.663676: step 1226, loss 0.424041, acc 0.78125\n",
      "2017-03-19T17:35:02.748947: step 1227, loss 0.319799, acc 0.796875\n",
      "2017-03-19T17:35:03.645543: step 1228, loss 0.436044, acc 0.84375\n",
      "2017-03-19T17:35:04.511608: step 1229, loss 0.307237, acc 0.875\n",
      "2017-03-19T17:35:05.382391: step 1230, loss 0.356675, acc 0.796875\n",
      "2017-03-19T17:35:06.250133: step 1231, loss 0.368796, acc 0.796875\n",
      "2017-03-19T17:35:07.144727: step 1232, loss 0.321591, acc 0.859375\n",
      "2017-03-19T17:35:08.025781: step 1233, loss 0.326633, acc 0.796875\n",
      "2017-03-19T17:35:09.071565: step 1234, loss 0.358171, acc 0.875\n",
      "2017-03-19T17:35:10.065933: step 1235, loss 0.478721, acc 0.765625\n",
      "2017-03-19T17:35:11.081551: step 1236, loss 0.440815, acc 0.765625\n",
      "2017-03-19T17:35:11.999706: step 1237, loss 0.327226, acc 0.859375\n",
      "2017-03-19T17:35:13.082130: step 1238, loss 0.557601, acc 0.640625\n",
      "2017-03-19T17:35:13.967725: step 1239, loss 0.406862, acc 0.78125\n",
      "2017-03-19T17:35:14.852802: step 1240, loss 0.342039, acc 0.828125\n",
      "2017-03-19T17:35:15.728823: step 1241, loss 0.538454, acc 0.71875\n",
      "2017-03-19T17:35:16.595957: step 1242, loss 0.405381, acc 0.78125\n",
      "2017-03-19T17:35:17.764696: step 1243, loss 0.38957, acc 0.8125\n",
      "2017-03-19T17:35:18.685234: step 1244, loss 0.453786, acc 0.8125\n",
      "2017-03-19T17:35:19.584250: step 1245, loss 0.431251, acc 0.78125\n",
      "2017-03-19T17:35:20.574852: step 1246, loss 0.453342, acc 0.78125\n",
      "2017-03-19T17:35:21.467259: step 1247, loss 0.321243, acc 0.9375\n",
      "2017-03-19T17:35:22.357528: step 1248, loss 0.395138, acc 0.765625\n",
      "2017-03-19T17:35:23.271579: step 1249, loss 0.373512, acc 0.84375\n",
      "2017-03-19T17:35:24.215716: step 1250, loss 0.395876, acc 0.765625\n",
      "2017-03-19T17:35:25.108364: step 1251, loss 0.438991, acc 0.765625\n",
      "2017-03-19T17:35:25.971889: step 1252, loss 0.394263, acc 0.8125\n",
      "2017-03-19T17:35:26.839421: step 1253, loss 0.364698, acc 0.78125\n",
      "2017-03-19T17:35:27.727121: step 1254, loss 0.367895, acc 0.8125\n",
      "2017-03-19T17:35:28.571200: step 1255, loss 0.431154, acc 0.765625\n",
      "2017-03-19T17:35:29.432837: step 1256, loss 0.419303, acc 0.796875\n",
      "2017-03-19T17:35:30.313596: step 1257, loss 0.278262, acc 0.859375\n",
      "2017-03-19T17:35:31.199121: step 1258, loss 0.37884, acc 0.828125\n",
      "2017-03-19T17:35:32.067127: step 1259, loss 0.403789, acc 0.8125\n",
      "2017-03-19T17:35:32.970184: step 1260, loss 0.497584, acc 0.703125\n",
      "2017-03-19T17:35:33.834419: step 1261, loss 0.527449, acc 0.75\n",
      "2017-03-19T17:35:34.718965: step 1262, loss 0.345817, acc 0.78125\n",
      "2017-03-19T17:35:35.605098: step 1263, loss 0.3649, acc 0.765625\n",
      "2017-03-19T17:35:36.658834: step 1264, loss 0.364243, acc 0.765625\n",
      "2017-03-19T17:35:37.540563: step 1265, loss 0.43693, acc 0.765625\n",
      "2017-03-19T17:35:38.395192: step 1266, loss 0.377342, acc 0.78125\n",
      "2017-03-19T17:35:39.256319: step 1267, loss 0.343685, acc 0.8125\n",
      "2017-03-19T17:35:40.125404: step 1268, loss 0.377931, acc 0.84375\n",
      "2017-03-19T17:35:40.735420: step 1269, loss 0.272853, acc 0.85\n",
      "2017-03-19T17:35:41.618211: step 1270, loss 0.29993, acc 0.875\n",
      "2017-03-19T17:35:42.518249: step 1271, loss 0.302332, acc 0.828125\n",
      "2017-03-19T17:35:43.390330: step 1272, loss 0.278928, acc 0.921875\n",
      "2017-03-19T17:35:44.241884: step 1273, loss 0.441677, acc 0.78125\n",
      "2017-03-19T17:35:45.324548: step 1274, loss 0.440631, acc 0.75\n",
      "2017-03-19T17:35:46.325314: step 1275, loss 0.281585, acc 0.875\n",
      "2017-03-19T17:35:47.220796: step 1276, loss 0.296098, acc 0.78125\n",
      "2017-03-19T17:35:48.174169: step 1277, loss 0.35087, acc 0.78125\n",
      "2017-03-19T17:35:49.231814: step 1278, loss 0.368266, acc 0.796875\n",
      "2017-03-19T17:35:50.150468: step 1279, loss 0.387673, acc 0.828125\n",
      "2017-03-19T17:35:51.261761: step 1280, loss 0.518031, acc 0.734375\n",
      "2017-03-19T17:35:52.313307: step 1281, loss 0.432321, acc 0.796875\n",
      "2017-03-19T17:35:53.402582: step 1282, loss 0.334019, acc 0.90625\n",
      "2017-03-19T17:35:54.276148: step 1283, loss 0.319454, acc 0.859375\n",
      "2017-03-19T17:35:55.394736: step 1284, loss 0.331924, acc 0.859375\n",
      "2017-03-19T17:35:56.323912: step 1285, loss 0.344573, acc 0.828125\n",
      "2017-03-19T17:35:57.313622: step 1286, loss 0.438389, acc 0.734375\n",
      "2017-03-19T17:35:58.214264: step 1287, loss 0.224142, acc 0.90625\n",
      "2017-03-19T17:35:59.327056: step 1288, loss 0.20338, acc 0.875\n",
      "2017-03-19T17:36:00.377496: step 1289, loss 0.511302, acc 0.71875\n",
      "2017-03-19T17:36:01.529817: step 1290, loss 0.339671, acc 0.859375\n",
      "2017-03-19T17:36:02.552217: step 1291, loss 0.312287, acc 0.84375\n",
      "2017-03-19T17:36:03.564953: step 1292, loss 0.359139, acc 0.796875\n",
      "2017-03-19T17:36:04.581589: step 1293, loss 0.316105, acc 0.921875\n",
      "2017-03-19T17:36:05.646340: step 1294, loss 0.307903, acc 0.875\n",
      "2017-03-19T17:36:06.681635: step 1295, loss 0.305674, acc 0.890625\n",
      "2017-03-19T17:36:07.555651: step 1296, loss 0.315388, acc 0.859375\n",
      "2017-03-19T17:36:08.640389: step 1297, loss 0.278934, acc 0.890625\n",
      "2017-03-19T17:36:09.648295: step 1298, loss 0.438533, acc 0.734375\n",
      "2017-03-19T17:36:10.682029: step 1299, loss 0.279597, acc 0.828125\n",
      "2017-03-19T17:36:11.694700: step 1300, loss 0.47682, acc 0.734375\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:36:16.222427: step 1300, loss 0.464648, acc 0.79\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1300\n",
      "\n",
      "2017-03-19T17:36:18.452677: step 1301, loss 0.404801, acc 0.796875\n",
      "2017-03-19T17:36:19.464897: step 1302, loss 0.252401, acc 0.921875\n",
      "2017-03-19T17:36:20.493130: step 1303, loss 0.305129, acc 0.84375\n",
      "2017-03-19T17:36:21.599919: step 1304, loss 0.385542, acc 0.765625\n",
      "2017-03-19T17:36:22.606064: step 1305, loss 0.415454, acc 0.796875\n",
      "2017-03-19T17:36:23.632976: step 1306, loss 0.582207, acc 0.671875\n",
      "2017-03-19T17:36:24.574073: step 1307, loss 0.378367, acc 0.71875\n",
      "2017-03-19T17:36:25.800446: step 1308, loss 0.297019, acc 0.84375\n",
      "2017-03-19T17:36:26.843247: step 1309, loss 0.251295, acc 0.890625\n",
      "2017-03-19T17:36:27.903970: step 1310, loss 0.203958, acc 0.875\n",
      "2017-03-19T17:36:28.938577: step 1311, loss 0.347777, acc 0.828125\n",
      "2017-03-19T17:36:29.947294: step 1312, loss 0.309284, acc 0.828125\n",
      "2017-03-19T17:36:30.848936: step 1313, loss 0.189589, acc 0.90625\n",
      "2017-03-19T17:36:31.989226: step 1314, loss 0.415402, acc 0.734375\n",
      "2017-03-19T17:36:33.009372: step 1315, loss 0.383128, acc 0.78125\n",
      "2017-03-19T17:36:34.001210: step 1316, loss 0.31417, acc 0.84375\n",
      "2017-03-19T17:36:35.012327: step 1317, loss 0.283938, acc 0.8125\n",
      "2017-03-19T17:36:36.016242: step 1318, loss 0.233245, acc 0.921875\n",
      "2017-03-19T17:36:37.038716: step 1319, loss 0.432673, acc 0.796875\n",
      "2017-03-19T17:36:38.009473: step 1320, loss 0.237999, acc 0.875\n",
      "2017-03-19T17:36:38.894232: step 1321, loss 0.235988, acc 0.890625\n",
      "2017-03-19T17:36:39.802977: step 1322, loss 0.291101, acc 0.875\n",
      "2017-03-19T17:36:40.958770: step 1323, loss 0.36246, acc 0.828125\n",
      "2017-03-19T17:36:41.986012: step 1324, loss 0.39424, acc 0.828125\n",
      "2017-03-19T17:36:42.965213: step 1325, loss 0.388066, acc 0.765625\n",
      "2017-03-19T17:36:44.025458: step 1326, loss 0.263549, acc 0.828125\n",
      "2017-03-19T17:36:44.950003: step 1327, loss 0.231378, acc 0.90625\n",
      "2017-03-19T17:36:45.838066: step 1328, loss 0.333389, acc 0.796875\n",
      "2017-03-19T17:36:46.795364: step 1329, loss 0.367602, acc 0.765625\n",
      "2017-03-19T17:36:47.991230: step 1330, loss 0.298047, acc 0.890625\n",
      "2017-03-19T17:36:48.924599: step 1331, loss 0.236571, acc 0.90625\n",
      "2017-03-19T17:36:49.847138: step 1332, loss 0.307219, acc 0.875\n",
      "2017-03-19T17:36:50.931365: step 1333, loss 0.347298, acc 0.8125\n",
      "2017-03-19T17:36:51.864998: step 1334, loss 0.434792, acc 0.71875\n",
      "2017-03-19T17:36:52.728113: step 1335, loss 0.386664, acc 0.765625\n",
      "2017-03-19T17:36:53.590645: step 1336, loss 0.351411, acc 0.875\n",
      "2017-03-19T17:36:54.480714: step 1337, loss 0.345946, acc 0.8125\n",
      "2017-03-19T17:36:55.374764: step 1338, loss 0.289303, acc 0.890625\n",
      "2017-03-19T17:36:56.424998: step 1339, loss 0.273082, acc 0.890625\n",
      "2017-03-19T17:36:57.409696: step 1340, loss 0.24514, acc 0.84375\n",
      "2017-03-19T17:36:58.285502: step 1341, loss 0.354405, acc 0.84375\n",
      "2017-03-19T17:36:59.159043: step 1342, loss 0.330559, acc 0.84375\n",
      "2017-03-19T17:37:00.103718: step 1343, loss 0.308766, acc 0.859375\n",
      "2017-03-19T17:37:00.954496: step 1344, loss 0.323998, acc 0.84375\n",
      "2017-03-19T17:37:01.860641: step 1345, loss 0.31664, acc 0.828125\n",
      "2017-03-19T17:37:02.734764: step 1346, loss 0.413291, acc 0.765625\n",
      "2017-03-19T17:37:03.647417: step 1347, loss 0.226347, acc 0.875\n",
      "2017-03-19T17:37:04.532770: step 1348, loss 0.334204, acc 0.859375\n",
      "2017-03-19T17:37:05.398424: step 1349, loss 0.355817, acc 0.875\n",
      "2017-03-19T17:37:06.282653: step 1350, loss 0.310297, acc 0.875\n",
      "2017-03-19T17:37:07.150790: step 1351, loss 0.349934, acc 0.875\n",
      "2017-03-19T17:37:08.045458: step 1352, loss 0.355947, acc 0.875\n",
      "2017-03-19T17:37:08.904524: step 1353, loss 0.420055, acc 0.796875\n",
      "2017-03-19T17:37:09.785883: step 1354, loss 0.283832, acc 0.84375\n",
      "2017-03-19T17:37:10.688973: step 1355, loss 0.331016, acc 0.828125\n",
      "2017-03-19T17:37:11.618635: step 1356, loss 0.3968, acc 0.75\n",
      "2017-03-19T17:37:12.644319: step 1357, loss 0.39466, acc 0.75\n",
      "2017-03-19T17:37:13.728378: step 1358, loss 0.231493, acc 0.890625\n",
      "2017-03-19T17:37:14.726043: step 1359, loss 0.343923, acc 0.859375\n",
      "2017-03-19T17:37:15.758058: step 1360, loss 0.284281, acc 0.828125\n",
      "2017-03-19T17:37:16.795520: step 1361, loss 0.330662, acc 0.84375\n",
      "2017-03-19T17:37:17.817747: step 1362, loss 0.336988, acc 0.8125\n",
      "2017-03-19T17:37:18.850081: step 1363, loss 0.330371, acc 0.859375\n",
      "2017-03-19T17:37:19.869761: step 1364, loss 0.247418, acc 0.890625\n",
      "2017-03-19T17:37:20.877557: step 1365, loss 0.337091, acc 0.84375\n",
      "2017-03-19T17:37:21.868136: step 1366, loss 0.293619, acc 0.859375\n",
      "2017-03-19T17:37:22.777553: step 1367, loss 0.268814, acc 0.84375\n",
      "2017-03-19T17:37:23.871783: step 1368, loss 0.333056, acc 0.828125\n",
      "2017-03-19T17:37:24.876464: step 1369, loss 0.512799, acc 0.78125\n",
      "2017-03-19T17:37:25.773179: step 1370, loss 0.334915, acc 0.796875\n",
      "2017-03-19T17:37:26.659027: step 1371, loss 0.416081, acc 0.78125\n",
      "2017-03-19T17:37:27.524143: step 1372, loss 0.390595, acc 0.828125\n",
      "2017-03-19T17:37:28.427287: step 1373, loss 0.334172, acc 0.859375\n",
      "2017-03-19T17:37:29.319423: step 1374, loss 0.363035, acc 0.78125\n",
      "2017-03-19T17:37:30.214060: step 1375, loss 0.44383, acc 0.75\n",
      "2017-03-19T17:37:31.114217: step 1376, loss 0.381473, acc 0.828125\n",
      "2017-03-19T17:37:32.002820: step 1377, loss 0.371038, acc 0.828125\n",
      "2017-03-19T17:37:33.057132: step 1378, loss 0.326915, acc 0.859375\n",
      "2017-03-19T17:37:34.095384: step 1379, loss 0.283829, acc 0.875\n",
      "2017-03-19T17:37:34.985012: step 1380, loss 0.35231, acc 0.84375\n",
      "2017-03-19T17:37:35.875042: step 1381, loss 0.423412, acc 0.71875\n",
      "2017-03-19T17:37:36.771608: step 1382, loss 0.34067, acc 0.859375\n",
      "2017-03-19T17:37:37.646731: step 1383, loss 0.367286, acc 0.828125\n",
      "2017-03-19T17:37:38.516747: step 1384, loss 0.358215, acc 0.78125\n",
      "2017-03-19T17:37:39.373410: step 1385, loss 0.325287, acc 0.78125\n",
      "2017-03-19T17:37:40.277953: step 1386, loss 0.241886, acc 0.875\n",
      "2017-03-19T17:37:41.190557: step 1387, loss 0.26065, acc 0.875\n",
      "2017-03-19T17:37:42.221960: step 1388, loss 0.295922, acc 0.8125\n",
      "2017-03-19T17:37:43.264164: step 1389, loss 0.353967, acc 0.890625\n",
      "2017-03-19T17:37:44.293569: step 1390, loss 0.48977, acc 0.78125\n",
      "2017-03-19T17:37:45.287778: step 1391, loss 0.360169, acc 0.75\n",
      "2017-03-19T17:37:46.305509: step 1392, loss 0.4421, acc 0.796875\n",
      "2017-03-19T17:37:47.276650: step 1393, loss 0.271711, acc 0.890625\n",
      "2017-03-19T17:37:48.169566: step 1394, loss 0.414143, acc 0.796875\n",
      "2017-03-19T17:37:49.325301: step 1395, loss 0.373413, acc 0.796875\n",
      "2017-03-19T17:37:50.286494: step 1396, loss 0.344723, acc 0.84375\n",
      "2017-03-19T17:37:51.355757: step 1397, loss 0.345766, acc 0.78125\n",
      "2017-03-19T17:37:52.386431: step 1398, loss 0.298037, acc 0.875\n",
      "2017-03-19T17:37:53.277580: step 1399, loss 0.387796, acc 0.8125\n",
      "2017-03-19T17:37:54.167633: step 1400, loss 0.199209, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:37:58.646809: step 1400, loss 0.462647, acc 0.798\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1400\n",
      "\n",
      "2017-03-19T17:38:00.907446: step 1401, loss 0.283642, acc 0.84375\n",
      "2017-03-19T17:38:01.814593: step 1402, loss 0.289059, acc 0.875\n",
      "2017-03-19T17:38:02.668768: step 1403, loss 0.273595, acc 0.875\n",
      "2017-03-19T17:38:03.506898: step 1404, loss 0.355698, acc 0.828125\n",
      "2017-03-19T17:38:04.351583: step 1405, loss 0.431635, acc 0.765625\n",
      "2017-03-19T17:38:05.232663: step 1406, loss 0.413343, acc 0.796875\n",
      "2017-03-19T17:38:06.053697: step 1407, loss 0.432916, acc 0.796875\n",
      "2017-03-19T17:38:07.003077: step 1408, loss 0.323786, acc 0.828125\n",
      "2017-03-19T17:38:08.003057: step 1409, loss 0.322742, acc 0.8125\n",
      "2017-03-19T17:38:08.576412: step 1410, loss 0.527636, acc 0.775\n",
      "2017-03-19T17:38:09.671692: step 1411, loss 0.298319, acc 0.84375\n",
      "2017-03-19T17:38:10.578919: step 1412, loss 0.26157, acc 0.875\n",
      "2017-03-19T17:38:11.707226: step 1413, loss 0.286877, acc 0.84375\n",
      "2017-03-19T17:38:12.727750: step 1414, loss 0.287944, acc 0.890625\n",
      "2017-03-19T17:38:13.790109: step 1415, loss 0.328507, acc 0.8125\n",
      "2017-03-19T17:38:14.863376: step 1416, loss 0.216273, acc 0.859375\n",
      "2017-03-19T17:38:15.871070: step 1417, loss 0.308581, acc 0.84375\n",
      "2017-03-19T17:38:16.885326: step 1418, loss 0.286932, acc 0.90625\n",
      "2017-03-19T17:38:17.911533: step 1419, loss 0.306984, acc 0.796875\n",
      "2017-03-19T17:38:18.883734: step 1420, loss 0.240253, acc 0.875\n",
      "2017-03-19T17:38:19.912460: step 1421, loss 0.26449, acc 0.859375\n",
      "2017-03-19T17:38:20.874195: step 1422, loss 0.301363, acc 0.828125\n",
      "2017-03-19T17:38:21.757412: step 1423, loss 0.220869, acc 0.890625\n",
      "2017-03-19T17:38:22.649969: step 1424, loss 0.300362, acc 0.875\n",
      "2017-03-19T17:38:23.579631: step 1425, loss 0.232358, acc 0.875\n",
      "2017-03-19T17:38:24.702927: step 1426, loss 0.368901, acc 0.796875\n",
      "2017-03-19T17:38:25.747639: step 1427, loss 0.316939, acc 0.84375\n",
      "2017-03-19T17:38:26.611662: step 1428, loss 0.213016, acc 0.9375\n",
      "2017-03-19T17:38:27.539647: step 1429, loss 0.291461, acc 0.84375\n",
      "2017-03-19T17:38:28.435682: step 1430, loss 0.340721, acc 0.78125\n",
      "2017-03-19T17:38:29.593133: step 1431, loss 0.321056, acc 0.8125\n",
      "2017-03-19T17:38:30.477691: step 1432, loss 0.289433, acc 0.828125\n",
      "2017-03-19T17:38:31.383241: step 1433, loss 0.290174, acc 0.890625\n",
      "2017-03-19T17:38:32.542353: step 1434, loss 0.333599, acc 0.828125\n",
      "2017-03-19T17:38:33.474011: step 1435, loss 0.293796, acc 0.890625\n",
      "2017-03-19T17:38:34.619826: step 1436, loss 0.242602, acc 0.90625\n",
      "2017-03-19T17:38:35.607030: step 1437, loss 0.374986, acc 0.78125\n",
      "2017-03-19T17:38:36.640266: step 1438, loss 0.303547, acc 0.8125\n",
      "2017-03-19T17:38:37.538293: step 1439, loss 0.235567, acc 0.90625\n",
      "2017-03-19T17:38:38.426870: step 1440, loss 0.278878, acc 0.828125\n",
      "2017-03-19T17:38:39.315900: step 1441, loss 0.285828, acc 0.84375\n",
      "2017-03-19T17:38:40.188471: step 1442, loss 0.328808, acc 0.875\n",
      "2017-03-19T17:38:41.072656: step 1443, loss 0.320279, acc 0.78125\n",
      "2017-03-19T17:38:41.940684: step 1444, loss 0.324516, acc 0.859375\n",
      "2017-03-19T17:38:42.797694: step 1445, loss 0.252567, acc 0.890625\n",
      "2017-03-19T17:38:43.729307: step 1446, loss 0.412599, acc 0.890625\n",
      "2017-03-19T17:38:44.903546: step 1447, loss 0.230796, acc 0.9375\n",
      "2017-03-19T17:38:45.908318: step 1448, loss 0.328181, acc 0.828125\n",
      "2017-03-19T17:38:46.920118: step 1449, loss 0.234371, acc 0.859375\n",
      "2017-03-19T17:38:47.959922: step 1450, loss 0.280659, acc 0.890625\n",
      "2017-03-19T17:38:49.022597: step 1451, loss 0.310102, acc 0.84375\n",
      "2017-03-19T17:38:50.043417: step 1452, loss 0.281156, acc 0.828125\n",
      "2017-03-19T17:38:51.014596: step 1453, loss 0.384742, acc 0.8125\n",
      "2017-03-19T17:38:52.089859: step 1454, loss 0.393992, acc 0.828125\n",
      "2017-03-19T17:38:52.993256: step 1455, loss 0.31669, acc 0.78125\n",
      "2017-03-19T17:38:53.863828: step 1456, loss 0.358535, acc 0.78125\n",
      "2017-03-19T17:38:54.753413: step 1457, loss 0.232422, acc 0.921875\n",
      "2017-03-19T17:38:55.775090: step 1458, loss 0.301075, acc 0.859375\n",
      "2017-03-19T17:38:56.812225: step 1459, loss 0.29763, acc 0.875\n",
      "2017-03-19T17:38:57.826947: step 1460, loss 0.319201, acc 0.84375\n",
      "2017-03-19T17:38:58.833053: step 1461, loss 0.27522, acc 0.875\n",
      "2017-03-19T17:38:59.849217: step 1462, loss 0.343899, acc 0.84375\n",
      "2017-03-19T17:39:00.939590: step 1463, loss 0.274579, acc 0.875\n",
      "2017-03-19T17:39:01.940670: step 1464, loss 0.294143, acc 0.84375\n",
      "2017-03-19T17:39:02.896875: step 1465, loss 0.290221, acc 0.84375\n",
      "2017-03-19T17:39:03.963144: step 1466, loss 0.21872, acc 0.875\n",
      "2017-03-19T17:39:04.995512: step 1467, loss 0.386214, acc 0.84375\n",
      "2017-03-19T17:39:06.068239: step 1468, loss 0.246932, acc 0.90625\n",
      "2017-03-19T17:39:07.047401: step 1469, loss 0.307792, acc 0.8125\n",
      "2017-03-19T17:39:08.053129: step 1470, loss 0.344061, acc 0.765625\n",
      "2017-03-19T17:39:09.009380: step 1471, loss 0.229248, acc 0.875\n",
      "2017-03-19T17:39:10.114244: step 1472, loss 0.279598, acc 0.875\n",
      "2017-03-19T17:39:11.111958: step 1473, loss 0.210787, acc 0.890625\n",
      "2017-03-19T17:39:12.114208: step 1474, loss 0.243387, acc 0.875\n",
      "2017-03-19T17:39:13.137480: step 1475, loss 0.20861, acc 0.875\n",
      "2017-03-19T17:39:14.146700: step 1476, loss 0.323411, acc 0.84375\n",
      "2017-03-19T17:39:15.136038: step 1477, loss 0.244089, acc 0.84375\n",
      "2017-03-19T17:39:16.027205: step 1478, loss 0.346636, acc 0.84375\n",
      "2017-03-19T17:39:17.001986: step 1479, loss 0.275647, acc 0.84375\n",
      "2017-03-19T17:39:18.094127: step 1480, loss 0.320094, acc 0.84375\n",
      "2017-03-19T17:39:19.160887: step 1481, loss 0.263401, acc 0.890625\n",
      "2017-03-19T17:39:20.179614: step 1482, loss 0.249858, acc 0.890625\n",
      "2017-03-19T17:39:21.196505: step 1483, loss 0.298557, acc 0.859375\n",
      "2017-03-19T17:39:22.213096: step 1484, loss 0.254127, acc 0.90625\n",
      "2017-03-19T17:39:23.267091: step 1485, loss 0.283974, acc 0.875\n",
      "2017-03-19T17:39:24.290320: step 1486, loss 0.262379, acc 0.875\n",
      "2017-03-19T17:39:25.292534: step 1487, loss 0.327032, acc 0.828125\n",
      "2017-03-19T17:39:26.315262: step 1488, loss 0.290009, acc 0.828125\n",
      "2017-03-19T17:39:27.251429: step 1489, loss 0.248892, acc 0.890625\n",
      "2017-03-19T17:39:28.167683: step 1490, loss 0.253199, acc 0.875\n",
      "2017-03-19T17:39:29.101849: step 1491, loss 0.259792, acc 0.90625\n",
      "2017-03-19T17:39:30.272031: step 1492, loss 0.352227, acc 0.8125\n",
      "2017-03-19T17:39:31.290740: step 1493, loss 0.312253, acc 0.859375\n",
      "2017-03-19T17:39:32.423046: step 1494, loss 0.378219, acc 0.828125\n",
      "2017-03-19T17:39:33.444211: step 1495, loss 0.311718, acc 0.859375\n",
      "2017-03-19T17:39:34.525983: step 1496, loss 0.302204, acc 0.78125\n",
      "2017-03-19T17:39:35.557217: step 1497, loss 0.392323, acc 0.8125\n",
      "2017-03-19T17:39:36.559432: step 1498, loss 0.374548, acc 0.78125\n",
      "2017-03-19T17:39:37.628192: step 1499, loss 0.41372, acc 0.765625\n",
      "2017-03-19T17:39:38.650420: step 1500, loss 0.309039, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:39:43.075493: step 1500, loss 0.522553, acc 0.777\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1500\n",
      "\n",
      "2017-03-19T17:39:45.147046: step 1501, loss 0.401558, acc 0.78125\n",
      "2017-03-19T17:39:46.295594: step 1502, loss 0.319813, acc 0.859375\n",
      "2017-03-19T17:39:47.434904: step 1503, loss 0.263675, acc 0.890625\n",
      "2017-03-19T17:39:48.584726: step 1504, loss 0.286659, acc 0.8125\n",
      "2017-03-19T17:39:49.668996: step 1505, loss 0.254262, acc 0.921875\n",
      "2017-03-19T17:39:50.733254: step 1506, loss 0.285766, acc 0.859375\n",
      "2017-03-19T17:39:51.824531: step 1507, loss 0.291262, acc 0.875\n",
      "2017-03-19T17:39:52.863771: step 1508, loss 0.267112, acc 0.890625\n",
      "2017-03-19T17:39:53.892504: step 1509, loss 0.401339, acc 0.78125\n",
      "2017-03-19T17:39:54.909229: step 1510, loss 0.441791, acc 0.71875\n",
      "2017-03-19T17:39:55.942998: step 1511, loss 0.445306, acc 0.765625\n",
      "2017-03-19T17:39:57.036777: step 1512, loss 0.21997, acc 0.90625\n",
      "2017-03-19T17:39:57.951233: step 1513, loss 0.363803, acc 0.8125\n",
      "2017-03-19T17:39:58.856879: step 1514, loss 0.309264, acc 0.828125\n",
      "2017-03-19T17:39:59.754466: step 1515, loss 0.288739, acc 0.8125\n",
      "2017-03-19T17:40:00.708039: step 1516, loss 0.329744, acc 0.84375\n",
      "2017-03-19T17:40:01.677240: step 1517, loss 0.275912, acc 0.890625\n",
      "2017-03-19T17:40:02.594901: step 1518, loss 0.29947, acc 0.875\n",
      "2017-03-19T17:40:03.533469: step 1519, loss 0.335366, acc 0.859375\n",
      "2017-03-19T17:40:04.446604: step 1520, loss 0.407112, acc 0.765625\n",
      "2017-03-19T17:40:05.448229: step 1521, loss 0.408386, acc 0.859375\n",
      "2017-03-19T17:40:06.427471: step 1522, loss 0.268338, acc 0.859375\n",
      "2017-03-19T17:40:07.389136: step 1523, loss 0.275662, acc 0.875\n",
      "2017-03-19T17:40:08.291199: step 1524, loss 0.400429, acc 0.84375\n",
      "2017-03-19T17:40:09.182332: step 1525, loss 0.407022, acc 0.796875\n",
      "2017-03-19T17:40:10.092981: step 1526, loss 0.341515, acc 0.84375\n",
      "2017-03-19T17:40:10.981383: step 1527, loss 0.255489, acc 0.84375\n",
      "2017-03-19T17:40:11.905463: step 1528, loss 0.306956, acc 0.796875\n",
      "2017-03-19T17:40:12.881158: step 1529, loss 0.24452, acc 0.859375\n",
      "2017-03-19T17:40:13.964430: step 1530, loss 0.306173, acc 0.859375\n",
      "2017-03-19T17:40:14.994775: step 1531, loss 0.236167, acc 0.890625\n",
      "2017-03-19T17:40:16.046006: step 1532, loss 0.288114, acc 0.828125\n",
      "2017-03-19T17:40:17.045647: step 1533, loss 0.361695, acc 0.859375\n",
      "2017-03-19T17:40:18.019807: step 1534, loss 0.247635, acc 0.90625\n",
      "2017-03-19T17:40:19.031527: step 1535, loss 0.407876, acc 0.78125\n",
      "2017-03-19T17:40:20.199630: step 1536, loss 0.248087, acc 0.859375\n",
      "2017-03-19T17:40:21.232422: step 1537, loss 0.332092, acc 0.78125\n",
      "2017-03-19T17:40:22.237137: step 1538, loss 0.307964, acc 0.84375\n",
      "2017-03-19T17:40:23.243855: step 1539, loss 0.406365, acc 0.75\n",
      "2017-03-19T17:40:24.271086: step 1540, loss 0.454599, acc 0.765625\n",
      "2017-03-19T17:40:25.267693: step 1541, loss 0.232525, acc 0.921875\n",
      "2017-03-19T17:40:26.442529: step 1542, loss 0.312952, acc 0.859375\n",
      "2017-03-19T17:40:27.490308: step 1543, loss 0.328332, acc 0.859375\n",
      "2017-03-19T17:40:28.556568: step 1544, loss 0.368059, acc 0.78125\n",
      "2017-03-19T17:40:29.578045: step 1545, loss 0.22851, acc 0.875\n",
      "2017-03-19T17:40:30.585763: step 1546, loss 0.241366, acc 0.859375\n",
      "2017-03-19T17:40:31.606990: step 1547, loss 0.25429, acc 0.859375\n",
      "2017-03-19T17:40:32.548660: step 1548, loss 0.208175, acc 0.84375\n",
      "2017-03-19T17:40:33.615421: step 1549, loss 0.351184, acc 0.84375\n",
      "2017-03-19T17:40:34.697692: step 1550, loss 0.313902, acc 0.875\n",
      "2017-03-19T17:40:35.356161: step 1551, loss 0.404221, acc 0.85\n",
      "2017-03-19T17:40:36.511985: step 1552, loss 0.320922, acc 0.84375\n",
      "2017-03-19T17:40:37.632784: step 1553, loss 0.214999, acc 0.84375\n",
      "2017-03-19T17:40:38.749078: step 1554, loss 0.219272, acc 0.921875\n",
      "2017-03-19T17:40:39.765812: step 1555, loss 0.279638, acc 0.84375\n",
      "2017-03-19T17:40:40.771967: step 1556, loss 0.303568, acc 0.84375\n",
      "2017-03-19T17:40:41.829220: step 1557, loss 0.398878, acc 0.828125\n",
      "2017-03-19T17:40:42.898953: step 1558, loss 0.293978, acc 0.84375\n",
      "2017-03-19T17:40:43.917179: step 1559, loss 0.298456, acc 0.828125\n",
      "2017-03-19T17:40:44.939907: step 1560, loss 0.24824, acc 0.859375\n",
      "2017-03-19T17:40:45.955631: step 1561, loss 0.208265, acc 0.859375\n",
      "2017-03-19T17:40:46.852789: step 1562, loss 0.21453, acc 0.921875\n",
      "2017-03-19T17:40:47.760442: step 1563, loss 0.146661, acc 0.90625\n",
      "2017-03-19T17:40:48.657629: step 1564, loss 0.272262, acc 0.875\n",
      "2017-03-19T17:40:49.559376: step 1565, loss 0.351222, acc 0.828125\n",
      "2017-03-19T17:40:50.444952: step 1566, loss 0.383501, acc 0.765625\n",
      "2017-03-19T17:40:51.363053: step 1567, loss 0.363609, acc 0.859375\n",
      "2017-03-19T17:40:52.377275: step 1568, loss 0.268078, acc 0.90625\n",
      "2017-03-19T17:40:53.398957: step 1569, loss 0.234415, acc 0.875\n",
      "2017-03-19T17:40:54.426682: step 1570, loss 0.219814, acc 0.921875\n",
      "2017-03-19T17:40:55.379313: step 1571, loss 0.184175, acc 0.9375\n",
      "2017-03-19T17:40:56.414051: step 1572, loss 0.270067, acc 0.859375\n",
      "2017-03-19T17:40:57.431839: step 1573, loss 0.223597, acc 0.890625\n",
      "2017-03-19T17:40:58.519495: step 1574, loss 0.206813, acc 0.890625\n",
      "2017-03-19T17:40:59.572194: step 1575, loss 0.147945, acc 0.953125\n",
      "2017-03-19T17:41:00.630659: step 1576, loss 0.274505, acc 0.84375\n",
      "2017-03-19T17:41:01.696859: step 1577, loss 0.258217, acc 0.828125\n",
      "2017-03-19T17:41:02.721187: step 1578, loss 0.219372, acc 0.859375\n",
      "2017-03-19T17:41:03.709973: step 1579, loss 0.278528, acc 0.859375\n",
      "2017-03-19T17:41:04.741209: step 1580, loss 0.19419, acc 0.90625\n",
      "2017-03-19T17:41:05.760922: step 1581, loss 0.196847, acc 0.9375\n",
      "2017-03-19T17:41:06.783651: step 1582, loss 0.321226, acc 0.8125\n",
      "2017-03-19T17:41:07.815396: step 1583, loss 0.217714, acc 0.890625\n",
      "2017-03-19T17:41:08.870140: step 1584, loss 0.340064, acc 0.765625\n",
      "2017-03-19T17:41:09.937641: step 1585, loss 0.289849, acc 0.90625\n",
      "2017-03-19T17:41:10.951950: step 1586, loss 0.292394, acc 0.828125\n",
      "2017-03-19T17:41:11.933130: step 1587, loss 0.211538, acc 0.890625\n",
      "2017-03-19T17:41:12.940848: step 1588, loss 0.274806, acc 0.78125\n",
      "2017-03-19T17:41:13.955548: step 1589, loss 0.20502, acc 0.84375\n",
      "2017-03-19T17:41:14.965740: step 1590, loss 0.248837, acc 0.828125\n",
      "2017-03-19T17:41:16.007485: step 1591, loss 0.286576, acc 0.859375\n",
      "2017-03-19T17:41:17.037662: step 1592, loss 0.150679, acc 0.953125\n",
      "2017-03-19T17:41:18.070544: step 1593, loss 0.202657, acc 0.890625\n",
      "2017-03-19T17:41:19.105784: step 1594, loss 0.23588, acc 0.890625\n",
      "2017-03-19T17:41:20.127009: step 1595, loss 0.280202, acc 0.84375\n",
      "2017-03-19T17:41:21.126178: step 1596, loss 0.221086, acc 0.921875\n",
      "2017-03-19T17:41:22.143919: step 1597, loss 0.356707, acc 0.78125\n",
      "2017-03-19T17:41:23.158690: step 1598, loss 0.256045, acc 0.875\n",
      "2017-03-19T17:41:24.165778: step 1599, loss 0.195448, acc 0.9375\n",
      "2017-03-19T17:41:25.212024: step 1600, loss 0.268803, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:41:29.842118: step 1600, loss 0.485622, acc 0.802\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1600\n",
      "\n",
      "2017-03-19T17:41:31.888292: step 1601, loss 0.259938, acc 0.84375\n",
      "2017-03-19T17:41:33.007591: step 1602, loss 0.243013, acc 0.84375\n",
      "2017-03-19T17:41:33.893221: step 1603, loss 0.226886, acc 0.921875\n",
      "2017-03-19T17:41:34.755336: step 1604, loss 0.332728, acc 0.828125\n",
      "2017-03-19T17:41:35.599437: step 1605, loss 0.359843, acc 0.75\n",
      "2017-03-19T17:41:36.443437: step 1606, loss 0.23315, acc 0.859375\n",
      "2017-03-19T17:41:37.287038: step 1607, loss 0.288747, acc 0.828125\n",
      "2017-03-19T17:41:38.149050: step 1608, loss 0.279348, acc 0.859375\n",
      "2017-03-19T17:41:39.024125: step 1609, loss 0.274631, acc 0.84375\n",
      "2017-03-19T17:41:39.956302: step 1610, loss 0.305297, acc 0.875\n",
      "2017-03-19T17:41:40.923491: step 1611, loss 0.300035, acc 0.84375\n",
      "2017-03-19T17:41:41.785556: step 1612, loss 0.222425, acc 0.953125\n",
      "2017-03-19T17:41:42.659579: step 1613, loss 0.28746, acc 0.875\n",
      "2017-03-19T17:41:43.564344: step 1614, loss 0.310293, acc 0.8125\n",
      "2017-03-19T17:41:44.477995: step 1615, loss 0.336389, acc 0.796875\n",
      "2017-03-19T17:41:45.367246: step 1616, loss 0.25812, acc 0.8125\n",
      "2017-03-19T17:41:46.357403: step 1617, loss 0.382998, acc 0.8125\n",
      "2017-03-19T17:41:47.365085: step 1618, loss 0.252158, acc 0.859375\n",
      "2017-03-19T17:41:48.327271: step 1619, loss 0.283967, acc 0.828125\n",
      "2017-03-19T17:41:49.362257: step 1620, loss 0.337006, acc 0.78125\n",
      "2017-03-19T17:41:50.376479: step 1621, loss 0.297839, acc 0.828125\n",
      "2017-03-19T17:41:51.335857: step 1622, loss 0.300345, acc 0.796875\n",
      "2017-03-19T17:41:52.217449: step 1623, loss 0.300153, acc 0.84375\n",
      "2017-03-19T17:41:53.180595: step 1624, loss 0.328157, acc 0.890625\n",
      "2017-03-19T17:41:54.184309: step 1625, loss 0.272125, acc 0.84375\n",
      "2017-03-19T17:41:55.176967: step 1626, loss 0.283893, acc 0.828125\n",
      "2017-03-19T17:41:56.162669: step 1627, loss 0.236244, acc 0.875\n",
      "2017-03-19T17:41:57.188400: step 1628, loss 0.259784, acc 0.890625\n",
      "2017-03-19T17:41:58.203624: step 1629, loss 0.240594, acc 0.84375\n",
      "2017-03-19T17:41:59.235858: step 1630, loss 0.327659, acc 0.796875\n",
      "2017-03-19T17:42:00.265493: step 1631, loss 0.305044, acc 0.859375\n",
      "2017-03-19T17:42:01.192928: step 1632, loss 0.334934, acc 0.859375\n",
      "2017-03-19T17:42:02.370267: step 1633, loss 0.277597, acc 0.859375\n",
      "2017-03-19T17:42:03.274939: step 1634, loss 0.301549, acc 0.875\n",
      "2017-03-19T17:42:04.182560: step 1635, loss 0.284845, acc 0.84375\n",
      "2017-03-19T17:42:05.101661: step 1636, loss 0.133677, acc 0.9375\n",
      "2017-03-19T17:42:05.983698: step 1637, loss 0.290464, acc 0.8125\n",
      "2017-03-19T17:42:06.857220: step 1638, loss 0.231529, acc 0.859375\n",
      "2017-03-19T17:42:07.747250: step 1639, loss 0.225154, acc 0.890625\n",
      "2017-03-19T17:42:08.642298: step 1640, loss 0.272615, acc 0.84375\n",
      "2017-03-19T17:42:09.529021: step 1641, loss 0.339814, acc 0.8125\n",
      "2017-03-19T17:42:10.440263: step 1642, loss 0.308341, acc 0.859375\n",
      "2017-03-19T17:42:11.363832: step 1643, loss 0.248611, acc 0.84375\n",
      "2017-03-19T17:42:12.256865: step 1644, loss 0.182319, acc 0.875\n",
      "2017-03-19T17:42:13.118479: step 1645, loss 0.241395, acc 0.890625\n",
      "2017-03-19T17:42:14.006070: step 1646, loss 0.244754, acc 0.84375\n",
      "2017-03-19T17:42:14.892598: step 1647, loss 0.318373, acc 0.78125\n",
      "2017-03-19T17:42:15.850677: step 1648, loss 0.245898, acc 0.921875\n",
      "2017-03-19T17:42:17.034520: step 1649, loss 0.253312, acc 0.84375\n",
      "2017-03-19T17:42:18.035368: step 1650, loss 0.252899, acc 0.859375\n",
      "2017-03-19T17:42:19.081114: step 1651, loss 0.297679, acc 0.859375\n",
      "2017-03-19T17:42:20.107846: step 1652, loss 0.21091, acc 0.859375\n",
      "2017-03-19T17:42:21.122209: step 1653, loss 0.294273, acc 0.828125\n",
      "2017-03-19T17:42:22.119419: step 1654, loss 0.286803, acc 0.828125\n",
      "2017-03-19T17:42:23.138645: step 1655, loss 0.295074, acc 0.859375\n",
      "2017-03-19T17:42:24.160873: step 1656, loss 0.242827, acc 0.859375\n",
      "2017-03-19T17:42:25.210621: step 1657, loss 0.2447, acc 0.90625\n",
      "2017-03-19T17:42:26.239300: step 1658, loss 0.295721, acc 0.859375\n",
      "2017-03-19T17:42:27.236611: step 1659, loss 0.255693, acc 0.84375\n",
      "2017-03-19T17:42:28.264842: step 1660, loss 0.296031, acc 0.84375\n",
      "2017-03-19T17:42:29.272511: step 1661, loss 0.3347, acc 0.859375\n",
      "2017-03-19T17:42:30.293192: step 1662, loss 0.304397, acc 0.90625\n",
      "2017-03-19T17:42:31.323999: step 1663, loss 0.264148, acc 0.90625\n",
      "2017-03-19T17:42:32.344239: step 1664, loss 0.275693, acc 0.859375\n",
      "2017-03-19T17:42:33.342950: step 1665, loss 0.292591, acc 0.8125\n",
      "2017-03-19T17:42:34.394199: step 1666, loss 0.262783, acc 0.890625\n",
      "2017-03-19T17:42:35.399916: step 1667, loss 0.452038, acc 0.828125\n",
      "2017-03-19T17:42:36.421143: step 1668, loss 0.270084, acc 0.859375\n",
      "2017-03-19T17:42:37.496909: step 1669, loss 0.260009, acc 0.84375\n",
      "2017-03-19T17:42:38.615708: step 1670, loss 0.256207, acc 0.828125\n",
      "2017-03-19T17:42:39.647441: step 1671, loss 0.232676, acc 0.84375\n",
      "2017-03-19T17:42:40.709128: step 1672, loss 0.292096, acc 0.796875\n",
      "2017-03-19T17:42:41.732108: step 1673, loss 0.165578, acc 0.890625\n",
      "2017-03-19T17:42:42.743842: step 1674, loss 0.350506, acc 0.78125\n",
      "2017-03-19T17:42:43.774076: step 1675, loss 0.351195, acc 0.84375\n",
      "2017-03-19T17:42:44.772062: step 1676, loss 0.231469, acc 0.875\n",
      "2017-03-19T17:42:45.792101: step 1677, loss 0.266039, acc 0.859375\n",
      "2017-03-19T17:42:46.814663: step 1678, loss 0.206749, acc 0.890625\n",
      "2017-03-19T17:42:47.785063: step 1679, loss 0.417835, acc 0.875\n",
      "2017-03-19T17:42:48.910365: step 1680, loss 0.22385, acc 0.890625\n",
      "2017-03-19T17:42:49.938320: step 1681, loss 0.238323, acc 0.890625\n",
      "2017-03-19T17:42:50.919470: step 1682, loss 0.187515, acc 0.890625\n",
      "2017-03-19T17:42:52.025257: step 1683, loss 0.248876, acc 0.84375\n",
      "2017-03-19T17:42:52.941941: step 1684, loss 0.232307, acc 0.890625\n",
      "2017-03-19T17:42:53.834249: step 1685, loss 0.275692, acc 0.84375\n",
      "2017-03-19T17:42:54.783323: step 1686, loss 0.293811, acc 0.890625\n",
      "2017-03-19T17:42:55.891613: step 1687, loss 0.232376, acc 0.875\n",
      "2017-03-19T17:42:56.873643: step 1688, loss 0.237653, acc 0.90625\n",
      "2017-03-19T17:42:57.910541: step 1689, loss 0.234173, acc 0.921875\n",
      "2017-03-19T17:42:58.952698: step 1690, loss 0.281551, acc 0.796875\n",
      "2017-03-19T17:42:59.958414: step 1691, loss 0.208046, acc 0.953125\n",
      "2017-03-19T17:43:00.587862: step 1692, loss 0.347366, acc 0.825\n",
      "2017-03-19T17:43:01.821195: step 1693, loss 0.228239, acc 0.921875\n",
      "2017-03-19T17:43:02.815834: step 1694, loss 0.205633, acc 0.890625\n",
      "2017-03-19T17:43:03.838063: step 1695, loss 0.284379, acc 0.828125\n",
      "2017-03-19T17:43:04.887312: step 1696, loss 0.284578, acc 0.859375\n",
      "2017-03-19T17:43:05.856001: step 1697, loss 0.218339, acc 0.875\n",
      "2017-03-19T17:43:06.954787: step 1698, loss 0.265732, acc 0.828125\n",
      "2017-03-19T17:43:07.851439: step 1699, loss 0.243122, acc 0.90625\n",
      "2017-03-19T17:43:08.820577: step 1700, loss 0.168302, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:43:13.225813: step 1700, loss 0.516504, acc 0.794\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1700\n",
      "\n",
      "2017-03-19T17:43:15.780358: step 1701, loss 0.230298, acc 0.890625\n",
      "2017-03-19T17:43:16.770564: step 1702, loss 0.217767, acc 0.90625\n",
      "2017-03-19T17:43:17.668203: step 1703, loss 0.224277, acc 0.859375\n",
      "2017-03-19T17:43:18.533819: step 1704, loss 0.28014, acc 0.828125\n",
      "2017-03-19T17:43:19.398437: step 1705, loss 0.307525, acc 0.796875\n",
      "2017-03-19T17:43:20.253443: step 1706, loss 0.174819, acc 0.921875\n",
      "2017-03-19T17:43:21.106551: step 1707, loss 0.197637, acc 0.921875\n",
      "2017-03-19T17:43:21.960055: step 1708, loss 0.18476, acc 0.921875\n",
      "2017-03-19T17:43:22.838181: step 1709, loss 0.28494, acc 0.828125\n",
      "2017-03-19T17:43:23.713754: step 1710, loss 0.286737, acc 0.84375\n",
      "2017-03-19T17:43:24.594882: step 1711, loss 0.186517, acc 0.921875\n",
      "2017-03-19T17:43:25.458897: step 1712, loss 0.27659, acc 0.828125\n",
      "2017-03-19T17:43:26.338088: step 1713, loss 0.293948, acc 0.8125\n",
      "2017-03-19T17:43:27.246635: step 1714, loss 0.350173, acc 0.734375\n",
      "2017-03-19T17:43:28.146673: step 1715, loss 0.312518, acc 0.84375\n",
      "2017-03-19T17:43:29.084741: step 1716, loss 0.214466, acc 0.890625\n",
      "2017-03-19T17:43:29.996892: step 1717, loss 0.277561, acc 0.875\n",
      "2017-03-19T17:43:30.903985: step 1718, loss 0.29618, acc 0.796875\n",
      "2017-03-19T17:43:31.851109: step 1719, loss 0.18774, acc 0.890625\n",
      "2017-03-19T17:43:32.760706: step 1720, loss 0.222301, acc 0.796875\n",
      "2017-03-19T17:43:33.687274: step 1721, loss 0.271254, acc 0.828125\n",
      "2017-03-19T17:43:34.586809: step 1722, loss 0.264296, acc 0.828125\n",
      "2017-03-19T17:43:35.477391: step 1723, loss 0.187999, acc 0.875\n",
      "2017-03-19T17:43:36.388611: step 1724, loss 0.322332, acc 0.828125\n",
      "2017-03-19T17:43:37.285691: step 1725, loss 0.295164, acc 0.84375\n",
      "2017-03-19T17:43:38.160555: step 1726, loss 0.274745, acc 0.84375\n",
      "2017-03-19T17:43:39.055693: step 1727, loss 0.212822, acc 0.890625\n",
      "2017-03-19T17:43:39.940397: step 1728, loss 0.305049, acc 0.859375\n",
      "2017-03-19T17:43:40.828530: step 1729, loss 0.158364, acc 0.921875\n",
      "2017-03-19T17:43:41.714600: step 1730, loss 0.40942, acc 0.796875\n",
      "2017-03-19T17:43:42.599731: step 1731, loss 0.243847, acc 0.9375\n",
      "2017-03-19T17:43:43.487013: step 1732, loss 0.178841, acc 0.9375\n",
      "2017-03-19T17:43:44.402613: step 1733, loss 0.240697, acc 0.890625\n",
      "2017-03-19T17:43:45.282796: step 1734, loss 0.221866, acc 0.890625\n",
      "2017-03-19T17:43:46.189924: step 1735, loss 0.28442, acc 0.890625\n",
      "2017-03-19T17:43:47.096497: step 1736, loss 0.199331, acc 0.875\n",
      "2017-03-19T17:43:47.994595: step 1737, loss 0.375523, acc 0.796875\n",
      "2017-03-19T17:43:49.050938: step 1738, loss 0.198953, acc 0.875\n",
      "2017-03-19T17:43:50.064078: step 1739, loss 0.248073, acc 0.859375\n",
      "2017-03-19T17:43:51.054537: step 1740, loss 0.194708, acc 0.921875\n",
      "2017-03-19T17:43:52.149813: step 1741, loss 0.211689, acc 0.90625\n",
      "2017-03-19T17:43:53.296633: step 1742, loss 0.25908, acc 0.890625\n",
      "2017-03-19T17:43:54.297326: step 1743, loss 0.283053, acc 0.78125\n",
      "2017-03-19T17:43:55.299039: step 1744, loss 0.278991, acc 0.8125\n",
      "2017-03-19T17:43:56.334777: step 1745, loss 0.244044, acc 0.84375\n",
      "2017-03-19T17:43:57.346980: step 1746, loss 0.288215, acc 0.875\n",
      "2017-03-19T17:43:58.365206: step 1747, loss 0.209238, acc 0.859375\n",
      "2017-03-19T17:43:59.428961: step 1748, loss 0.198035, acc 0.859375\n",
      "2017-03-19T17:44:00.576778: step 1749, loss 0.295782, acc 0.796875\n",
      "2017-03-19T17:44:01.578582: step 1750, loss 0.363173, acc 0.796875\n",
      "2017-03-19T17:44:02.600309: step 1751, loss 0.218379, acc 0.90625\n",
      "2017-03-19T17:44:03.640551: step 1752, loss 0.193296, acc 0.875\n",
      "2017-03-19T17:44:04.622750: step 1753, loss 0.244758, acc 0.875\n",
      "2017-03-19T17:44:05.692512: step 1754, loss 0.186607, acc 0.859375\n",
      "2017-03-19T17:44:06.708236: step 1755, loss 0.239113, acc 0.828125\n",
      "2017-03-19T17:44:07.696945: step 1756, loss 0.201019, acc 0.9375\n",
      "2017-03-19T17:44:08.791720: step 1757, loss 0.252333, acc 0.890625\n",
      "2017-03-19T17:44:09.802565: step 1758, loss 0.305101, acc 0.890625\n",
      "2017-03-19T17:44:10.836804: step 1759, loss 0.264283, acc 0.875\n",
      "2017-03-19T17:44:11.821414: step 1760, loss 0.265453, acc 0.875\n",
      "2017-03-19T17:44:12.823127: step 1761, loss 0.189054, acc 0.90625\n",
      "2017-03-19T17:44:13.890387: step 1762, loss 0.275008, acc 0.828125\n",
      "2017-03-19T17:44:14.891601: step 1763, loss 0.241642, acc 0.78125\n",
      "2017-03-19T17:44:15.932344: step 1764, loss 0.22483, acc 0.921875\n",
      "2017-03-19T17:44:16.947398: step 1765, loss 0.219657, acc 0.890625\n",
      "2017-03-19T17:44:17.916584: step 1766, loss 0.225547, acc 0.828125\n",
      "2017-03-19T17:44:19.039391: step 1767, loss 0.323552, acc 0.875\n",
      "2017-03-19T17:44:20.093541: step 1768, loss 0.311508, acc 0.84375\n",
      "2017-03-19T17:44:21.077737: step 1769, loss 0.350258, acc 0.765625\n",
      "2017-03-19T17:44:22.091458: step 1770, loss 0.303021, acc 0.875\n",
      "2017-03-19T17:44:23.141207: step 1771, loss 0.175965, acc 0.921875\n",
      "2017-03-19T17:44:24.053856: step 1772, loss 0.243339, acc 0.84375\n",
      "2017-03-19T17:44:24.972391: step 1773, loss 0.284889, acc 0.875\n",
      "2017-03-19T17:44:25.919566: step 1774, loss 0.29479, acc 0.875\n",
      "2017-03-19T17:44:26.894713: step 1775, loss 0.278038, acc 0.875\n",
      "2017-03-19T17:44:28.001499: step 1776, loss 0.313666, acc 0.859375\n",
      "2017-03-19T17:44:29.022799: step 1777, loss 0.225644, acc 0.875\n",
      "2017-03-19T17:44:30.035646: step 1778, loss 0.171971, acc 0.921875\n",
      "2017-03-19T17:44:31.084895: step 1779, loss 0.212568, acc 0.875\n",
      "2017-03-19T17:44:32.031066: step 1780, loss 0.293296, acc 0.921875\n",
      "2017-03-19T17:44:32.989198: step 1781, loss 0.260913, acc 0.875\n",
      "2017-03-19T17:44:34.056961: step 1782, loss 0.201673, acc 0.921875\n",
      "2017-03-19T17:44:35.052113: step 1783, loss 0.309562, acc 0.84375\n",
      "2017-03-19T17:44:36.065835: step 1784, loss 0.146631, acc 0.9375\n",
      "2017-03-19T17:44:37.081558: step 1785, loss 0.39693, acc 0.828125\n",
      "2017-03-19T17:44:38.035738: step 1786, loss 0.275512, acc 0.859375\n",
      "2017-03-19T17:44:38.925153: step 1787, loss 0.26932, acc 0.875\n",
      "2017-03-19T17:44:39.929821: step 1788, loss 0.233225, acc 0.875\n",
      "2017-03-19T17:44:40.986580: step 1789, loss 0.237649, acc 0.875\n",
      "2017-03-19T17:44:41.981789: step 1790, loss 0.229651, acc 0.875\n",
      "2017-03-19T17:44:43.017026: step 1791, loss 0.362739, acc 0.828125\n",
      "2017-03-19T17:44:43.939173: step 1792, loss 0.288844, acc 0.859375\n",
      "2017-03-19T17:44:45.107744: step 1793, loss 0.27436, acc 0.875\n",
      "2017-03-19T17:44:46.138479: step 1794, loss 0.386326, acc 0.765625\n",
      "2017-03-19T17:44:47.065872: step 1795, loss 0.242799, acc 0.875\n",
      "2017-03-19T17:44:48.271111: step 1796, loss 0.193692, acc 0.921875\n",
      "2017-03-19T17:44:49.382405: step 1797, loss 0.469211, acc 0.71875\n",
      "2017-03-19T17:44:50.454168: step 1798, loss 0.263019, acc 0.90625\n",
      "2017-03-19T17:44:51.536530: step 1799, loss 0.232041, acc 0.90625\n",
      "2017-03-19T17:44:52.559729: step 1800, loss 0.190179, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:44:57.152428: step 1800, loss 0.497387, acc 0.798\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1800\n",
      "\n",
      "2017-03-19T17:44:59.120923: step 1801, loss 0.256689, acc 0.8125\n",
      "2017-03-19T17:45:00.255214: step 1802, loss 0.243208, acc 0.875\n",
      "2017-03-19T17:45:01.478084: step 1803, loss 0.192969, acc 0.921875\n",
      "2017-03-19T17:45:02.365524: step 1804, loss 0.31187, acc 0.84375\n",
      "2017-03-19T17:45:03.256574: step 1805, loss 0.329055, acc 0.8125\n",
      "2017-03-19T17:45:04.158215: step 1806, loss 0.240881, acc 0.890625\n",
      "2017-03-19T17:45:05.174060: step 1807, loss 0.259963, acc 0.859375\n",
      "2017-03-19T17:45:06.259327: step 1808, loss 0.219922, acc 0.890625\n",
      "2017-03-19T17:45:07.133902: step 1809, loss 0.200805, acc 0.890625\n",
      "2017-03-19T17:45:08.016979: step 1810, loss 0.18629, acc 0.921875\n",
      "2017-03-19T17:45:08.977618: step 1811, loss 0.270497, acc 0.828125\n",
      "2017-03-19T17:45:09.916854: step 1812, loss 0.264887, acc 0.859375\n",
      "2017-03-19T17:45:10.831503: step 1813, loss 0.308867, acc 0.875\n",
      "2017-03-19T17:45:11.892900: step 1814, loss 0.202818, acc 0.890625\n",
      "2017-03-19T17:45:12.918630: step 1815, loss 0.253149, acc 0.84375\n",
      "2017-03-19T17:45:13.856298: step 1816, loss 0.210851, acc 0.90625\n",
      "2017-03-19T17:45:14.901944: step 1817, loss 0.186706, acc 0.921875\n",
      "2017-03-19T17:45:15.990715: step 1818, loss 0.303775, acc 0.796875\n",
      "2017-03-19T17:45:16.906341: step 1819, loss 0.24854, acc 0.828125\n",
      "2017-03-19T17:45:17.801640: step 1820, loss 0.147981, acc 0.984375\n",
      "2017-03-19T17:45:18.715306: step 1821, loss 0.206048, acc 0.90625\n",
      "2017-03-19T17:45:19.617897: step 1822, loss 0.395393, acc 0.8125\n",
      "2017-03-19T17:45:20.540004: step 1823, loss 0.201268, acc 0.90625\n",
      "2017-03-19T17:45:21.464984: step 1824, loss 0.284893, acc 0.875\n",
      "2017-03-19T17:45:22.364022: step 1825, loss 0.34048, acc 0.890625\n",
      "2017-03-19T17:45:23.293153: step 1826, loss 0.275144, acc 0.796875\n",
      "2017-03-19T17:45:24.179664: step 1827, loss 0.219825, acc 0.890625\n",
      "2017-03-19T17:45:25.255948: step 1828, loss 0.260981, acc 0.875\n",
      "2017-03-19T17:45:26.263163: step 1829, loss 0.223955, acc 0.875\n",
      "2017-03-19T17:45:27.340545: step 1830, loss 0.257894, acc 0.859375\n",
      "2017-03-19T17:45:28.339176: step 1831, loss 0.186515, acc 0.890625\n",
      "2017-03-19T17:45:29.352898: step 1832, loss 0.263352, acc 0.890625\n",
      "2017-03-19T17:45:29.982846: step 1833, loss 0.213984, acc 0.875\n",
      "2017-03-19T17:45:31.163687: step 1834, loss 0.200045, acc 0.9375\n",
      "2017-03-19T17:45:32.296995: step 1835, loss 0.174127, acc 0.90625\n",
      "2017-03-19T17:45:33.268670: step 1836, loss 0.144291, acc 0.9375\n",
      "2017-03-19T17:45:34.250870: step 1837, loss 0.196899, acc 0.875\n",
      "2017-03-19T17:45:35.326136: step 1838, loss 0.196113, acc 0.890625\n",
      "2017-03-19T17:45:36.466457: step 1839, loss 0.188067, acc 0.921875\n",
      "2017-03-19T17:45:37.408270: step 1840, loss 0.168756, acc 0.90625\n",
      "2017-03-19T17:45:38.310576: step 1841, loss 0.25345, acc 0.859375\n",
      "2017-03-19T17:45:39.288668: step 1842, loss 0.200326, acc 0.921875\n",
      "2017-03-19T17:45:40.301389: step 1843, loss 0.233552, acc 0.875\n",
      "2017-03-19T17:45:41.304603: step 1844, loss 0.174301, acc 0.9375\n",
      "2017-03-19T17:45:42.214297: step 1845, loss 0.142871, acc 0.96875\n",
      "2017-03-19T17:45:43.128972: step 1846, loss 0.269524, acc 0.84375\n",
      "2017-03-19T17:45:44.015521: step 1847, loss 0.227927, acc 0.921875\n",
      "2017-03-19T17:45:44.920545: step 1848, loss 0.265512, acc 0.859375\n",
      "2017-03-19T17:45:45.861114: step 1849, loss 0.240088, acc 0.84375\n",
      "2017-03-19T17:45:46.806742: step 1850, loss 0.192276, acc 0.890625\n",
      "2017-03-19T17:45:47.716963: step 1851, loss 0.266237, acc 0.859375\n",
      "2017-03-19T17:45:48.641261: step 1852, loss 0.205227, acc 0.890625\n",
      "2017-03-19T17:45:49.547328: step 1853, loss 0.204692, acc 0.859375\n",
      "2017-03-19T17:45:50.441465: step 1854, loss 0.231822, acc 0.90625\n",
      "2017-03-19T17:45:51.396092: step 1855, loss 0.34725, acc 0.90625\n",
      "2017-03-19T17:45:52.299733: step 1856, loss 0.271582, acc 0.828125\n",
      "2017-03-19T17:45:53.192264: step 1857, loss 0.240329, acc 0.875\n",
      "2017-03-19T17:45:54.086848: step 1858, loss 0.214572, acc 0.921875\n",
      "2017-03-19T17:45:54.993288: step 1859, loss 0.151192, acc 0.953125\n",
      "2017-03-19T17:45:55.902025: step 1860, loss 0.419287, acc 0.75\n",
      "2017-03-19T17:45:56.830185: step 1861, loss 0.381824, acc 0.796875\n",
      "2017-03-19T17:45:57.927503: step 1862, loss 0.277296, acc 0.859375\n",
      "2017-03-19T17:45:58.969245: step 1863, loss 0.240426, acc 0.8125\n",
      "2017-03-19T17:45:59.903372: step 1864, loss 0.308796, acc 0.8125\n",
      "2017-03-19T17:46:01.030326: step 1865, loss 0.25778, acc 0.859375\n",
      "2017-03-19T17:46:01.935468: step 1866, loss 0.26998, acc 0.84375\n",
      "2017-03-19T17:46:02.892264: step 1867, loss 0.278898, acc 0.84375\n",
      "2017-03-19T17:46:04.014062: step 1868, loss 0.215641, acc 0.84375\n",
      "2017-03-19T17:46:04.939756: step 1869, loss 0.19418, acc 0.90625\n",
      "2017-03-19T17:46:05.915363: step 1870, loss 0.264055, acc 0.90625\n",
      "2017-03-19T17:46:06.922081: step 1871, loss 0.218117, acc 0.890625\n",
      "2017-03-19T17:46:07.840325: step 1872, loss 0.293387, acc 0.828125\n",
      "2017-03-19T17:46:08.936608: step 1873, loss 0.190822, acc 0.890625\n",
      "2017-03-19T17:46:10.013374: step 1874, loss 0.312227, acc 0.8125\n",
      "2017-03-19T17:46:11.029097: step 1875, loss 0.272914, acc 0.84375\n",
      "2017-03-19T17:46:12.057329: step 1876, loss 0.273149, acc 0.8125\n",
      "2017-03-19T17:46:13.067049: step 1877, loss 0.264906, acc 0.859375\n",
      "2017-03-19T17:46:14.002440: step 1878, loss 0.275643, acc 0.875\n",
      "2017-03-19T17:46:14.889489: step 1879, loss 0.258738, acc 0.78125\n",
      "2017-03-19T17:46:15.859680: step 1880, loss 0.287889, acc 0.828125\n",
      "2017-03-19T17:46:16.945954: step 1881, loss 0.257605, acc 0.859375\n",
      "2017-03-19T17:46:18.031727: step 1882, loss 0.18733, acc 0.96875\n",
      "2017-03-19T17:46:19.054934: step 1883, loss 0.212203, acc 0.875\n",
      "2017-03-19T17:46:20.036132: step 1884, loss 0.193303, acc 0.953125\n",
      "2017-03-19T17:46:20.948567: step 1885, loss 0.2034, acc 0.9375\n",
      "2017-03-19T17:46:21.932216: step 1886, loss 0.220799, acc 0.859375\n",
      "2017-03-19T17:46:22.898403: step 1887, loss 0.163826, acc 0.953125\n",
      "2017-03-19T17:46:23.921398: step 1888, loss 0.356577, acc 0.796875\n",
      "2017-03-19T17:46:24.982654: step 1889, loss 0.286905, acc 0.84375\n",
      "2017-03-19T17:46:26.076436: step 1890, loss 0.227825, acc 0.875\n",
      "2017-03-19T17:46:27.054131: step 1891, loss 0.228234, acc 0.875\n",
      "2017-03-19T17:46:28.210954: step 1892, loss 0.376913, acc 0.734375\n",
      "2017-03-19T17:46:29.194656: step 1893, loss 0.162525, acc 0.9375\n",
      "2017-03-19T17:46:30.342477: step 1894, loss 0.206512, acc 0.859375\n",
      "2017-03-19T17:46:31.321671: step 1895, loss 0.256089, acc 0.859375\n",
      "2017-03-19T17:46:32.493005: step 1896, loss 0.212548, acc 0.875\n",
      "2017-03-19T17:46:33.487713: step 1897, loss 0.138928, acc 0.953125\n",
      "2017-03-19T17:46:34.530457: step 1898, loss 0.184686, acc 0.859375\n",
      "2017-03-19T17:46:35.629239: step 1899, loss 0.332697, acc 0.78125\n",
      "2017-03-19T17:46:36.664538: step 1900, loss 0.282982, acc 0.78125\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:46:41.145996: step 1900, loss 0.522982, acc 0.804\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-1900\n",
      "\n",
      "2017-03-19T17:46:43.352086: step 1901, loss 0.276012, acc 0.890625\n",
      "2017-03-19T17:46:44.332284: step 1902, loss 0.260579, acc 0.859375\n",
      "2017-03-19T17:46:45.428565: step 1903, loss 0.220565, acc 0.859375\n",
      "2017-03-19T17:46:46.409264: step 1904, loss 0.203913, acc 0.84375\n",
      "2017-03-19T17:46:47.415980: step 1905, loss 0.208729, acc 0.859375\n",
      "2017-03-19T17:46:48.314375: step 1906, loss 0.234119, acc 0.890625\n",
      "2017-03-19T17:46:49.391142: step 1907, loss 0.312455, acc 0.84375\n",
      "2017-03-19T17:46:50.478416: step 1908, loss 0.195378, acc 0.9375\n",
      "2017-03-19T17:46:51.472624: step 1909, loss 0.207352, acc 0.921875\n",
      "2017-03-19T17:46:52.379347: step 1910, loss 0.254152, acc 0.859375\n",
      "2017-03-19T17:46:53.321683: step 1911, loss 0.254043, acc 0.90625\n",
      "2017-03-19T17:46:54.326347: step 1912, loss 0.1804, acc 0.90625\n",
      "2017-03-19T17:46:55.276421: step 1913, loss 0.240029, acc 0.875\n",
      "2017-03-19T17:46:56.220991: step 1914, loss 0.320321, acc 0.796875\n",
      "2017-03-19T17:46:57.200190: step 1915, loss 0.23697, acc 0.875\n",
      "2017-03-19T17:46:58.264448: step 1916, loss 0.200315, acc 0.890625\n",
      "2017-03-19T17:46:59.276647: step 1917, loss 0.227206, acc 0.84375\n",
      "2017-03-19T17:47:00.202166: step 1918, loss 0.226495, acc 0.921875\n",
      "2017-03-19T17:47:01.152753: step 1919, loss 0.299854, acc 0.796875\n",
      "2017-03-19T17:47:02.163373: step 1920, loss 0.23147, acc 0.921875\n",
      "2017-03-19T17:47:03.109546: step 1921, loss 0.205696, acc 0.921875\n",
      "2017-03-19T17:47:04.021731: step 1922, loss 0.172353, acc 0.890625\n",
      "2017-03-19T17:47:04.939849: step 1923, loss 0.202351, acc 0.9375\n",
      "2017-03-19T17:47:05.818094: step 1924, loss 0.293922, acc 0.859375\n",
      "2017-03-19T17:47:06.710637: step 1925, loss 0.346335, acc 0.796875\n",
      "2017-03-19T17:47:07.646970: step 1926, loss 0.268624, acc 0.828125\n",
      "2017-03-19T17:47:08.538260: step 1927, loss 0.260149, acc 0.875\n",
      "2017-03-19T17:47:09.441798: step 1928, loss 0.269663, acc 0.84375\n",
      "2017-03-19T17:47:10.352876: step 1929, loss 0.195797, acc 0.875\n",
      "2017-03-19T17:47:11.273755: step 1930, loss 0.336216, acc 0.828125\n",
      "2017-03-19T17:47:12.162799: step 1931, loss 0.316472, acc 0.828125\n",
      "2017-03-19T17:47:13.055845: step 1932, loss 0.197534, acc 0.859375\n",
      "2017-03-19T17:47:13.957500: step 1933, loss 0.203114, acc 0.921875\n",
      "2017-03-19T17:47:14.888603: step 1934, loss 0.272728, acc 0.859375\n",
      "2017-03-19T17:47:15.790890: step 1935, loss 0.162409, acc 0.921875\n",
      "2017-03-19T17:47:16.700539: step 1936, loss 0.162877, acc 0.9375\n",
      "2017-03-19T17:47:17.600130: step 1937, loss 0.241218, acc 0.890625\n",
      "2017-03-19T17:47:18.531373: step 1938, loss 0.207201, acc 0.890625\n",
      "2017-03-19T17:47:19.499982: step 1939, loss 0.366265, acc 0.78125\n",
      "2017-03-19T17:47:20.407415: step 1940, loss 0.17462, acc 0.921875\n",
      "2017-03-19T17:47:21.311214: step 1941, loss 0.207597, acc 0.890625\n",
      "2017-03-19T17:47:22.207397: step 1942, loss 0.317352, acc 0.859375\n",
      "2017-03-19T17:47:23.113449: step 1943, loss 0.225199, acc 0.828125\n",
      "2017-03-19T17:47:24.030630: step 1944, loss 0.264974, acc 0.890625\n",
      "2017-03-19T17:47:24.946741: step 1945, loss 0.171033, acc 0.96875\n",
      "2017-03-19T17:47:25.851842: step 1946, loss 0.196557, acc 0.921875\n",
      "2017-03-19T17:47:26.757687: step 1947, loss 0.273428, acc 0.875\n",
      "2017-03-19T17:47:27.662831: step 1948, loss 0.22023, acc 0.875\n",
      "2017-03-19T17:47:28.564621: step 1949, loss 0.225705, acc 0.90625\n",
      "2017-03-19T17:47:29.495182: step 1950, loss 0.2009, acc 0.90625\n",
      "2017-03-19T17:47:30.395105: step 1951, loss 0.170731, acc 0.953125\n",
      "2017-03-19T17:47:31.306649: step 1952, loss 0.322784, acc 0.828125\n",
      "2017-03-19T17:47:32.215296: step 1953, loss 0.302993, acc 0.890625\n",
      "2017-03-19T17:47:33.106431: step 1954, loss 0.260612, acc 0.875\n",
      "2017-03-19T17:47:34.028588: step 1955, loss 0.282585, acc 0.828125\n",
      "2017-03-19T17:47:34.952246: step 1956, loss 0.264981, acc 0.875\n",
      "2017-03-19T17:47:35.862381: step 1957, loss 0.319547, acc 0.828125\n",
      "2017-03-19T17:47:36.759625: step 1958, loss 0.137368, acc 0.96875\n",
      "2017-03-19T17:47:37.661162: step 1959, loss 0.26903, acc 0.875\n",
      "2017-03-19T17:47:38.561084: step 1960, loss 0.234079, acc 0.828125\n",
      "2017-03-19T17:47:39.457687: step 1961, loss 0.249685, acc 0.890625\n",
      "2017-03-19T17:47:40.353275: step 1962, loss 0.225231, acc 0.921875\n",
      "2017-03-19T17:47:41.285832: step 1963, loss 0.177789, acc 0.859375\n",
      "2017-03-19T17:47:42.205968: step 1964, loss 0.20323, acc 0.90625\n",
      "2017-03-19T17:47:43.148639: step 1965, loss 0.277413, acc 0.84375\n",
      "2017-03-19T17:47:44.057287: step 1966, loss 0.220586, acc 0.921875\n",
      "2017-03-19T17:47:44.955369: step 1967, loss 0.201904, acc 0.875\n",
      "2017-03-19T17:47:45.881501: step 1968, loss 0.250517, acc 0.890625\n",
      "2017-03-19T17:47:46.801558: step 1969, loss 0.222272, acc 0.90625\n",
      "2017-03-19T17:47:47.721612: step 1970, loss 0.233072, acc 0.890625\n",
      "2017-03-19T17:47:48.622150: step 1971, loss 0.214075, acc 0.890625\n",
      "2017-03-19T17:47:49.522644: step 1972, loss 0.206895, acc 0.890625\n",
      "2017-03-19T17:47:50.445470: step 1973, loss 0.207566, acc 0.890625\n",
      "2017-03-19T17:47:51.066331: step 1974, loss 0.220122, acc 0.825\n",
      "2017-03-19T17:47:52.047529: step 1975, loss 0.180045, acc 0.890625\n",
      "2017-03-19T17:47:52.988095: step 1976, loss 0.150942, acc 0.9375\n",
      "2017-03-19T17:47:53.910259: step 1977, loss 0.238457, acc 0.875\n",
      "2017-03-19T17:47:54.827608: step 1978, loss 0.218428, acc 0.90625\n",
      "2017-03-19T17:47:55.732129: step 1979, loss 0.159573, acc 0.953125\n",
      "2017-03-19T17:47:56.642453: step 1980, loss 0.242259, acc 0.84375\n",
      "2017-03-19T17:47:57.558605: step 1981, loss 0.159397, acc 0.90625\n",
      "2017-03-19T17:47:58.481893: step 1982, loss 0.178511, acc 0.90625\n",
      "2017-03-19T17:47:59.415956: step 1983, loss 0.178479, acc 0.90625\n",
      "2017-03-19T17:48:00.421628: step 1984, loss 0.222887, acc 0.84375\n",
      "2017-03-19T17:48:01.519407: step 1985, loss 0.157854, acc 0.921875\n",
      "2017-03-19T17:48:02.534720: step 1986, loss 0.143786, acc 0.90625\n",
      "2017-03-19T17:48:03.508432: step 1987, loss 0.182645, acc 0.90625\n",
      "2017-03-19T17:48:04.647744: step 1988, loss 0.200607, acc 0.875\n",
      "2017-03-19T17:48:05.707998: step 1989, loss 0.227432, acc 0.875\n",
      "2017-03-19T17:48:06.703207: step 1990, loss 0.246425, acc 0.890625\n",
      "2017-03-19T17:48:07.655403: step 1991, loss 0.143732, acc 0.9375\n",
      "2017-03-19T17:48:08.572604: step 1992, loss 0.269305, acc 0.859375\n",
      "2017-03-19T17:48:09.493261: step 1993, loss 0.239456, acc 0.796875\n",
      "2017-03-19T17:48:10.406815: step 1994, loss 0.259531, acc 0.859375\n",
      "2017-03-19T17:48:11.314709: step 1995, loss 0.208962, acc 0.84375\n",
      "2017-03-19T17:48:12.404919: step 1996, loss 0.185003, acc 0.875\n",
      "2017-03-19T17:48:13.366103: step 1997, loss 0.188695, acc 0.890625\n",
      "2017-03-19T17:48:14.512422: step 1998, loss 0.268502, acc 0.796875\n",
      "2017-03-19T17:48:15.444749: step 1999, loss 0.189345, acc 0.890625\n",
      "2017-03-19T17:48:16.352360: step 2000, loss 0.280247, acc 0.859375\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:48:20.587448: step 2000, loss 0.529993, acc 0.806\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-2000\n",
      "\n",
      "2017-03-19T17:48:22.788562: step 2001, loss 0.135951, acc 0.953125\n",
      "2017-03-19T17:48:23.873836: step 2002, loss 0.290073, acc 0.828125\n",
      "2017-03-19T17:48:24.895063: step 2003, loss 0.16419, acc 0.890625\n",
      "2017-03-19T17:48:25.971330: step 2004, loss 0.197382, acc 0.9375\n",
      "2017-03-19T17:48:26.992557: step 2005, loss 0.169363, acc 0.859375\n",
      "2017-03-19T17:48:28.088838: step 2006, loss 0.255704, acc 0.84375\n",
      "2017-03-19T17:48:29.109065: step 2007, loss 0.194539, acc 0.859375\n",
      "2017-03-19T17:48:30.126289: step 2008, loss 0.251588, acc 0.875\n",
      "2017-03-19T17:48:31.262099: step 2009, loss 0.207891, acc 0.890625\n",
      "2017-03-19T17:48:32.374894: step 2010, loss 0.234925, acc 0.859375\n",
      "2017-03-19T17:48:33.334575: step 2011, loss 0.179998, acc 0.90625\n",
      "2017-03-19T17:48:34.453372: step 2012, loss 0.213591, acc 0.875\n",
      "2017-03-19T17:48:35.413556: step 2013, loss 0.213412, acc 0.921875\n",
      "2017-03-19T17:48:36.352145: step 2014, loss 0.193655, acc 0.890625\n",
      "2017-03-19T17:48:37.533987: step 2015, loss 0.225207, acc 0.875\n",
      "2017-03-19T17:48:38.548291: step 2016, loss 0.35173, acc 0.84375\n",
      "2017-03-19T17:48:39.586689: step 2017, loss 0.263316, acc 0.875\n",
      "2017-03-19T17:48:40.585897: step 2018, loss 0.190607, acc 0.90625\n",
      "2017-03-19T17:48:41.506994: step 2019, loss 0.199939, acc 0.90625\n",
      "2017-03-19T17:48:42.431112: step 2020, loss 0.163733, acc 0.921875\n",
      "2017-03-19T17:48:43.354426: step 2021, loss 0.163862, acc 0.890625\n",
      "2017-03-19T17:48:44.264658: step 2022, loss 0.199163, acc 0.859375\n",
      "2017-03-19T17:48:45.168602: step 2023, loss 0.17632, acc 0.90625\n",
      "2017-03-19T17:48:46.074651: step 2024, loss 0.248294, acc 0.875\n",
      "2017-03-19T17:48:46.972270: step 2025, loss 0.199069, acc 0.859375\n",
      "2017-03-19T17:48:47.897547: step 2026, loss 0.235906, acc 0.890625\n",
      "2017-03-19T17:48:48.815077: step 2027, loss 0.20779, acc 0.859375\n",
      "2017-03-19T17:48:49.714405: step 2028, loss 0.155183, acc 0.921875\n",
      "2017-03-19T17:48:50.653073: step 2029, loss 0.174534, acc 0.875\n",
      "2017-03-19T17:48:51.617190: step 2030, loss 0.178648, acc 0.921875\n",
      "2017-03-19T17:48:52.546524: step 2031, loss 0.154159, acc 0.921875\n",
      "2017-03-19T17:48:53.452065: step 2032, loss 0.199504, acc 0.875\n",
      "2017-03-19T17:48:54.339173: step 2033, loss 0.159501, acc 0.921875\n",
      "2017-03-19T17:48:55.249894: step 2034, loss 0.191892, acc 0.890625\n",
      "2017-03-19T17:48:56.170827: step 2035, loss 0.203876, acc 0.828125\n",
      "2017-03-19T17:48:57.051850: step 2036, loss 0.229934, acc 0.84375\n",
      "2017-03-19T17:48:57.959006: step 2037, loss 0.158198, acc 0.953125\n",
      "2017-03-19T17:48:58.877430: step 2038, loss 0.13394, acc 0.9375\n",
      "2017-03-19T17:48:59.781223: step 2039, loss 0.190576, acc 0.921875\n",
      "2017-03-19T17:49:00.698549: step 2040, loss 0.154522, acc 0.96875\n",
      "2017-03-19T17:49:01.742273: step 2041, loss 0.275975, acc 0.859375\n",
      "2017-03-19T17:49:02.872579: step 2042, loss 0.251269, acc 0.828125\n",
      "2017-03-19T17:49:03.902312: step 2043, loss 0.231318, acc 0.84375\n",
      "2017-03-19T17:49:04.999093: step 2044, loss 0.178075, acc 0.90625\n",
      "2017-03-19T17:49:05.987297: step 2045, loss 0.23918, acc 0.90625\n",
      "2017-03-19T17:49:07.120604: step 2046, loss 0.193179, acc 0.890625\n",
      "2017-03-19T17:49:08.087793: step 2047, loss 0.139873, acc 0.90625\n",
      "2017-03-19T17:49:09.216598: step 2048, loss 0.245504, acc 0.875\n",
      "2017-03-19T17:49:10.267347: step 2049, loss 0.260752, acc 0.890625\n",
      "2017-03-19T17:49:11.334107: step 2050, loss 0.156785, acc 0.921875\n",
      "2017-03-19T17:49:12.462414: step 2051, loss 0.256482, acc 0.9375\n",
      "2017-03-19T17:49:13.446369: step 2052, loss 0.216212, acc 0.90625\n",
      "2017-03-19T17:49:14.517132: step 2053, loss 0.205849, acc 0.859375\n",
      "2017-03-19T17:49:15.503195: step 2054, loss 0.222834, acc 0.875\n",
      "2017-03-19T17:49:16.624995: step 2055, loss 0.176859, acc 0.9375\n",
      "2017-03-19T17:49:17.623705: step 2056, loss 0.198191, acc 0.90625\n",
      "2017-03-19T17:49:18.560869: step 2057, loss 0.194848, acc 0.875\n",
      "2017-03-19T17:49:19.482469: step 2058, loss 0.255144, acc 0.859375\n",
      "2017-03-19T17:49:20.406212: step 2059, loss 0.241011, acc 0.875\n",
      "2017-03-19T17:49:21.313831: step 2060, loss 0.298291, acc 0.765625\n",
      "2017-03-19T17:49:22.229875: step 2061, loss 0.160337, acc 0.921875\n",
      "2017-03-19T17:49:23.157145: step 2062, loss 0.156045, acc 0.921875\n",
      "2017-03-19T17:49:24.068262: step 2063, loss 0.197143, acc 0.921875\n",
      "2017-03-19T17:49:24.979878: step 2064, loss 0.220269, acc 0.859375\n",
      "2017-03-19T17:49:25.922993: step 2065, loss 0.263916, acc 0.859375\n",
      "2017-03-19T17:49:26.851676: step 2066, loss 0.174024, acc 0.90625\n",
      "2017-03-19T17:49:27.757821: step 2067, loss 0.191868, acc 0.90625\n",
      "2017-03-19T17:49:28.768120: step 2068, loss 0.247625, acc 0.875\n",
      "2017-03-19T17:49:29.738807: step 2069, loss 0.229514, acc 0.875\n",
      "2017-03-19T17:49:30.714516: step 2070, loss 0.159785, acc 0.9375\n",
      "2017-03-19T17:49:31.616173: step 2071, loss 0.164374, acc 0.9375\n",
      "2017-03-19T17:49:32.695846: step 2072, loss 0.267105, acc 0.90625\n",
      "2017-03-19T17:49:33.789125: step 2073, loss 0.110236, acc 0.984375\n",
      "2017-03-19T17:49:34.790839: step 2074, loss 0.239235, acc 0.875\n",
      "2017-03-19T17:49:35.757054: step 2075, loss 0.223081, acc 0.90625\n",
      "2017-03-19T17:49:36.754951: step 2076, loss 0.275937, acc 0.8125\n",
      "2017-03-19T17:49:37.856235: step 2077, loss 0.212438, acc 0.890625\n",
      "2017-03-19T17:49:38.853451: step 2078, loss 0.196945, acc 0.890625\n",
      "2017-03-19T17:49:39.860664: step 2079, loss 0.202094, acc 0.890625\n",
      "2017-03-19T17:49:40.987966: step 2080, loss 0.225899, acc 0.890625\n",
      "2017-03-19T17:49:41.945648: step 2081, loss 0.230233, acc 0.84375\n",
      "2017-03-19T17:49:42.871034: step 2082, loss 0.159014, acc 0.875\n",
      "2017-03-19T17:49:44.060807: step 2083, loss 0.133073, acc 0.921875\n",
      "2017-03-19T17:49:45.111053: step 2084, loss 0.161012, acc 0.984375\n",
      "2017-03-19T17:49:46.162802: step 2085, loss 0.161785, acc 0.921875\n",
      "2017-03-19T17:49:47.194538: step 2086, loss 0.203327, acc 0.859375\n",
      "2017-03-19T17:49:48.169733: step 2087, loss 0.212699, acc 0.875\n",
      "2017-03-19T17:49:49.172947: step 2088, loss 0.225795, acc 0.921875\n",
      "2017-03-19T17:49:50.173535: step 2089, loss 0.223045, acc 0.875\n",
      "2017-03-19T17:49:51.238293: step 2090, loss 0.254692, acc 0.859375\n",
      "2017-03-19T17:49:52.178431: step 2091, loss 0.195122, acc 0.859375\n",
      "2017-03-19T17:49:53.125354: step 2092, loss 0.19981, acc 0.890625\n",
      "2017-03-19T17:49:54.047028: step 2093, loss 0.281073, acc 0.828125\n",
      "2017-03-19T17:49:54.947193: step 2094, loss 0.234618, acc 0.890625\n",
      "2017-03-19T17:49:55.857342: step 2095, loss 0.167227, acc 0.9375\n",
      "2017-03-19T17:49:56.791457: step 2096, loss 0.171687, acc 0.9375\n",
      "2017-03-19T17:49:57.713009: step 2097, loss 0.177203, acc 0.9375\n",
      "2017-03-19T17:49:58.617729: step 2098, loss 0.259624, acc 0.921875\n",
      "2017-03-19T17:49:59.534382: step 2099, loss 0.277399, acc 0.875\n",
      "2017-03-19T17:50:00.463440: step 2100, loss 0.280153, acc 0.828125\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:50:04.891353: step 2100, loss 0.531138, acc 0.812\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-2100\n",
      "\n",
      "2017-03-19T17:50:06.819242: step 2101, loss 0.335727, acc 0.75\n",
      "2017-03-19T17:50:07.913522: step 2102, loss 0.26853, acc 0.78125\n",
      "2017-03-19T17:50:08.891218: step 2103, loss 0.31651, acc 0.90625\n",
      "2017-03-19T17:50:09.847900: step 2104, loss 0.235337, acc 0.828125\n",
      "2017-03-19T17:50:10.715415: step 2105, loss 0.178465, acc 0.90625\n",
      "2017-03-19T17:50:11.605499: step 2106, loss 0.211579, acc 0.828125\n",
      "2017-03-19T17:50:12.691273: step 2107, loss 0.231918, acc 0.875\n",
      "2017-03-19T17:50:13.602421: step 2108, loss 0.280053, acc 0.796875\n",
      "2017-03-19T17:50:14.511754: step 2109, loss 0.191422, acc 0.921875\n",
      "2017-03-19T17:50:15.445426: step 2110, loss 0.234891, acc 0.84375\n",
      "2017-03-19T17:50:16.386497: step 2111, loss 0.239166, acc 0.890625\n",
      "2017-03-19T17:50:17.352990: step 2112, loss 0.299461, acc 0.8125\n",
      "2017-03-19T17:50:18.312174: step 2113, loss 0.195351, acc 0.90625\n",
      "2017-03-19T17:50:19.327348: step 2114, loss 0.27196, acc 0.859375\n",
      "2017-03-19T17:50:19.964800: step 2115, loss 0.171761, acc 0.875\n",
      "2017-03-19T17:50:21.057079: step 2116, loss 0.212475, acc 0.890625\n",
      "2017-03-19T17:50:22.206398: step 2117, loss 0.121114, acc 0.921875\n",
      "2017-03-19T17:50:23.178104: step 2118, loss 0.155997, acc 0.890625\n",
      "2017-03-19T17:50:24.086200: step 2119, loss 0.224334, acc 0.859375\n",
      "2017-03-19T17:50:25.005854: step 2120, loss 0.187397, acc 0.890625\n",
      "2017-03-19T17:50:26.009586: step 2121, loss 0.222274, acc 0.875\n",
      "2017-03-19T17:50:26.934708: step 2122, loss 0.245928, acc 0.828125\n",
      "2017-03-19T17:50:27.997891: step 2123, loss 0.199851, acc 0.875\n",
      "2017-03-19T17:50:28.961577: step 2124, loss 0.231665, acc 0.875\n",
      "2017-03-19T17:50:29.976332: step 2125, loss 0.190454, acc 0.875\n",
      "2017-03-19T17:50:30.879163: step 2126, loss 0.16907, acc 0.890625\n",
      "2017-03-19T17:50:31.806719: step 2127, loss 0.246767, acc 0.890625\n",
      "2017-03-19T17:50:32.740782: step 2128, loss 0.170161, acc 0.921875\n",
      "2017-03-19T17:50:33.661000: step 2129, loss 0.276594, acc 0.84375\n",
      "2017-03-19T17:50:34.582105: step 2130, loss 0.123175, acc 0.921875\n",
      "2017-03-19T17:50:35.507367: step 2131, loss 0.194383, acc 0.875\n",
      "2017-03-19T17:50:36.411998: step 2132, loss 0.184492, acc 0.859375\n",
      "2017-03-19T17:50:37.313559: step 2133, loss 0.187285, acc 0.859375\n",
      "2017-03-19T17:50:38.227132: step 2134, loss 0.17267, acc 0.890625\n",
      "2017-03-19T17:50:39.152238: step 2135, loss 0.217377, acc 0.90625\n",
      "2017-03-19T17:50:40.296922: step 2136, loss 0.134923, acc 0.9375\n",
      "2017-03-19T17:50:41.328737: step 2137, loss 0.193234, acc 0.875\n",
      "2017-03-19T17:50:42.347963: step 2138, loss 0.250056, acc 0.875\n",
      "2017-03-19T17:50:43.358182: step 2139, loss 0.199278, acc 0.890625\n",
      "2017-03-19T17:50:44.390918: step 2140, loss 0.170268, acc 0.9375\n",
      "2017-03-19T17:50:45.441167: step 2141, loss 0.213554, acc 0.890625\n",
      "2017-03-19T17:50:46.465899: step 2142, loss 0.22371, acc 0.90625\n",
      "2017-03-19T17:50:47.471170: step 2143, loss 0.184164, acc 0.90625\n",
      "2017-03-19T17:50:48.453287: step 2144, loss 0.277539, acc 0.875\n",
      "2017-03-19T17:50:49.540080: step 2145, loss 0.22672, acc 0.890625\n",
      "2017-03-19T17:50:50.559306: step 2146, loss 0.14354, acc 0.921875\n",
      "2017-03-19T17:50:51.603049: step 2147, loss 0.123793, acc 0.953125\n",
      "2017-03-19T17:50:52.618773: step 2148, loss 0.162953, acc 0.90625\n",
      "2017-03-19T17:50:53.536341: step 2149, loss 0.178716, acc 0.921875\n",
      "2017-03-19T17:50:54.435526: step 2150, loss 0.160443, acc 0.921875\n",
      "2017-03-19T17:50:55.347274: step 2151, loss 0.149232, acc 0.90625\n",
      "2017-03-19T17:50:56.272097: step 2152, loss 0.194596, acc 0.859375\n",
      "2017-03-19T17:50:57.211161: step 2153, loss 0.198333, acc 0.875\n",
      "2017-03-19T17:50:58.134320: step 2154, loss 0.20454, acc 0.90625\n",
      "2017-03-19T17:50:59.056033: step 2155, loss 0.2153, acc 0.859375\n",
      "2017-03-19T17:50:59.965218: step 2156, loss 0.344236, acc 0.84375\n",
      "2017-03-19T17:51:00.981943: step 2157, loss 0.205376, acc 0.890625\n",
      "2017-03-19T17:51:01.974152: step 2158, loss 0.196115, acc 0.890625\n",
      "2017-03-19T17:51:02.972386: step 2159, loss 0.167349, acc 0.953125\n",
      "2017-03-19T17:51:03.893873: step 2160, loss 0.197237, acc 0.859375\n",
      "2017-03-19T17:51:04.820973: step 2161, loss 0.156037, acc 0.9375\n",
      "2017-03-19T17:51:05.729185: step 2162, loss 0.280061, acc 0.90625\n",
      "2017-03-19T17:51:06.640744: step 2163, loss 0.212602, acc 0.859375\n",
      "2017-03-19T17:51:07.656968: step 2164, loss 0.252831, acc 0.84375\n",
      "2017-03-19T17:51:08.745243: step 2165, loss 0.201517, acc 0.921875\n",
      "2017-03-19T17:51:09.692418: step 2166, loss 0.202392, acc 0.90625\n",
      "2017-03-19T17:51:10.790525: step 2167, loss 0.175889, acc 0.90625\n",
      "2017-03-19T17:51:11.825762: step 2168, loss 0.13895, acc 0.921875\n",
      "2017-03-19T17:51:12.770435: step 2169, loss 0.205546, acc 0.890625\n",
      "2017-03-19T17:51:13.678067: step 2170, loss 0.194101, acc 0.890625\n",
      "2017-03-19T17:51:14.572130: step 2171, loss 0.271054, acc 0.796875\n",
      "2017-03-19T17:51:15.537269: step 2172, loss 0.188975, acc 0.875\n",
      "2017-03-19T17:51:16.658071: step 2173, loss 0.25612, acc 0.8125\n",
      "2017-03-19T17:51:17.688302: step 2174, loss 0.198563, acc 0.890625\n",
      "2017-03-19T17:51:18.724042: step 2175, loss 0.245093, acc 0.828125\n",
      "2017-03-19T17:51:19.825825: step 2176, loss 0.239743, acc 0.84375\n",
      "2017-03-19T17:51:20.863565: step 2177, loss 0.096512, acc 0.96875\n",
      "2017-03-19T17:51:21.861275: step 2178, loss 0.184849, acc 0.84375\n",
      "2017-03-19T17:51:22.777856: step 2179, loss 0.167619, acc 0.921875\n",
      "2017-03-19T17:51:23.667998: step 2180, loss 0.310368, acc 0.859375\n",
      "2017-03-19T17:51:24.597572: step 2181, loss 0.234149, acc 0.859375\n",
      "2017-03-19T17:51:25.513172: step 2182, loss 0.118795, acc 0.9375\n",
      "2017-03-19T17:51:26.416387: step 2183, loss 0.211907, acc 0.921875\n",
      "2017-03-19T17:51:27.317028: step 2184, loss 0.215196, acc 0.875\n",
      "2017-03-19T17:51:28.250712: step 2185, loss 0.219793, acc 0.921875\n",
      "2017-03-19T17:51:29.306861: step 2186, loss 0.250622, acc 0.859375\n",
      "2017-03-19T17:51:30.375121: step 2187, loss 0.217929, acc 0.890625\n",
      "2017-03-19T17:51:31.421366: step 2188, loss 0.113899, acc 0.953125\n",
      "2017-03-19T17:51:32.496133: step 2189, loss 0.192974, acc 0.921875\n",
      "2017-03-19T17:51:33.544379: step 2190, loss 0.2626, acc 0.828125\n",
      "2017-03-19T17:51:34.542090: step 2191, loss 0.249964, acc 0.84375\n",
      "2017-03-19T17:51:35.576827: step 2192, loss 0.177886, acc 0.9375\n",
      "2017-03-19T17:51:36.684617: step 2193, loss 0.21408, acc 0.921875\n",
      "2017-03-19T17:51:37.690833: step 2194, loss 0.154317, acc 0.921875\n",
      "2017-03-19T17:51:38.831646: step 2195, loss 0.261041, acc 0.875\n",
      "2017-03-19T17:51:39.810344: step 2196, loss 0.222196, acc 0.921875\n",
      "2017-03-19T17:51:40.942149: step 2197, loss 0.151422, acc 0.953125\n",
      "2017-03-19T17:51:41.981890: step 2198, loss 0.183677, acc 0.859375\n",
      "2017-03-19T17:51:43.023634: step 2199, loss 0.233149, acc 0.8125\n",
      "2017-03-19T17:51:44.004331: step 2200, loss 0.170864, acc 0.90625\n",
      "\n",
      "Evaluation:\n",
      "2017-03-19T17:51:48.507056: step 2200, loss 0.549707, acc 0.811\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\stat198-gicf\\runs\\1489968887\\checkpoints\\model-2200\n",
      "\n",
      "2017-03-19T17:51:50.628039: step 2201, loss 0.182745, acc 0.953125\n",
      "2017-03-19T17:51:51.636256: step 2202, loss 0.259017, acc 0.890625\n",
      "2017-03-19T17:51:52.621458: step 2203, loss 0.217436, acc 0.828125\n",
      "2017-03-19T17:51:53.678326: step 2204, loss 0.170919, acc 0.875\n",
      "2017-03-19T17:51:54.643511: step 2205, loss 0.22896, acc 0.859375\n",
      "2017-03-19T17:51:55.520136: step 2206, loss 0.173225, acc 0.859375\n",
      "2017-03-19T17:51:56.389262: step 2207, loss 0.203873, acc 0.90625\n",
      "2017-03-19T17:51:57.267318: step 2208, loss 0.162449, acc 0.859375\n",
      "2017-03-19T17:51:58.149345: step 2209, loss 0.146247, acc 0.90625\n",
      "2017-03-19T17:51:59.075006: step 2210, loss 0.189376, acc 0.859375\n",
      "2017-03-19T17:52:00.146768: step 2211, loss 0.202927, acc 0.921875\n",
      "2017-03-19T17:52:01.297588: step 2212, loss 0.255843, acc 0.828125\n",
      "2017-03-19T17:52:02.451911: step 2213, loss 0.214883, acc 0.859375\n",
      "2017-03-19T17:52:03.458128: step 2214, loss 0.214124, acc 0.84375\n",
      "2017-03-19T17:52:04.517884: step 2215, loss 0.126215, acc 0.921875\n",
      "2017-03-19T17:52:05.669703: step 2216, loss 0.280455, acc 0.828125\n",
      "2017-03-19T17:52:06.785998: step 2217, loss 0.252678, acc 0.796875\n",
      "2017-03-19T17:52:07.741755: step 2218, loss 0.217946, acc 0.84375\n",
      "2017-03-19T17:52:08.853047: step 2219, loss 0.119874, acc 0.9375\n",
      "2017-03-19T17:52:09.798716: step 2220, loss 0.279535, acc 0.890625\n",
      "2017-03-19T17:52:10.767806: step 2221, loss 0.360447, acc 0.828125\n",
      "2017-03-19T17:52:11.858088: step 2222, loss 0.232387, acc 0.859375\n",
      "2017-03-19T17:52:12.786727: step 2223, loss 0.256303, acc 0.859375\n",
      "2017-03-19T17:52:13.710017: step 2224, loss 0.198284, acc 0.859375\n",
      "2017-03-19T17:52:14.632175: step 2225, loss 0.195261, acc 0.90625\n",
      "2017-03-19T17:52:15.559348: step 2226, loss 0.175131, acc 0.90625\n",
      "2017-03-19T17:52:16.452063: step 2227, loss 0.20432, acc 0.890625\n",
      "2017-03-19T17:52:17.360605: step 2228, loss 0.117997, acc 0.96875\n",
      "2017-03-19T17:52:18.292827: step 2229, loss 0.149791, acc 0.921875\n",
      "2017-03-19T17:52:19.237949: step 2230, loss 0.259092, acc 0.765625\n",
      "2017-03-19T17:52:20.163041: step 2231, loss 0.215399, acc 0.859375\n",
      "2017-03-19T17:52:21.095805: step 2232, loss 0.17791, acc 0.9375\n",
      "2017-03-19T17:52:22.009380: step 2233, loss 0.163343, acc 0.90625\n",
      "2017-03-19T17:52:22.929985: step 2234, loss 0.204864, acc 0.859375\n",
      "2017-03-19T17:52:23.841604: step 2235, loss 0.146101, acc 0.96875\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "#tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "#tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "\n",
    "x_text, y = load_data_and_labels(train_pos_sample, train_neg_sample)\n",
    "x_test, y_test = load_data_and_labels(test_pos, test_neg)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([max([len(x.split(\" \")) for x in x_text]), max([len(x.split(\" \")) for x in x_test])])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "x_1 = np.array(list(vocab_processor.fit_transform(x_test)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "shuffle_indices_1 = np.random.permutation(np.arange(len(y_test)))\n",
    "x_test_shuffled = x_1[shuffle_indices_1]\n",
    "y_test_shuffled = y_test[shuffle_indices_1]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_test_shuffled#x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_test_shuffled#y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
