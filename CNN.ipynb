{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CNN Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The mic is great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                               text\n",
       "0       0  So there is no way for me to plug it in here i...\n",
       "1       1                        Good case, Excellent value.\n",
       "2       1                             Great for the jawbone.\n",
       "3       0  Tied to charger for conversations lasting more...\n",
       "4       1                                  The mic is great."
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file = open(\"amazon_cells_labelled.txt\", 'r')\n",
    "text_file = text_file.read()\n",
    "text_file = re.split(\"\\t|\\n\", text_file)\n",
    "text_file = text_file[:len(text_file)-1]\n",
    "labeled = pd.DataFrame({\"text\":text_file[:len(text_file):2], \"rating\":[int(x) for x in text_file[1:len(text_file):2]]})\n",
    "labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'So there is no way for me to plug it in here in the US unless I go by a converter.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos = labeled[labeled[\"rating\"] == 1]\n",
    "train_neg = labeled[labeled[\"rating\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "      self, sequence_length, num_classes, vocab_size,\n",
    "      embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "\n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "\n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "\n",
    "        # Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "            self.W = tf.Variable(\n",
    "                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                name=\"W\")\n",
    "            self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                    self.embedded_chars_expanded,\n",
    "                    W,\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding=\"VALID\",\n",
    "                    name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                    h,\n",
    "                    ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                    strides=[1, 1, 1, 1],\n",
    "                    padding='VALID',\n",
    "                    name=\"pool\")\n",
    "                pooled_outputs.append(pooled)\n",
    "\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "\n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "\n",
    "        # CalculateMean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads MR polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(positive_data_file[\"text\"])#list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(negative_data_file[\"text\"])#list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.0\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_CHECKPOINTS=5\n",
      "NUM_EPOCHS=200\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n",
      "Loading data...\n",
      "Vocabulary Size: 1861\n",
      "Train/Dev split: 900/100\n",
      "INFO:tensorflow:Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,298 : INFO : Summary name embedding/W:0/grad/hist is illegal; using embedding/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,334 : INFO : Summary name embedding/W:0/grad/sparsity is illegal; using embedding/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,343 : INFO : Summary name conv-maxpool-3/W:0/grad/hist is illegal; using conv-maxpool-3/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,361 : INFO : Summary name conv-maxpool-3/W:0/grad/sparsity is illegal; using conv-maxpool-3/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,368 : INFO : Summary name conv-maxpool-3/b:0/grad/hist is illegal; using conv-maxpool-3/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,385 : INFO : Summary name conv-maxpool-3/b:0/grad/sparsity is illegal; using conv-maxpool-3/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,392 : INFO : Summary name conv-maxpool-4/W:0/grad/hist is illegal; using conv-maxpool-4/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,418 : INFO : Summary name conv-maxpool-4/W:0/grad/sparsity is illegal; using conv-maxpool-4/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,427 : INFO : Summary name conv-maxpool-4/b:0/grad/hist is illegal; using conv-maxpool-4/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,448 : INFO : Summary name conv-maxpool-4/b:0/grad/sparsity is illegal; using conv-maxpool-4/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,454 : INFO : Summary name conv-maxpool-5/W:0/grad/hist is illegal; using conv-maxpool-5/W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,477 : INFO : Summary name conv-maxpool-5/W:0/grad/sparsity is illegal; using conv-maxpool-5/W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,484 : INFO : Summary name conv-maxpool-5/b:0/grad/hist is illegal; using conv-maxpool-5/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,512 : INFO : Summary name conv-maxpool-5/b:0/grad/sparsity is illegal; using conv-maxpool-5/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,520 : INFO : Summary name W:0/grad/hist is illegal; using W_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,546 : INFO : Summary name W:0/grad/sparsity is illegal; using W_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,551 : INFO : Summary name output/b:0/grad/hist is illegal; using output/b_0/grad/hist instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-03-13 14:13:16,563 : INFO : Summary name output/b:0/grad/sparsity is illegal; using output/b_0/grad/sparsity instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\n",
      "\n",
      "2017-03-13T14:13:24.612288: step 1, loss 1.8943, acc 0.40625\n",
      "2017-03-13T14:13:24.792302: step 2, loss 1.90785, acc 0.484375\n",
      "2017-03-13T14:13:25.111059: step 3, loss 2.35822, acc 0.359375\n",
      "2017-03-13T14:13:25.311073: step 4, loss 1.95532, acc 0.5\n",
      "2017-03-13T14:13:25.543756: step 5, loss 2.0014, acc 0.453125\n",
      "2017-03-13T14:13:25.715746: step 6, loss 1.29569, acc 0.609375\n",
      "2017-03-13T14:13:25.971593: step 7, loss 2.02579, acc 0.515625\n",
      "2017-03-13T14:13:26.111604: step 8, loss 1.50217, acc 0.46875\n",
      "2017-03-13T14:13:26.286839: step 9, loss 1.48045, acc 0.5\n",
      "2017-03-13T14:13:26.453925: step 10, loss 1.517, acc 0.5625\n",
      "2017-03-13T14:13:26.638082: step 11, loss 1.48833, acc 0.59375\n",
      "2017-03-13T14:13:26.800239: step 12, loss 1.21415, acc 0.609375\n",
      "2017-03-13T14:13:27.030044: step 13, loss 1.22732, acc 0.671875\n",
      "2017-03-13T14:13:27.249197: step 14, loss 1.84451, acc 0.578125\n",
      "2017-03-13T14:13:27.338243: step 15, loss 1.58554, acc 0.75\n",
      "2017-03-13T14:13:27.478253: step 16, loss 1.15333, acc 0.578125\n",
      "2017-03-13T14:13:27.628263: step 17, loss 1.52808, acc 0.515625\n",
      "2017-03-13T14:13:27.778273: step 18, loss 1.12011, acc 0.625\n",
      "2017-03-13T14:13:27.958286: step 19, loss 0.891154, acc 0.65625\n",
      "2017-03-13T14:13:28.111914: step 20, loss 1.09696, acc 0.625\n",
      "2017-03-13T14:13:28.260125: step 21, loss 1.32774, acc 0.671875\n",
      "2017-03-13T14:13:28.411444: step 22, loss 1.09136, acc 0.671875\n",
      "2017-03-13T14:13:28.561453: step 23, loss 1.02966, acc 0.640625\n",
      "2017-03-13T14:13:28.711464: step 24, loss 0.933355, acc 0.6875\n",
      "2017-03-13T14:13:28.851473: step 25, loss 1.36261, acc 0.5625\n",
      "2017-03-13T14:13:29.021486: step 26, loss 0.658084, acc 0.71875\n",
      "2017-03-13T14:13:29.172267: step 27, loss 1.2428, acc 0.640625\n",
      "2017-03-13T14:13:29.380416: step 28, loss 1.4471, acc 0.578125\n",
      "2017-03-13T14:13:29.574561: step 29, loss 1.0713, acc 0.671875\n",
      "2017-03-13T14:13:29.639144: step 30, loss 0.592155, acc 0.75\n",
      "2017-03-13T14:13:29.794546: step 31, loss 0.801483, acc 0.734375\n",
      "2017-03-13T14:13:29.995322: step 32, loss 0.82304, acc 0.703125\n",
      "2017-03-13T14:13:30.203470: step 33, loss 0.676563, acc 0.734375\n",
      "2017-03-13T14:13:30.424627: step 34, loss 0.516924, acc 0.765625\n",
      "2017-03-13T14:13:30.623771: step 35, loss 1.11582, acc 0.640625\n",
      "2017-03-13T14:13:30.830915: step 36, loss 1.13744, acc 0.671875\n",
      "2017-03-13T14:13:31.049070: step 37, loss 0.853382, acc 0.625\n",
      "2017-03-13T14:13:31.242207: step 38, loss 0.560444, acc 0.765625\n",
      "2017-03-13T14:13:31.441348: step 39, loss 0.834858, acc 0.625\n",
      "2017-03-13T14:13:31.634240: step 40, loss 0.809802, acc 0.6875\n",
      "2017-03-13T14:13:31.843388: step 41, loss 0.872415, acc 0.734375\n",
      "2017-03-13T14:13:32.075552: step 42, loss 0.758124, acc 0.671875\n",
      "2017-03-13T14:13:32.262685: step 43, loss 1.1277, acc 0.609375\n",
      "2017-03-13T14:13:32.484843: step 44, loss 0.992756, acc 0.671875\n",
      "2017-03-13T14:13:32.565900: step 45, loss 0.683677, acc 0.75\n",
      "2017-03-13T14:13:32.759038: step 46, loss 0.713115, acc 0.65625\n",
      "2017-03-13T14:13:32.974192: step 47, loss 1.23338, acc 0.6875\n",
      "2017-03-13T14:13:33.185339: step 48, loss 0.822538, acc 0.640625\n",
      "2017-03-13T14:13:33.384480: step 49, loss 0.631055, acc 0.75\n",
      "2017-03-13T14:13:33.587626: step 50, loss 0.839341, acc 0.703125\n",
      "2017-03-13T14:13:33.784765: step 51, loss 0.592258, acc 0.734375\n",
      "2017-03-13T14:13:33.982905: step 52, loss 0.644465, acc 0.796875\n",
      "2017-03-13T14:13:34.122980: step 53, loss 0.525628, acc 0.75\n",
      "2017-03-13T14:13:34.290388: step 54, loss 0.793752, acc 0.71875\n",
      "2017-03-13T14:13:34.478518: step 55, loss 0.766364, acc 0.75\n",
      "2017-03-13T14:13:34.676659: step 56, loss 0.772175, acc 0.765625\n",
      "2017-03-13T14:13:34.886816: step 57, loss 0.630136, acc 0.84375\n",
      "2017-03-13T14:13:35.075941: step 58, loss 0.562825, acc 0.8125\n",
      "2017-03-13T14:13:35.275084: step 59, loss 1.14813, acc 0.65625\n",
      "2017-03-13T14:13:35.339129: step 60, loss 0.0366127, acc 1\n",
      "2017-03-13T14:13:35.524260: step 61, loss 0.955657, acc 0.71875\n",
      "2017-03-13T14:13:35.715395: step 62, loss 0.630166, acc 0.75\n",
      "2017-03-13T14:13:35.897524: step 63, loss 0.618049, acc 0.8125\n",
      "2017-03-13T14:13:36.111677: step 64, loss 0.42342, acc 0.8125\n",
      "2017-03-13T14:13:36.312819: step 65, loss 0.29245, acc 0.859375\n",
      "2017-03-13T14:13:36.496950: step 66, loss 0.854703, acc 0.78125\n",
      "2017-03-13T14:13:36.703096: step 67, loss 0.639329, acc 0.734375\n",
      "2017-03-13T14:13:36.919250: step 68, loss 0.797207, acc 0.703125\n",
      "2017-03-13T14:13:37.133401: step 69, loss 0.365543, acc 0.8125\n",
      "2017-03-13T14:13:37.283515: step 70, loss 0.668118, acc 0.78125\n",
      "2017-03-13T14:13:37.444900: step 71, loss 0.667825, acc 0.796875\n",
      "2017-03-13T14:13:37.643041: step 72, loss 0.482718, acc 0.84375\n",
      "2017-03-13T14:13:37.850188: step 73, loss 0.728857, acc 0.703125\n",
      "2017-03-13T14:13:38.068342: step 74, loss 0.608702, acc 0.796875\n",
      "2017-03-13T14:13:38.130387: step 75, loss 0.205498, acc 0.75\n",
      "2017-03-13T14:13:38.326529: step 76, loss 0.292411, acc 0.890625\n",
      "2017-03-13T14:13:38.542679: step 77, loss 0.691768, acc 0.734375\n",
      "2017-03-13T14:13:38.746826: step 78, loss 0.415993, acc 0.859375\n",
      "2017-03-13T14:13:38.948967: step 79, loss 0.421846, acc 0.78125\n",
      "2017-03-13T14:13:39.149109: step 80, loss 0.494107, acc 0.734375\n",
      "2017-03-13T14:13:39.336242: step 81, loss 0.448598, acc 0.8125\n",
      "2017-03-13T14:13:39.530380: step 82, loss 0.358873, acc 0.796875\n",
      "2017-03-13T14:13:39.690492: step 83, loss 0.69928, acc 0.78125\n",
      "2017-03-13T14:13:39.877626: step 84, loss 0.357159, acc 0.890625\n",
      "2017-03-13T14:13:40.078769: step 85, loss 0.468888, acc 0.84375\n",
      "2017-03-13T14:13:40.258901: step 86, loss 0.414757, acc 0.84375\n",
      "2017-03-13T14:13:40.462040: step 87, loss 0.478655, acc 0.796875\n",
      "2017-03-13T14:13:40.670189: step 88, loss 0.366252, acc 0.875\n",
      "2017-03-13T14:13:40.880337: step 89, loss 0.351438, acc 0.875\n",
      "2017-03-13T14:13:40.938378: step 90, loss 0.839711, acc 0.5\n",
      "2017-03-13T14:13:41.126512: step 91, loss 0.435779, acc 0.859375\n",
      "2017-03-13T14:13:41.305639: step 92, loss 0.346715, acc 0.859375\n",
      "2017-03-13T14:13:41.501779: step 93, loss 0.49476, acc 0.875\n",
      "2017-03-13T14:13:41.686911: step 94, loss 0.449992, acc 0.84375\n",
      "2017-03-13T14:13:41.879046: step 95, loss 0.314099, acc 0.90625\n",
      "2017-03-13T14:13:42.055171: step 96, loss 0.488713, acc 0.8125\n",
      "2017-03-13T14:13:42.236312: step 97, loss 0.471706, acc 0.828125\n",
      "2017-03-13T14:13:42.419445: step 98, loss 0.386006, acc 0.8125\n",
      "2017-03-13T14:13:42.605573: step 99, loss 0.57118, acc 0.84375\n",
      "2017-03-13T14:13:42.812727: step 100, loss 0.385951, acc 0.84375\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:13:42.941812: step 100, loss 0.39038, acc 0.81\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-100\n",
      "\n",
      "2017-03-13T14:13:44.082442: step 101, loss 0.288029, acc 0.90625\n",
      "2017-03-13T14:13:44.313318: step 102, loss 0.578033, acc 0.765625\n",
      "2017-03-13T14:13:44.502556: step 103, loss 0.566204, acc 0.765625\n",
      "2017-03-13T14:13:44.684427: step 104, loss 0.356145, acc 0.875\n",
      "2017-03-13T14:13:44.732513: step 105, loss 0.0273949, acc 1\n",
      "2017-03-13T14:13:44.872523: step 106, loss 0.223197, acc 0.890625\n",
      "2017-03-13T14:13:45.030463: step 107, loss 0.696586, acc 0.8125\n",
      "2017-03-13T14:13:45.221598: step 108, loss 0.239939, acc 0.890625\n",
      "2017-03-13T14:13:45.423741: step 109, loss 0.541176, acc 0.765625\n",
      "2017-03-13T14:13:45.603877: step 110, loss 0.323507, acc 0.859375\n",
      "2017-03-13T14:13:45.802010: step 111, loss 0.415823, acc 0.875\n",
      "2017-03-13T14:13:46.011159: step 112, loss 0.554071, acc 0.796875\n",
      "2017-03-13T14:13:46.198290: step 113, loss 0.598709, acc 0.828125\n",
      "2017-03-13T14:13:46.352406: step 114, loss 0.378926, acc 0.890625\n",
      "2017-03-13T14:13:46.532527: step 115, loss 0.379697, acc 0.875\n",
      "2017-03-13T14:13:46.709654: step 116, loss 0.594883, acc 0.828125\n",
      "2017-03-13T14:13:46.905795: step 117, loss 0.366269, acc 0.84375\n",
      "2017-03-13T14:13:47.086921: step 118, loss 0.237829, acc 0.890625\n",
      "2017-03-13T14:13:47.247037: step 119, loss 0.413349, acc 0.875\n",
      "2017-03-13T14:13:47.308078: step 120, loss 0.753627, acc 0.75\n",
      "2017-03-13T14:13:47.496212: step 121, loss 0.362414, acc 0.828125\n",
      "2017-03-13T14:13:47.687348: step 122, loss 0.321898, acc 0.875\n",
      "2017-03-13T14:13:47.881485: step 123, loss 0.407109, acc 0.84375\n",
      "2017-03-13T14:13:48.063614: step 124, loss 0.426694, acc 0.8125\n",
      "2017-03-13T14:13:48.235736: step 125, loss 0.361308, acc 0.828125\n",
      "2017-03-13T14:13:48.408859: step 126, loss 0.221033, acc 0.90625\n",
      "2017-03-13T14:13:48.578110: step 127, loss 0.434986, acc 0.859375\n",
      "2017-03-13T14:13:48.737624: step 128, loss 0.541023, acc 0.8125\n",
      "2017-03-13T14:13:48.928758: step 129, loss 0.278041, acc 0.875\n",
      "2017-03-13T14:13:49.106885: step 130, loss 0.342611, acc 0.890625\n",
      "2017-03-13T14:13:49.300022: step 131, loss 0.256275, acc 0.921875\n",
      "2017-03-13T14:13:49.481151: step 132, loss 0.211148, acc 0.890625\n",
      "2017-03-13T14:13:49.653272: step 133, loss 0.474797, acc 0.859375\n",
      "2017-03-13T14:13:49.797379: step 134, loss 0.276043, acc 0.9375\n",
      "2017-03-13T14:13:49.837383: step 135, loss 0.0257444, acc 1\n",
      "2017-03-13T14:13:50.006216: step 136, loss 0.278535, acc 0.921875\n",
      "2017-03-13T14:13:50.141721: step 137, loss 0.284279, acc 0.921875\n",
      "2017-03-13T14:13:50.280226: step 138, loss 0.338942, acc 0.90625\n",
      "2017-03-13T14:13:50.437338: step 139, loss 0.279452, acc 0.921875\n",
      "2017-03-13T14:13:50.599858: step 140, loss 0.453624, acc 0.828125\n",
      "2017-03-13T14:13:50.783833: step 141, loss 0.371368, acc 0.828125\n",
      "2017-03-13T14:13:50.979968: step 142, loss 0.60606, acc 0.796875\n",
      "2017-03-13T14:13:51.176110: step 143, loss 0.0792029, acc 0.984375\n",
      "2017-03-13T14:13:51.380252: step 144, loss 0.19414, acc 0.90625\n",
      "2017-03-13T14:13:51.570388: step 145, loss 0.182657, acc 0.890625\n",
      "2017-03-13T14:13:51.758520: step 146, loss 0.308467, acc 0.90625\n",
      "2017-03-13T14:13:51.947653: step 147, loss 0.215107, acc 0.90625\n",
      "2017-03-13T14:13:52.139794: step 148, loss 0.321439, acc 0.890625\n",
      "2017-03-13T14:13:52.281883: step 149, loss 0.462433, acc 0.84375\n",
      "2017-03-13T14:13:52.321880: step 150, loss 0.121303, acc 1\n",
      "2017-03-13T14:13:52.482147: step 151, loss 0.270407, acc 0.9375\n",
      "2017-03-13T14:13:52.611252: step 152, loss 0.115302, acc 0.96875\n",
      "2017-03-13T14:13:52.771799: step 153, loss 0.212023, acc 0.921875\n",
      "2017-03-13T14:13:52.955928: step 154, loss 0.245136, acc 0.921875\n",
      "2017-03-13T14:13:53.154068: step 155, loss 0.197017, acc 0.921875\n",
      "2017-03-13T14:13:53.292156: step 156, loss 0.16304, acc 0.9375\n",
      "2017-03-13T14:13:53.423591: step 157, loss 0.103237, acc 0.96875\n",
      "2017-03-13T14:13:53.571744: step 158, loss 0.322984, acc 0.84375\n",
      "2017-03-13T14:13:53.716600: step 159, loss 0.242536, acc 0.90625\n",
      "2017-03-13T14:13:53.866210: step 160, loss 0.209209, acc 0.890625\n",
      "2017-03-13T14:13:53.999746: step 161, loss 0.22004, acc 0.9375\n",
      "2017-03-13T14:13:54.144911: step 162, loss 0.447376, acc 0.859375\n",
      "2017-03-13T14:13:54.293911: step 163, loss 0.418507, acc 0.875\n",
      "2017-03-13T14:13:54.429205: step 164, loss 0.168432, acc 0.9375\n",
      "2017-03-13T14:13:54.469208: step 165, loss 0.0580123, acc 1\n",
      "2017-03-13T14:13:54.626971: step 166, loss 0.108496, acc 0.96875\n",
      "2017-03-13T14:13:54.766052: step 167, loss 0.201621, acc 0.96875\n",
      "2017-03-13T14:13:54.901347: step 168, loss 0.229641, acc 0.90625\n",
      "2017-03-13T14:13:55.055518: step 169, loss 0.216461, acc 0.90625\n",
      "2017-03-13T14:13:55.198281: step 170, loss 0.227002, acc 0.890625\n",
      "2017-03-13T14:13:55.349434: step 171, loss 0.238938, acc 0.84375\n",
      "2017-03-13T14:13:55.488989: step 172, loss 0.348126, acc 0.859375\n",
      "2017-03-13T14:13:55.632141: step 173, loss 0.257199, acc 0.9375\n",
      "2017-03-13T14:13:55.773755: step 174, loss 0.269686, acc 0.859375\n",
      "2017-03-13T14:13:55.922912: step 175, loss 0.288681, acc 0.90625\n",
      "2017-03-13T14:13:56.061273: step 176, loss 0.199045, acc 0.96875\n",
      "2017-03-13T14:13:56.212420: step 177, loss 0.15298, acc 0.9375\n",
      "2017-03-13T14:13:56.347252: step 178, loss 0.160389, acc 0.9375\n",
      "2017-03-13T14:13:56.496410: step 179, loss 0.365355, acc 0.84375\n",
      "2017-03-13T14:13:56.526408: step 180, loss 0.64453, acc 0.75\n",
      "2017-03-13T14:13:56.681876: step 181, loss 0.12978, acc 0.96875\n",
      "2017-03-13T14:13:56.812988: step 182, loss 0.264662, acc 0.875\n",
      "2017-03-13T14:13:56.941479: step 183, loss 0.201936, acc 0.9375\n",
      "2017-03-13T14:13:57.079625: step 184, loss 0.346631, acc 0.90625\n",
      "2017-03-13T14:13:57.263491: step 185, loss 0.123385, acc 0.9375\n",
      "2017-03-13T14:13:57.445621: step 186, loss 0.14238, acc 0.953125\n",
      "2017-03-13T14:13:57.630753: step 187, loss 0.251358, acc 0.921875\n",
      "2017-03-13T14:13:57.814884: step 188, loss 0.182828, acc 0.953125\n",
      "2017-03-13T14:13:57.992008: step 189, loss 0.122314, acc 0.984375\n",
      "2017-03-13T14:13:58.150121: step 190, loss 0.270536, acc 0.859375\n",
      "2017-03-13T14:13:58.305232: step 191, loss 0.150112, acc 0.953125\n",
      "2017-03-13T14:13:58.480355: step 192, loss 0.195517, acc 0.90625\n",
      "2017-03-13T14:13:58.679497: step 193, loss 0.227104, acc 0.890625\n",
      "2017-03-13T14:13:58.861625: step 194, loss 0.15392, acc 0.90625\n",
      "2017-03-13T14:13:58.914664: step 195, loss 1.05925, acc 0.75\n",
      "2017-03-13T14:13:59.103804: step 196, loss 0.129828, acc 0.9375\n",
      "2017-03-13T14:13:59.278922: step 197, loss 0.116086, acc 0.953125\n",
      "2017-03-13T14:13:59.449046: step 198, loss 0.20948, acc 0.9375\n",
      "2017-03-13T14:13:59.632173: step 199, loss 0.223298, acc 0.90625\n",
      "2017-03-13T14:13:59.827314: step 200, loss 0.177585, acc 0.890625\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:13:59.946394: step 200, loss 0.399691, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-200\n",
      "\n",
      "2017-03-13T14:14:00.791376: step 201, loss 0.106873, acc 0.921875\n",
      "2017-03-13T14:14:01.020538: step 202, loss 0.144328, acc 0.9375\n",
      "2017-03-13T14:14:01.241697: step 203, loss 0.136654, acc 0.953125\n",
      "2017-03-13T14:14:01.446842: step 204, loss 0.119881, acc 0.953125\n",
      "2017-03-13T14:14:01.636975: step 205, loss 0.10121, acc 0.953125\n",
      "2017-03-13T14:14:01.796089: step 206, loss 0.250386, acc 0.9375\n",
      "2017-03-13T14:14:01.949265: step 207, loss 0.241542, acc 0.96875\n",
      "2017-03-13T14:14:02.098373: step 208, loss 0.177057, acc 0.953125\n",
      "2017-03-13T14:14:02.236469: step 209, loss 0.24521, acc 0.921875\n",
      "2017-03-13T14:14:02.278499: step 210, loss 0.855439, acc 0.5\n",
      "2017-03-13T14:14:02.424675: step 211, loss 0.115408, acc 0.953125\n",
      "2017-03-13T14:14:02.570966: step 212, loss 0.195155, acc 0.90625\n",
      "2017-03-13T14:14:02.708198: step 213, loss 0.227561, acc 0.90625\n",
      "2017-03-13T14:14:02.852964: step 214, loss 0.207193, acc 0.9375\n",
      "2017-03-13T14:14:02.998139: step 215, loss 0.274587, acc 0.890625\n",
      "2017-03-13T14:14:03.142725: step 216, loss 0.258706, acc 0.84375\n",
      "2017-03-13T14:14:03.284902: step 217, loss 0.240257, acc 0.890625\n",
      "2017-03-13T14:14:03.416510: step 218, loss 0.252082, acc 0.953125\n",
      "2017-03-13T14:14:03.584630: step 219, loss 0.0859103, acc 0.953125\n",
      "2017-03-13T14:14:03.771762: step 220, loss 0.171271, acc 0.9375\n",
      "2017-03-13T14:14:03.945886: step 221, loss 0.135365, acc 0.96875\n",
      "2017-03-13T14:14:04.092990: step 222, loss 0.254769, acc 0.890625\n",
      "2017-03-13T14:14:04.238145: step 223, loss 0.110555, acc 0.953125\n",
      "2017-03-13T14:14:04.389652: step 224, loss 0.447458, acc 0.859375\n",
      "2017-03-13T14:14:04.440691: step 225, loss 0.0746289, acc 1\n",
      "2017-03-13T14:14:04.595000: step 226, loss 0.138123, acc 0.9375\n",
      "2017-03-13T14:14:04.784134: step 227, loss 0.217316, acc 0.921875\n",
      "2017-03-13T14:14:04.954255: step 228, loss 0.274055, acc 0.890625\n",
      "2017-03-13T14:14:05.144390: step 229, loss 0.126413, acc 0.953125\n",
      "2017-03-13T14:14:05.328523: step 230, loss 0.0852734, acc 0.953125\n",
      "2017-03-13T14:14:05.516654: step 231, loss 0.204512, acc 0.90625\n",
      "2017-03-13T14:14:05.678770: step 232, loss 0.0703845, acc 0.96875\n",
      "2017-03-13T14:14:05.847890: step 233, loss 0.168957, acc 0.9375\n",
      "2017-03-13T14:14:06.020012: step 234, loss 0.210231, acc 0.921875\n",
      "2017-03-13T14:14:06.195135: step 235, loss 0.203682, acc 0.890625\n",
      "2017-03-13T14:14:06.380270: step 236, loss 0.162361, acc 0.953125\n",
      "2017-03-13T14:14:06.576407: step 237, loss 0.12275, acc 0.953125\n",
      "2017-03-13T14:14:06.770548: step 238, loss 0.200068, acc 0.90625\n",
      "2017-03-13T14:14:06.960679: step 239, loss 0.195825, acc 0.953125\n",
      "2017-03-13T14:14:07.013717: step 240, loss 0.064565, acc 1\n",
      "2017-03-13T14:14:07.181837: step 241, loss 0.128965, acc 0.9375\n",
      "2017-03-13T14:14:07.379977: step 242, loss 0.17029, acc 0.90625\n",
      "2017-03-13T14:14:07.543092: step 243, loss 0.106415, acc 0.96875\n",
      "2017-03-13T14:14:07.726222: step 244, loss 0.24783, acc 0.890625\n",
      "2017-03-13T14:14:07.903353: step 245, loss 0.104442, acc 0.984375\n",
      "2017-03-13T14:14:08.079474: step 246, loss 0.0651763, acc 0.96875\n",
      "2017-03-13T14:14:08.261601: step 247, loss 0.0492999, acc 0.984375\n",
      "2017-03-13T14:14:08.420715: step 248, loss 0.123943, acc 0.96875\n",
      "2017-03-13T14:14:08.600842: step 249, loss 0.111397, acc 0.9375\n",
      "2017-03-13T14:14:08.793979: step 250, loss 0.203719, acc 0.9375\n",
      "2017-03-13T14:14:08.980115: step 251, loss 0.283507, acc 0.921875\n",
      "2017-03-13T14:14:09.166243: step 252, loss 0.221373, acc 0.90625\n",
      "2017-03-13T14:14:09.361381: step 253, loss 0.214223, acc 0.9375\n",
      "2017-03-13T14:14:09.525498: step 254, loss 0.0723136, acc 0.984375\n",
      "2017-03-13T14:14:09.578535: step 255, loss 0.132894, acc 1\n",
      "2017-03-13T14:14:09.775676: step 256, loss 0.190686, acc 0.9375\n",
      "2017-03-13T14:14:09.946797: step 257, loss 0.0814205, acc 0.984375\n",
      "2017-03-13T14:14:10.145938: step 258, loss 0.317114, acc 0.890625\n",
      "2017-03-13T14:14:10.334072: step 259, loss 0.189057, acc 0.921875\n",
      "2017-03-13T14:14:10.509199: step 260, loss 0.422906, acc 0.84375\n",
      "2017-03-13T14:14:10.696329: step 261, loss 0.138236, acc 0.953125\n",
      "2017-03-13T14:14:10.870454: step 262, loss 0.115338, acc 0.953125\n",
      "2017-03-13T14:14:11.055584: step 263, loss 0.269139, acc 0.953125\n",
      "2017-03-13T14:14:11.227706: step 264, loss 0.109478, acc 0.953125\n",
      "2017-03-13T14:14:11.422845: step 265, loss 0.194504, acc 0.96875\n",
      "2017-03-13T14:14:11.590964: step 266, loss 0.0794918, acc 0.96875\n",
      "2017-03-13T14:14:11.772092: step 267, loss 0.225081, acc 0.9375\n",
      "2017-03-13T14:14:11.939210: step 268, loss 0.0726452, acc 0.96875\n",
      "2017-03-13T14:14:12.118338: step 269, loss 0.112221, acc 0.96875\n",
      "2017-03-13T14:14:12.171375: step 270, loss 1.29079, acc 0.75\n",
      "2017-03-13T14:14:12.355505: step 271, loss 0.0919618, acc 0.96875\n",
      "2017-03-13T14:14:12.548643: step 272, loss 0.101649, acc 0.96875\n",
      "2017-03-13T14:14:12.735776: step 273, loss 0.0862444, acc 0.953125\n",
      "2017-03-13T14:14:12.903895: step 274, loss 0.109518, acc 0.96875\n",
      "2017-03-13T14:14:13.079018: step 275, loss 0.127571, acc 0.9375\n",
      "2017-03-13T14:14:13.253146: step 276, loss 0.0834762, acc 0.96875\n",
      "2017-03-13T14:14:13.448283: step 277, loss 0.0339046, acc 1\n",
      "2017-03-13T14:14:13.626407: step 278, loss 0.154878, acc 0.953125\n",
      "2017-03-13T14:14:13.810538: step 279, loss 0.207717, acc 0.9375\n",
      "2017-03-13T14:14:13.985662: step 280, loss 0.140963, acc 0.953125\n",
      "2017-03-13T14:14:14.164790: step 281, loss 0.0717767, acc 0.96875\n",
      "2017-03-13T14:14:14.353924: step 282, loss 0.0754368, acc 0.984375\n",
      "2017-03-13T14:14:14.529048: step 283, loss 0.142309, acc 0.921875\n",
      "2017-03-13T14:14:14.711181: step 284, loss 0.147661, acc 0.953125\n",
      "2017-03-13T14:14:14.763213: step 285, loss 0.0560655, acc 1\n",
      "2017-03-13T14:14:14.960353: step 286, loss 0.0991697, acc 0.953125\n",
      "2017-03-13T14:14:15.146485: step 287, loss 0.0489807, acc 0.984375\n",
      "2017-03-13T14:14:15.315606: step 288, loss 0.0970639, acc 0.96875\n",
      "2017-03-13T14:14:15.484726: step 289, loss 0.0725268, acc 0.96875\n",
      "2017-03-13T14:14:15.655847: step 290, loss 0.133116, acc 0.96875\n",
      "2017-03-13T14:14:15.797487: step 291, loss 0.14553, acc 0.953125\n",
      "2017-03-13T14:14:15.967609: step 292, loss 0.0538839, acc 0.984375\n",
      "2017-03-13T14:14:16.146735: step 293, loss 0.188524, acc 0.9375\n",
      "2017-03-13T14:14:16.343875: step 294, loss 0.214536, acc 0.90625\n",
      "2017-03-13T14:14:16.522001: step 295, loss 0.0686149, acc 0.984375\n",
      "2017-03-13T14:14:16.710135: step 296, loss 0.0737053, acc 0.953125\n",
      "2017-03-13T14:14:16.888261: step 297, loss 0.113852, acc 0.953125\n",
      "2017-03-13T14:14:17.065387: step 298, loss 0.0437707, acc 1\n",
      "2017-03-13T14:14:17.226502: step 299, loss 0.0651186, acc 0.984375\n",
      "2017-03-13T14:14:17.283541: step 300, loss 0.0355085, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:14:17.380610: step 300, loss 0.38069, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-300\n",
      "\n",
      "2017-03-13T14:14:19.915524: step 301, loss 0.0527696, acc 1\n",
      "2017-03-13T14:14:20.078640: step 302, loss 0.0569322, acc 0.984375\n",
      "2017-03-13T14:14:20.220066: step 303, loss 0.150231, acc 0.953125\n",
      "2017-03-13T14:14:20.402639: step 304, loss 0.0783295, acc 0.984375\n",
      "2017-03-13T14:14:20.570758: step 305, loss 0.11121, acc 0.953125\n",
      "2017-03-13T14:14:20.747886: step 306, loss 0.0610923, acc 0.984375\n",
      "2017-03-13T14:14:20.939024: step 307, loss 0.0840166, acc 0.984375\n",
      "2017-03-13T14:14:21.118147: step 308, loss 0.0517276, acc 0.984375\n",
      "2017-03-13T14:14:21.269253: step 309, loss 0.0760706, acc 0.96875\n",
      "2017-03-13T14:14:21.417524: step 310, loss 0.0337971, acc 0.984375\n",
      "2017-03-13T14:14:21.578638: step 311, loss 0.160444, acc 0.9375\n",
      "2017-03-13T14:14:21.714587: step 312, loss 0.156682, acc 0.921875\n",
      "2017-03-13T14:14:21.854233: step 313, loss 0.0941242, acc 0.953125\n",
      "2017-03-13T14:14:21.996750: step 314, loss 0.0889207, acc 0.96875\n",
      "2017-03-13T14:14:22.029773: step 315, loss 1.05329, acc 0.5\n",
      "2017-03-13T14:14:22.174284: step 316, loss 0.269351, acc 0.90625\n",
      "2017-03-13T14:14:22.314011: step 317, loss 0.0913282, acc 0.96875\n",
      "2017-03-13T14:14:22.454167: step 318, loss 0.064821, acc 0.984375\n",
      "2017-03-13T14:14:22.587858: step 319, loss 0.0349531, acc 0.984375\n",
      "2017-03-13T14:14:22.733963: step 320, loss 0.13505, acc 0.96875\n",
      "2017-03-13T14:14:22.877178: step 321, loss 0.207248, acc 0.953125\n",
      "2017-03-13T14:14:23.046299: step 322, loss 0.0781528, acc 0.984375\n",
      "2017-03-13T14:14:23.237434: step 323, loss 0.0586235, acc 0.984375\n",
      "2017-03-13T14:14:23.416560: step 324, loss 0.0984311, acc 0.953125\n",
      "2017-03-13T14:14:23.593688: step 325, loss 0.107377, acc 0.953125\n",
      "2017-03-13T14:14:23.769811: step 326, loss 0.0767252, acc 0.96875\n",
      "2017-03-13T14:14:23.956945: step 327, loss 0.0840844, acc 0.96875\n",
      "2017-03-13T14:14:24.124063: step 328, loss 0.204265, acc 0.953125\n",
      "2017-03-13T14:14:24.297185: step 329, loss 0.0623059, acc 0.984375\n",
      "2017-03-13T14:14:24.350225: step 330, loss 7.83747e-05, acc 1\n",
      "2017-03-13T14:14:24.501331: step 331, loss 0.0212012, acc 1\n",
      "2017-03-13T14:14:24.674453: step 332, loss 0.159644, acc 0.90625\n",
      "2017-03-13T14:14:24.856582: step 333, loss 0.059541, acc 0.984375\n",
      "2017-03-13T14:14:25.043715: step 334, loss 0.0348704, acc 1\n",
      "2017-03-13T14:14:25.204829: step 335, loss 0.0849392, acc 0.96875\n",
      "2017-03-13T14:14:25.355612: step 336, loss 0.119541, acc 0.953125\n",
      "2017-03-13T14:14:25.508945: step 337, loss 0.066842, acc 0.984375\n",
      "2017-03-13T14:14:25.658778: step 338, loss 0.129381, acc 0.96875\n",
      "2017-03-13T14:14:25.814889: step 339, loss 0.0571551, acc 0.96875\n",
      "2017-03-13T14:14:26.014030: step 340, loss 0.0974441, acc 0.953125\n",
      "2017-03-13T14:14:26.197161: step 341, loss 0.124901, acc 0.984375\n",
      "2017-03-13T14:14:26.385293: step 342, loss 0.114684, acc 0.953125\n",
      "2017-03-13T14:14:26.563420: step 343, loss 0.134518, acc 0.953125\n",
      "2017-03-13T14:14:26.740546: step 344, loss 0.0868868, acc 0.96875\n",
      "2017-03-13T14:14:26.793583: step 345, loss 0.197434, acc 1\n",
      "2017-03-13T14:14:26.980720: step 346, loss 0.0876263, acc 0.96875\n",
      "2017-03-13T14:14:27.153839: step 347, loss 0.128736, acc 0.96875\n",
      "2017-03-13T14:14:27.326966: step 348, loss 0.0835149, acc 0.96875\n",
      "2017-03-13T14:14:27.485074: step 349, loss 0.135982, acc 0.953125\n",
      "2017-03-13T14:14:27.658197: step 350, loss 0.0272397, acc 1\n",
      "2017-03-13T14:14:27.835322: step 351, loss 0.0719525, acc 0.953125\n",
      "2017-03-13T14:14:28.023456: step 352, loss 0.0440057, acc 0.984375\n",
      "2017-03-13T14:14:28.188573: step 353, loss 0.130202, acc 0.921875\n",
      "2017-03-13T14:14:28.357695: step 354, loss 0.0150339, acc 1\n",
      "2017-03-13T14:14:28.515806: step 355, loss 0.0258975, acc 1\n",
      "2017-03-13T14:14:28.703938: step 356, loss 0.0323675, acc 0.984375\n",
      "2017-03-13T14:14:28.862050: step 357, loss 0.0330988, acc 1\n",
      "2017-03-13T14:14:29.070199: step 358, loss 0.0762233, acc 0.984375\n",
      "2017-03-13T14:14:29.239318: step 359, loss 0.129507, acc 0.953125\n",
      "2017-03-13T14:14:29.288353: step 360, loss 0.0281341, acc 1\n",
      "2017-03-13T14:14:29.473485: step 361, loss 0.0901197, acc 0.953125\n",
      "2017-03-13T14:14:29.656614: step 362, loss 0.0574546, acc 0.96875\n",
      "2017-03-13T14:14:29.833739: step 363, loss 0.0713192, acc 0.96875\n",
      "2017-03-13T14:14:29.983846: step 364, loss 0.0434581, acc 1\n",
      "2017-03-13T14:14:30.154972: step 365, loss 0.131608, acc 0.953125\n",
      "2017-03-13T14:14:30.339099: step 366, loss 0.094678, acc 0.96875\n",
      "2017-03-13T14:14:30.525232: step 367, loss 0.0850044, acc 0.984375\n",
      "2017-03-13T14:14:30.683342: step 368, loss 0.0639333, acc 0.984375\n",
      "2017-03-13T14:14:30.876482: step 369, loss 0.087304, acc 0.953125\n",
      "2017-03-13T14:14:31.033590: step 370, loss 0.0419768, acc 0.984375\n",
      "2017-03-13T14:14:31.214720: step 371, loss 0.147278, acc 0.953125\n",
      "2017-03-13T14:14:31.379837: step 372, loss 0.0544176, acc 0.96875\n",
      "2017-03-13T14:14:31.553960: step 373, loss 0.0757527, acc 0.96875\n",
      "2017-03-13T14:14:31.702068: step 374, loss 0.030672, acc 1\n",
      "2017-03-13T14:14:31.758109: step 375, loss 0.000764584, acc 1\n",
      "2017-03-13T14:14:31.932229: step 376, loss 0.0286711, acc 0.984375\n",
      "2017-03-13T14:14:32.115359: step 377, loss 0.114567, acc 0.96875\n",
      "2017-03-13T14:14:32.287480: step 378, loss 0.0951005, acc 0.9375\n",
      "2017-03-13T14:14:32.443591: step 379, loss 0.045044, acc 0.984375\n",
      "2017-03-13T14:14:32.615713: step 380, loss 0.0747933, acc 0.984375\n",
      "2017-03-13T14:14:32.785834: step 381, loss 0.0561171, acc 0.984375\n",
      "2017-03-13T14:14:32.971968: step 382, loss 0.0721481, acc 0.96875\n",
      "2017-03-13T14:14:33.136082: step 383, loss 0.0324981, acc 0.984375\n",
      "2017-03-13T14:14:33.330220: step 384, loss 0.0140776, acc 1\n",
      "2017-03-13T14:14:33.527360: step 385, loss 0.146482, acc 0.953125\n",
      "2017-03-13T14:14:33.722498: step 386, loss 0.0243297, acc 1\n",
      "2017-03-13T14:14:33.872605: step 387, loss 0.0329149, acc 1\n",
      "2017-03-13T14:14:34.043726: step 388, loss 0.0252386, acc 1\n",
      "2017-03-13T14:14:34.197836: step 389, loss 0.106351, acc 0.96875\n",
      "2017-03-13T14:14:34.250873: step 390, loss 0.0463598, acc 1\n",
      "2017-03-13T14:14:34.424001: step 391, loss 0.062347, acc 0.984375\n",
      "2017-03-13T14:14:34.612130: step 392, loss 0.0716953, acc 0.984375\n",
      "2017-03-13T14:14:34.791259: step 393, loss 0.0391733, acc 0.984375\n",
      "2017-03-13T14:14:34.969384: step 394, loss 0.15829, acc 0.9375\n",
      "2017-03-13T14:14:35.140504: step 395, loss 0.0616003, acc 0.984375\n",
      "2017-03-13T14:14:35.295615: step 396, loss 0.0848984, acc 0.953125\n",
      "2017-03-13T14:14:35.478746: step 397, loss 0.0650788, acc 0.953125\n",
      "2017-03-13T14:14:35.656873: step 398, loss 0.0311772, acc 1\n",
      "2017-03-13T14:14:35.857014: step 399, loss 0.0441544, acc 0.96875\n",
      "2017-03-13T14:14:36.012122: step 400, loss 0.0278014, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:14:36.108191: step 400, loss 0.380068, acc 0.85\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-400\n",
      "\n",
      "2017-03-13T14:14:37.098858: step 401, loss 0.0324496, acc 1\n",
      "2017-03-13T14:14:37.279423: step 402, loss 0.0986906, acc 0.953125\n",
      "2017-03-13T14:14:37.497578: step 403, loss 0.0963104, acc 0.984375\n",
      "2017-03-13T14:14:37.678707: step 404, loss 0.0212566, acc 1\n",
      "2017-03-13T14:14:37.732746: step 405, loss 0.0231077, acc 1\n",
      "2017-03-13T14:14:37.874189: step 406, loss 0.105416, acc 0.9375\n",
      "2017-03-13T14:14:38.058320: step 407, loss 0.0449951, acc 0.984375\n",
      "2017-03-13T14:14:38.222438: step 408, loss 0.0154719, acc 1\n",
      "2017-03-13T14:14:38.404568: step 409, loss 0.044853, acc 0.984375\n",
      "2017-03-13T14:14:38.581695: step 410, loss 0.0304068, acc 1\n",
      "2017-03-13T14:14:38.766823: step 411, loss 0.150519, acc 0.96875\n",
      "2017-03-13T14:14:38.919933: step 412, loss 0.0409293, acc 0.984375\n",
      "2017-03-13T14:14:39.115070: step 413, loss 0.143416, acc 0.984375\n",
      "2017-03-13T14:14:39.279188: step 414, loss 0.108705, acc 0.953125\n",
      "2017-03-13T14:14:39.464318: step 415, loss 0.0693427, acc 0.96875\n",
      "2017-03-13T14:14:39.624432: step 416, loss 0.0461268, acc 0.984375\n",
      "2017-03-13T14:14:39.805560: step 417, loss 0.0197732, acc 1\n",
      "2017-03-13T14:14:39.989691: step 418, loss 0.0909505, acc 0.984375\n",
      "2017-03-13T14:14:40.156810: step 419, loss 0.0661928, acc 0.96875\n",
      "2017-03-13T14:14:40.203842: step 420, loss 0.012243, acc 1\n",
      "2017-03-13T14:14:40.353949: step 421, loss 0.0892517, acc 0.984375\n",
      "2017-03-13T14:14:40.507058: step 422, loss 0.0698288, acc 0.953125\n",
      "2017-03-13T14:14:40.660299: step 423, loss 0.0618703, acc 0.984375\n",
      "2017-03-13T14:14:40.793871: step 424, loss 0.0522453, acc 0.984375\n",
      "2017-03-13T14:14:40.942977: step 425, loss 0.16583, acc 0.921875\n",
      "2017-03-13T14:14:41.079558: step 426, loss 0.0665452, acc 0.96875\n",
      "2017-03-13T14:14:41.216655: step 427, loss 0.0659808, acc 0.984375\n",
      "2017-03-13T14:14:41.358136: step 428, loss 0.0867249, acc 0.96875\n",
      "2017-03-13T14:14:41.489229: step 429, loss 0.106642, acc 0.96875\n",
      "2017-03-13T14:14:41.643338: step 430, loss 0.0515667, acc 0.984375\n",
      "2017-03-13T14:14:41.766426: step 431, loss 0.104996, acc 0.984375\n",
      "2017-03-13T14:14:41.930542: step 432, loss 0.111978, acc 0.9375\n",
      "2017-03-13T14:14:42.081649: step 433, loss 0.0363667, acc 1\n",
      "2017-03-13T14:14:42.214947: step 434, loss 0.0636204, acc 0.96875\n",
      "2017-03-13T14:14:42.246971: step 435, loss 0.00121527, acc 1\n",
      "2017-03-13T14:14:42.383527: step 436, loss 0.0237893, acc 1\n",
      "2017-03-13T14:14:42.530540: step 437, loss 0.157781, acc 0.953125\n",
      "2017-03-13T14:14:42.676645: step 438, loss 0.0339423, acc 0.984375\n",
      "2017-03-13T14:14:42.814549: step 439, loss 0.0560083, acc 0.984375\n",
      "2017-03-13T14:14:42.941639: step 440, loss 0.117194, acc 0.96875\n",
      "2017-03-13T14:14:43.094753: step 441, loss 0.0498843, acc 0.96875\n",
      "2017-03-13T14:14:43.223839: step 442, loss 0.0256547, acc 0.984375\n",
      "2017-03-13T14:14:43.380951: step 443, loss 0.0148269, acc 1\n",
      "2017-03-13T14:14:43.531058: step 444, loss 0.0166577, acc 1\n",
      "2017-03-13T14:14:43.665929: step 445, loss 0.0779685, acc 0.96875\n",
      "2017-03-13T14:14:43.818037: step 446, loss 0.0339126, acc 1\n",
      "2017-03-13T14:14:43.951002: step 447, loss 0.0791409, acc 0.96875\n",
      "2017-03-13T14:14:44.099107: step 448, loss 0.0226795, acc 1\n",
      "2017-03-13T14:14:44.239426: step 449, loss 0.0448483, acc 0.984375\n",
      "2017-03-13T14:14:44.275451: step 450, loss 0.0122704, acc 1\n",
      "2017-03-13T14:14:44.420786: step 451, loss 0.0704258, acc 0.984375\n",
      "2017-03-13T14:14:44.564618: step 452, loss 0.0362468, acc 0.984375\n",
      "2017-03-13T14:14:44.717474: step 453, loss 0.0500156, acc 0.984375\n",
      "2017-03-13T14:14:44.853521: step 454, loss 0.0175726, acc 1\n",
      "2017-03-13T14:14:45.000626: step 455, loss 0.0231467, acc 1\n",
      "2017-03-13T14:14:45.144808: step 456, loss 0.0229173, acc 1\n",
      "2017-03-13T14:14:45.291913: step 457, loss 0.0710788, acc 0.96875\n",
      "2017-03-13T14:14:45.428786: step 458, loss 0.0520255, acc 0.984375\n",
      "2017-03-13T14:14:45.587903: step 459, loss 0.0594429, acc 0.96875\n",
      "2017-03-13T14:14:45.730774: step 460, loss 0.0192149, acc 1\n",
      "2017-03-13T14:14:45.875878: step 461, loss 0.0403707, acc 0.984375\n",
      "2017-03-13T14:14:46.015606: step 462, loss 0.139991, acc 0.921875\n",
      "2017-03-13T14:14:46.141695: step 463, loss 0.109378, acc 0.96875\n",
      "2017-03-13T14:14:46.291806: step 464, loss 0.0374495, acc 0.984375\n",
      "2017-03-13T14:14:46.328829: step 465, loss 0.0504188, acc 1\n",
      "2017-03-13T14:14:46.477934: step 466, loss 0.0862266, acc 0.96875\n",
      "2017-03-13T14:14:46.619034: step 467, loss 0.0288293, acc 0.984375\n",
      "2017-03-13T14:14:46.768250: step 468, loss 0.051689, acc 0.984375\n",
      "2017-03-13T14:14:46.901732: step 469, loss 0.115144, acc 0.96875\n",
      "2017-03-13T14:14:47.048837: step 470, loss 0.0427378, acc 0.984375\n",
      "2017-03-13T14:14:47.196531: step 471, loss 0.0137235, acc 1\n",
      "2017-03-13T14:14:47.345638: step 472, loss 0.0749458, acc 0.953125\n",
      "2017-03-13T14:14:47.477600: step 473, loss 0.0754224, acc 0.96875\n",
      "2017-03-13T14:14:47.628708: step 474, loss 0.0120136, acc 1\n",
      "2017-03-13T14:14:47.771883: step 475, loss 0.0516102, acc 0.984375\n",
      "2017-03-13T14:14:47.918038: step 476, loss 0.0449519, acc 0.96875\n",
      "2017-03-13T14:14:48.053626: step 477, loss 0.0690677, acc 0.96875\n",
      "2017-03-13T14:14:48.204735: step 478, loss 0.0648604, acc 0.984375\n",
      "2017-03-13T14:14:48.344871: step 479, loss 0.0295001, acc 1\n",
      "2017-03-13T14:14:48.379896: step 480, loss 0.0138135, acc 1\n",
      "2017-03-13T14:14:48.522116: step 481, loss 0.0473106, acc 0.984375\n",
      "2017-03-13T14:14:48.659167: step 482, loss 0.0256782, acc 0.984375\n",
      "2017-03-13T14:14:48.810274: step 483, loss 0.0133367, acc 1\n",
      "2017-03-13T14:14:48.948185: step 484, loss 0.0102595, acc 1\n",
      "2017-03-13T14:14:49.095290: step 485, loss 0.0218106, acc 1\n",
      "2017-03-13T14:14:49.229107: step 486, loss 0.00958189, acc 1\n",
      "2017-03-13T14:14:49.386219: step 487, loss 0.0418819, acc 0.953125\n",
      "2017-03-13T14:14:49.525394: step 488, loss 0.0521459, acc 0.984375\n",
      "2017-03-13T14:14:49.679503: step 489, loss 0.0302883, acc 1\n",
      "2017-03-13T14:14:49.805052: step 490, loss 0.03023, acc 0.984375\n",
      "2017-03-13T14:14:49.955160: step 491, loss 0.00910273, acc 1\n",
      "2017-03-13T14:14:50.099024: step 492, loss 0.0281291, acc 0.984375\n",
      "2017-03-13T14:14:50.250131: step 493, loss 0.0547731, acc 0.984375\n",
      "2017-03-13T14:14:50.385322: step 494, loss 0.0594887, acc 0.984375\n",
      "2017-03-13T14:14:50.419347: step 495, loss 0.0480671, acc 1\n",
      "2017-03-13T14:14:50.572455: step 496, loss 0.0368716, acc 0.984375\n",
      "2017-03-13T14:14:50.715647: step 497, loss 0.0316491, acc 0.984375\n",
      "2017-03-13T14:14:50.867755: step 498, loss 0.090818, acc 0.9375\n",
      "2017-03-13T14:14:51.000480: step 499, loss 0.0138389, acc 1\n",
      "2017-03-13T14:14:51.149586: step 500, loss 0.0134451, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:14:51.228373: step 500, loss 0.367836, acc 0.84\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-500\n",
      "\n",
      "2017-03-13T14:14:51.886784: step 501, loss 0.0306465, acc 0.984375\n",
      "2017-03-13T14:14:52.014337: step 502, loss 0.0510177, acc 0.984375\n",
      "2017-03-13T14:14:52.150434: step 503, loss 0.0463314, acc 0.984375\n",
      "2017-03-13T14:14:52.327560: step 504, loss 0.094028, acc 0.96875\n",
      "2017-03-13T14:14:52.531705: step 505, loss 0.101417, acc 0.96875\n",
      "2017-03-13T14:14:52.739235: step 506, loss 0.016427, acc 1\n",
      "2017-03-13T14:14:52.923360: step 507, loss 0.0632183, acc 0.984375\n",
      "2017-03-13T14:14:53.066461: step 508, loss 0.03614, acc 0.984375\n",
      "2017-03-13T14:14:53.203558: step 509, loss 0.0162519, acc 1\n",
      "2017-03-13T14:14:53.250591: step 510, loss 0.0150382, acc 1\n",
      "2017-03-13T14:14:53.385754: step 511, loss 0.0305031, acc 0.984375\n",
      "2017-03-13T14:14:53.536910: step 512, loss 0.0278049, acc 0.984375\n",
      "2017-03-13T14:14:53.699003: step 513, loss 0.0134253, acc 1\n",
      "2017-03-13T14:14:53.831096: step 514, loss 0.0670522, acc 0.984375\n",
      "2017-03-13T14:14:53.988209: step 515, loss 0.0581782, acc 0.984375\n",
      "2017-03-13T14:14:54.137314: step 516, loss 0.0329831, acc 0.984375\n",
      "2017-03-13T14:14:54.308437: step 517, loss 0.0490406, acc 0.984375\n",
      "2017-03-13T14:14:54.460543: step 518, loss 0.0408266, acc 1\n",
      "2017-03-13T14:14:54.599556: step 519, loss 0.112489, acc 0.953125\n",
      "2017-03-13T14:14:54.763673: step 520, loss 0.0749653, acc 0.953125\n",
      "2017-03-13T14:14:54.909940: step 521, loss 0.0404995, acc 0.984375\n",
      "2017-03-13T14:14:55.064049: step 522, loss 0.026959, acc 1\n",
      "2017-03-13T14:14:55.225558: step 523, loss 0.0390925, acc 0.984375\n",
      "2017-03-13T14:14:55.371662: step 524, loss 0.0105437, acc 1\n",
      "2017-03-13T14:14:55.410689: step 525, loss 0.125554, acc 1\n",
      "2017-03-13T14:14:55.546178: step 526, loss 0.0255056, acc 1\n",
      "2017-03-13T14:14:55.688279: step 527, loss 0.0566148, acc 0.96875\n",
      "2017-03-13T14:14:55.843388: step 528, loss 0.0725549, acc 0.96875\n",
      "2017-03-13T14:14:55.971480: step 529, loss 0.0100084, acc 1\n",
      "2017-03-13T14:14:56.136597: step 530, loss 0.00972689, acc 1\n",
      "2017-03-13T14:14:56.296711: step 531, loss 0.0188745, acc 1\n",
      "2017-03-13T14:14:56.443814: step 532, loss 0.01748, acc 1\n",
      "2017-03-13T14:14:56.596923: step 533, loss 0.0147355, acc 1\n",
      "2017-03-13T14:14:56.729529: step 534, loss 0.0105663, acc 1\n",
      "2017-03-13T14:14:56.861623: step 535, loss 0.0583872, acc 0.984375\n",
      "2017-03-13T14:14:57.003725: step 536, loss 0.070142, acc 0.96875\n",
      "2017-03-13T14:14:57.143823: step 537, loss 0.00992996, acc 1\n",
      "2017-03-13T14:14:57.303939: step 538, loss 0.0137735, acc 1\n",
      "2017-03-13T14:14:57.458047: step 539, loss 0.0502259, acc 0.984375\n",
      "2017-03-13T14:14:57.501076: step 540, loss 0.00491169, acc 1\n",
      "2017-03-13T14:14:57.638918: step 541, loss 0.0463881, acc 0.984375\n",
      "2017-03-13T14:14:57.789025: step 542, loss 0.0628928, acc 0.96875\n",
      "2017-03-13T14:14:57.928921: step 543, loss 0.0186354, acc 1\n",
      "2017-03-13T14:14:58.059013: step 544, loss 0.124019, acc 0.984375\n",
      "2017-03-13T14:14:58.211123: step 545, loss 0.00960139, acc 1\n",
      "2017-03-13T14:14:58.370234: step 546, loss 0.0169, acc 1\n",
      "2017-03-13T14:14:58.509332: step 547, loss 0.0213655, acc 1\n",
      "2017-03-13T14:14:58.662441: step 548, loss 0.0590288, acc 0.984375\n",
      "2017-03-13T14:14:58.809840: step 549, loss 0.0402882, acc 1\n",
      "2017-03-13T14:14:58.963954: step 550, loss 0.0231099, acc 1\n",
      "2017-03-13T14:14:59.101518: step 551, loss 0.0180011, acc 1\n",
      "2017-03-13T14:14:59.250625: step 552, loss 0.00643878, acc 1\n",
      "2017-03-13T14:14:59.390347: step 553, loss 0.014873, acc 1\n",
      "2017-03-13T14:14:59.536451: step 554, loss 0.01663, acc 1\n",
      "2017-03-13T14:14:59.572604: step 555, loss 0.000984637, acc 1\n",
      "2017-03-13T14:14:59.711432: step 556, loss 0.0377321, acc 0.96875\n",
      "2017-03-13T14:14:59.867542: step 557, loss 0.0681852, acc 0.984375\n",
      "2017-03-13T14:15:00.000235: step 558, loss 0.0213562, acc 0.984375\n",
      "2017-03-13T14:15:00.153344: step 559, loss 0.0212859, acc 1\n",
      "2017-03-13T14:15:00.299448: step 560, loss 0.0171692, acc 1\n",
      "2017-03-13T14:15:00.441549: step 561, loss 0.0102335, acc 1\n",
      "2017-03-13T14:15:00.583666: step 562, loss 0.0274913, acc 0.984375\n",
      "2017-03-13T14:15:00.736774: step 563, loss 0.0111613, acc 1\n",
      "2017-03-13T14:15:00.884018: step 564, loss 0.00486146, acc 1\n",
      "2017-03-13T14:15:01.046136: step 565, loss 0.0116682, acc 1\n",
      "2017-03-13T14:15:01.187606: step 566, loss 0.015313, acc 1\n",
      "2017-03-13T14:15:01.335712: step 567, loss 0.00696522, acc 1\n",
      "2017-03-13T14:15:01.491331: step 568, loss 0.0147978, acc 1\n",
      "2017-03-13T14:15:01.640437: step 569, loss 0.0345588, acc 0.984375\n",
      "2017-03-13T14:15:01.678464: step 570, loss 0.00598421, acc 1\n",
      "2017-03-13T14:15:01.815404: step 571, loss 0.0440293, acc 0.984375\n",
      "2017-03-13T14:15:01.953502: step 572, loss 0.023215, acc 0.984375\n",
      "2017-03-13T14:15:02.104153: step 573, loss 0.03984, acc 0.984375\n",
      "2017-03-13T14:15:02.255261: step 574, loss 0.0242066, acc 0.984375\n",
      "2017-03-13T14:15:02.419377: step 575, loss 0.010621, acc 1\n",
      "2017-03-13T14:15:02.574488: step 576, loss 0.0557256, acc 0.96875\n",
      "2017-03-13T14:15:02.723766: step 577, loss 0.00944258, acc 1\n",
      "2017-03-13T14:15:02.876875: step 578, loss 0.0162244, acc 1\n",
      "2017-03-13T14:15:03.013760: step 579, loss 0.0101193, acc 1\n",
      "2017-03-13T14:15:03.158863: step 580, loss 0.0665538, acc 0.96875\n",
      "2017-03-13T14:15:03.303035: step 581, loss 0.0366601, acc 0.984375\n",
      "2017-03-13T14:15:03.453143: step 582, loss 0.0364418, acc 0.984375\n",
      "2017-03-13T14:15:03.599598: step 583, loss 0.020051, acc 1\n",
      "2017-03-13T14:15:03.752707: step 584, loss 0.0352555, acc 0.984375\n",
      "2017-03-13T14:15:03.796738: step 585, loss 0.0182482, acc 1\n",
      "2017-03-13T14:15:03.934867: step 586, loss 0.0295612, acc 0.984375\n",
      "2017-03-13T14:15:04.085974: step 587, loss 0.0200798, acc 1\n",
      "2017-03-13T14:15:04.222051: step 588, loss 0.0361778, acc 0.984375\n",
      "2017-03-13T14:15:04.374163: step 589, loss 0.0121861, acc 1\n",
      "2017-03-13T14:15:04.511361: step 590, loss 0.0325468, acc 0.984375\n",
      "2017-03-13T14:15:04.649459: step 591, loss 0.00834969, acc 1\n",
      "2017-03-13T14:15:04.805571: step 592, loss 0.0182791, acc 1\n",
      "2017-03-13T14:15:04.963682: step 593, loss 0.0192037, acc 1\n",
      "2017-03-13T14:15:05.102489: step 594, loss 0.0106658, acc 1\n",
      "2017-03-13T14:15:05.254597: step 595, loss 0.00295923, acc 1\n",
      "2017-03-13T14:15:05.391560: step 596, loss 0.0425693, acc 0.984375\n",
      "2017-03-13T14:15:05.533660: step 597, loss 0.00621351, acc 1\n",
      "2017-03-13T14:15:05.686768: step 598, loss 0.00559648, acc 1\n",
      "2017-03-13T14:15:05.838877: step 599, loss 0.0446565, acc 0.984375\n",
      "2017-03-13T14:15:05.880906: step 600, loss 0.104484, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:15:05.952995: step 600, loss 0.386355, acc 0.84\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-600\n",
      "\n",
      "2017-03-13T14:15:08.167908: step 601, loss 0.0274834, acc 0.984375\n",
      "2017-03-13T14:15:08.289561: step 602, loss 0.0108734, acc 1\n",
      "2017-03-13T14:15:08.433664: step 603, loss 0.0171294, acc 0.984375\n",
      "2017-03-13T14:15:08.594777: step 604, loss 0.0620238, acc 0.953125\n",
      "2017-03-13T14:15:08.750890: step 605, loss 0.00827403, acc 1\n",
      "2017-03-13T14:15:08.886532: step 606, loss 0.0103533, acc 1\n",
      "2017-03-13T14:15:09.024630: step 607, loss 0.0214792, acc 1\n",
      "2017-03-13T14:15:09.181741: step 608, loss 0.0417404, acc 0.984375\n",
      "2017-03-13T14:15:09.337852: step 609, loss 0.00717191, acc 1\n",
      "2017-03-13T14:15:09.476712: step 610, loss 0.0136234, acc 1\n",
      "2017-03-13T14:15:09.623817: step 611, loss 0.0295958, acc 1\n",
      "2017-03-13T14:15:09.775924: step 612, loss 0.0764452, acc 0.96875\n",
      "2017-03-13T14:15:09.931034: step 613, loss 0.0439799, acc 0.96875\n",
      "2017-03-13T14:15:10.083142: step 614, loss 0.0221046, acc 0.984375\n",
      "2017-03-13T14:15:10.119167: step 615, loss 0.0298062, acc 1\n",
      "2017-03-13T14:15:10.275199: step 616, loss 0.0131681, acc 1\n",
      "2017-03-13T14:15:10.423144: step 617, loss 0.0296373, acc 0.984375\n",
      "2017-03-13T14:15:10.565246: step 618, loss 0.00840936, acc 1\n",
      "2017-03-13T14:15:10.701373: step 619, loss 0.0106148, acc 1\n",
      "2017-03-13T14:15:10.832466: step 620, loss 0.00871321, acc 1\n",
      "2017-03-13T14:15:10.989578: step 621, loss 0.0485155, acc 0.984375\n",
      "2017-03-13T14:15:11.132680: step 622, loss 0.0218092, acc 1\n",
      "2017-03-13T14:15:11.298797: step 623, loss 0.00669, acc 1\n",
      "2017-03-13T14:15:11.448906: step 624, loss 0.0217673, acc 1\n",
      "2017-03-13T14:15:11.589824: step 625, loss 0.0594097, acc 0.984375\n",
      "2017-03-13T14:15:11.746935: step 626, loss 0.0159376, acc 1\n",
      "2017-03-13T14:15:11.886105: step 627, loss 0.0740747, acc 0.984375\n",
      "2017-03-13T14:15:12.040215: step 628, loss 0.0211014, acc 0.984375\n",
      "2017-03-13T14:15:12.182509: step 629, loss 0.018016, acc 1\n",
      "2017-03-13T14:15:12.213531: step 630, loss 0.478549, acc 0.75\n",
      "2017-03-13T14:15:12.374646: step 631, loss 0.00872839, acc 1\n",
      "2017-03-13T14:15:12.512638: step 632, loss 0.0237767, acc 1\n",
      "2017-03-13T14:15:12.659743: step 633, loss 0.0283927, acc 1\n",
      "2017-03-13T14:15:12.794589: step 634, loss 0.010057, acc 1\n",
      "2017-03-13T14:15:12.942694: step 635, loss 0.0333516, acc 0.984375\n",
      "2017-03-13T14:15:13.096803: step 636, loss 0.0134211, acc 1\n",
      "2017-03-13T14:15:13.224897: step 637, loss 0.0226484, acc 1\n",
      "2017-03-13T14:15:13.377002: step 638, loss 0.0211184, acc 1\n",
      "2017-03-13T14:15:13.517101: step 639, loss 0.0406825, acc 0.984375\n",
      "2017-03-13T14:15:13.666208: step 640, loss 0.0255185, acc 1\n",
      "2017-03-13T14:15:13.806306: step 641, loss 0.0216321, acc 1\n",
      "2017-03-13T14:15:13.961418: step 642, loss 0.0893455, acc 0.96875\n",
      "2017-03-13T14:15:14.096512: step 643, loss 0.0201292, acc 1\n",
      "2017-03-13T14:15:14.249621: step 644, loss 0.0243756, acc 1\n",
      "2017-03-13T14:15:14.282644: step 645, loss 0.00105842, acc 1\n",
      "2017-03-13T14:15:14.411736: step 646, loss 0.0424001, acc 0.96875\n",
      "2017-03-13T14:15:14.575852: step 647, loss 0.041031, acc 0.984375\n",
      "2017-03-13T14:15:14.725959: step 648, loss 0.00861367, acc 1\n",
      "2017-03-13T14:15:14.863336: step 649, loss 0.0264229, acc 0.984375\n",
      "2017-03-13T14:15:14.992429: step 650, loss 0.072671, acc 0.984375\n",
      "2017-03-13T14:15:15.150541: step 651, loss 0.0646202, acc 0.984375\n",
      "2017-03-13T14:15:15.295643: step 652, loss 0.0196966, acc 1\n",
      "2017-03-13T14:15:15.447754: step 653, loss 0.0448591, acc 0.984375\n",
      "2017-03-13T14:15:15.584848: step 654, loss 0.00858354, acc 1\n",
      "2017-03-13T14:15:15.743964: step 655, loss 0.0148515, acc 1\n",
      "2017-03-13T14:15:15.903074: step 656, loss 0.0485927, acc 0.984375\n",
      "2017-03-13T14:15:16.045175: step 657, loss 0.0246116, acc 0.984375\n",
      "2017-03-13T14:15:16.177269: step 658, loss 0.0328535, acc 0.984375\n",
      "2017-03-13T14:15:16.327375: step 659, loss 0.0659826, acc 0.984375\n",
      "2017-03-13T14:15:16.362400: step 660, loss 0.316576, acc 0.75\n",
      "2017-03-13T14:15:16.488489: step 661, loss 0.0100386, acc 1\n",
      "2017-03-13T14:15:16.653607: step 662, loss 0.00986329, acc 1\n",
      "2017-03-13T14:15:16.809718: step 663, loss 0.00945282, acc 1\n",
      "2017-03-13T14:15:16.956822: step 664, loss 0.0441114, acc 0.96875\n",
      "2017-03-13T14:15:17.090917: step 665, loss 0.0109304, acc 1\n",
      "2017-03-13T14:15:17.237020: step 666, loss 0.0190189, acc 1\n",
      "2017-03-13T14:15:17.383124: step 667, loss 0.0393265, acc 0.984375\n",
      "2017-03-13T14:15:17.534233: step 668, loss 0.0131189, acc 1\n",
      "2017-03-13T14:15:17.662322: step 669, loss 0.0401412, acc 0.984375\n",
      "2017-03-13T14:15:17.819433: step 670, loss 0.0398931, acc 0.984375\n",
      "2017-03-13T14:15:17.963535: step 671, loss 0.0330783, acc 1\n",
      "2017-03-13T14:15:18.118646: step 672, loss 0.0969711, acc 0.96875\n",
      "2017-03-13T14:15:18.259746: step 673, loss 0.0126663, acc 1\n",
      "2017-03-13T14:15:18.411854: step 674, loss 0.0401942, acc 0.984375\n",
      "2017-03-13T14:15:18.449881: step 675, loss 0.00153723, acc 1\n",
      "2017-03-13T14:15:18.603992: step 676, loss 0.10048, acc 0.953125\n",
      "2017-03-13T14:15:18.744089: step 677, loss 0.0938202, acc 0.984375\n",
      "2017-03-13T14:15:18.896198: step 678, loss 0.0144038, acc 1\n",
      "2017-03-13T14:15:19.034241: step 679, loss 0.01251, acc 1\n",
      "2017-03-13T14:15:19.157328: step 680, loss 0.0155238, acc 1\n",
      "2017-03-13T14:15:19.307434: step 681, loss 0.0560191, acc 0.96875\n",
      "2017-03-13T14:15:19.453538: step 682, loss 0.0134809, acc 1\n",
      "2017-03-13T14:15:19.608648: step 683, loss 0.0475798, acc 0.984375\n",
      "2017-03-13T14:15:19.750749: step 684, loss 0.0523291, acc 0.96875\n",
      "2017-03-13T14:15:19.909863: step 685, loss 0.0692878, acc 0.984375\n",
      "2017-03-13T14:15:20.047961: step 686, loss 0.012818, acc 1\n",
      "2017-03-13T14:15:20.203071: step 687, loss 0.0127494, acc 1\n",
      "2017-03-13T14:15:20.339167: step 688, loss 0.123505, acc 0.953125\n",
      "2017-03-13T14:15:20.495279: step 689, loss 0.0307312, acc 1\n",
      "2017-03-13T14:15:20.532303: step 690, loss 0.00743439, acc 1\n",
      "2017-03-13T14:15:20.657392: step 691, loss 0.0128134, acc 1\n",
      "2017-03-13T14:15:20.825512: step 692, loss 0.053549, acc 0.96875\n",
      "2017-03-13T14:15:20.967612: step 693, loss 0.01707, acc 1\n",
      "2017-03-13T14:15:21.117720: step 694, loss 0.0288593, acc 0.984375\n",
      "2017-03-13T14:15:21.257818: step 695, loss 0.00943812, acc 1\n",
      "2017-03-13T14:15:21.416931: step 696, loss 0.00664308, acc 1\n",
      "2017-03-13T14:15:21.547024: step 697, loss 0.029309, acc 0.984375\n",
      "2017-03-13T14:15:21.711140: step 698, loss 0.00678718, acc 1\n",
      "2017-03-13T14:15:21.867251: step 699, loss 0.0222919, acc 1\n",
      "2017-03-13T14:15:22.012148: step 700, loss 0.0441649, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:15:22.080196: step 700, loss 0.454336, acc 0.8\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-700\n",
      "\n",
      "2017-03-13T14:15:22.736815: step 701, loss 0.0154445, acc 1\n",
      "2017-03-13T14:15:22.861373: step 702, loss 0.0798566, acc 0.984375\n",
      "2017-03-13T14:15:23.004475: step 703, loss 0.00843817, acc 1\n",
      "2017-03-13T14:15:23.178598: step 704, loss 0.0088603, acc 1\n",
      "2017-03-13T14:15:23.224631: step 705, loss 0.0061044, acc 1\n",
      "2017-03-13T14:15:23.367733: step 706, loss 0.00716969, acc 1\n",
      "2017-03-13T14:15:23.620912: step 707, loss 0.0090144, acc 1\n",
      "2017-03-13T14:15:23.772019: step 708, loss 0.024202, acc 0.984375\n",
      "2017-03-13T14:15:23.916121: step 709, loss 0.0105506, acc 1\n",
      "2017-03-13T14:15:24.073233: step 710, loss 0.0740756, acc 0.96875\n",
      "2017-03-13T14:15:24.242354: step 711, loss 0.00516475, acc 1\n",
      "2017-03-13T14:15:24.386455: step 712, loss 0.0136579, acc 1\n",
      "2017-03-13T14:15:24.548571: step 713, loss 0.00741181, acc 1\n",
      "2017-03-13T14:15:24.680664: step 714, loss 0.0237836, acc 1\n",
      "2017-03-13T14:15:24.833773: step 715, loss 0.0217916, acc 0.984375\n",
      "2017-03-13T14:15:24.975873: step 716, loss 0.047846, acc 0.96875\n",
      "2017-03-13T14:15:25.135988: step 717, loss 0.00363518, acc 1\n",
      "2017-03-13T14:15:25.277086: step 718, loss 0.0344565, acc 0.984375\n",
      "2017-03-13T14:15:25.432197: step 719, loss 0.0146374, acc 1\n",
      "2017-03-13T14:15:25.468222: step 720, loss 0.0877674, acc 1\n",
      "2017-03-13T14:15:25.601317: step 721, loss 0.00860079, acc 1\n",
      "2017-03-13T14:15:25.778445: step 722, loss 0.0817986, acc 0.984375\n",
      "2017-03-13T14:15:25.960573: step 723, loss 0.0148943, acc 1\n",
      "2017-03-13T14:15:26.133694: step 724, loss 0.0065209, acc 1\n",
      "2017-03-13T14:15:26.284801: step 725, loss 0.0191067, acc 0.984375\n",
      "2017-03-13T14:15:26.441913: step 726, loss 0.0574037, acc 0.96875\n",
      "2017-03-13T14:15:26.586015: step 727, loss 0.0693739, acc 0.953125\n",
      "2017-03-13T14:15:26.747130: step 728, loss 0.00638551, acc 1\n",
      "2017-03-13T14:15:26.886228: step 729, loss 0.0117972, acc 1\n",
      "2017-03-13T14:15:27.061352: step 730, loss 0.0182963, acc 1\n",
      "2017-03-13T14:15:27.209458: step 731, loss 0.049516, acc 0.96875\n",
      "2017-03-13T14:15:27.359564: step 732, loss 0.0675068, acc 0.984375\n",
      "2017-03-13T14:15:27.531686: step 733, loss 0.00459418, acc 1\n",
      "2017-03-13T14:15:27.665781: step 734, loss 0.0210953, acc 0.984375\n",
      "2017-03-13T14:15:27.714816: step 735, loss 0.00106149, acc 1\n",
      "2017-03-13T14:15:27.904951: step 736, loss 0.00531563, acc 1\n",
      "2017-03-13T14:15:28.082078: step 737, loss 0.0100965, acc 1\n",
      "2017-03-13T14:15:28.224177: step 738, loss 0.0164095, acc 1\n",
      "2017-03-13T14:15:28.359274: step 739, loss 0.0436245, acc 0.984375\n",
      "2017-03-13T14:15:28.523389: step 740, loss 0.0162887, acc 1\n",
      "2017-03-13T14:15:28.670494: step 741, loss 0.00629998, acc 1\n",
      "2017-03-13T14:15:28.840615: step 742, loss 0.0046121, acc 1\n",
      "2017-03-13T14:15:29.019746: step 743, loss 0.0189189, acc 1\n",
      "2017-03-13T14:15:29.187861: step 744, loss 0.0299842, acc 0.984375\n",
      "2017-03-13T14:15:29.334965: step 745, loss 0.0182807, acc 1\n",
      "2017-03-13T14:15:29.494078: step 746, loss 0.0320033, acc 0.984375\n",
      "2017-03-13T14:15:29.633177: step 747, loss 0.0109614, acc 1\n",
      "2017-03-13T14:15:29.806300: step 748, loss 0.0106286, acc 1\n",
      "2017-03-13T14:15:29.947400: step 749, loss 0.019239, acc 0.984375\n",
      "2017-03-13T14:15:30.002441: step 750, loss 0.00545053, acc 1\n",
      "2017-03-13T14:15:30.194575: step 751, loss 0.017512, acc 1\n",
      "2017-03-13T14:15:30.349685: step 752, loss 0.0246817, acc 0.984375\n",
      "2017-03-13T14:15:30.527812: step 753, loss 0.00451841, acc 1\n",
      "2017-03-13T14:15:30.685923: step 754, loss 0.0241609, acc 1\n",
      "2017-03-13T14:15:30.850040: step 755, loss 0.0226653, acc 1\n",
      "2017-03-13T14:15:31.002155: step 756, loss 0.0329704, acc 0.984375\n",
      "2017-03-13T14:15:31.178273: step 757, loss 0.0186375, acc 1\n",
      "2017-03-13T14:15:31.327379: step 758, loss 0.013433, acc 1\n",
      "2017-03-13T14:15:31.486491: step 759, loss 0.0112991, acc 1\n",
      "2017-03-13T14:15:31.649608: step 760, loss 0.00828402, acc 1\n",
      "2017-03-13T14:15:31.818727: step 761, loss 0.0348527, acc 0.984375\n",
      "2017-03-13T14:15:31.969835: step 762, loss 0.0566821, acc 0.984375\n",
      "2017-03-13T14:15:32.143960: step 763, loss 0.0119911, acc 1\n",
      "2017-03-13T14:15:32.282056: step 764, loss 0.00628134, acc 1\n",
      "2017-03-13T14:15:32.312078: step 765, loss 0.000618519, acc 1\n",
      "2017-03-13T14:15:32.496208: step 766, loss 0.0157399, acc 0.984375\n",
      "2017-03-13T14:15:32.644313: step 767, loss 0.0301422, acc 0.984375\n",
      "2017-03-13T14:15:32.816435: step 768, loss 0.0567496, acc 0.96875\n",
      "2017-03-13T14:15:32.968543: step 769, loss 0.0138997, acc 1\n",
      "2017-03-13T14:15:33.143667: step 770, loss 0.0208352, acc 0.984375\n",
      "2017-03-13T14:15:33.291772: step 771, loss 0.0178401, acc 0.984375\n",
      "2017-03-13T14:15:33.446882: step 772, loss 0.00842032, acc 1\n",
      "2017-03-13T14:15:33.593987: step 773, loss 0.0762421, acc 0.96875\n",
      "2017-03-13T14:15:33.756102: step 774, loss 0.00383309, acc 1\n",
      "2017-03-13T14:15:33.927224: step 775, loss 0.0105702, acc 1\n",
      "2017-03-13T14:15:34.104349: step 776, loss 0.0344057, acc 0.984375\n",
      "2017-03-13T14:15:34.237443: step 777, loss 0.0184294, acc 0.984375\n",
      "2017-03-13T14:15:34.409565: step 778, loss 0.0144431, acc 1\n",
      "2017-03-13T14:15:34.566677: step 779, loss 0.00998054, acc 1\n",
      "2017-03-13T14:15:34.617713: step 780, loss 0.000224978, acc 1\n",
      "2017-03-13T14:15:34.763816: step 781, loss 0.0147319, acc 1\n",
      "2017-03-13T14:15:34.900913: step 782, loss 0.033832, acc 0.984375\n",
      "2017-03-13T14:15:35.067032: step 783, loss 0.0512138, acc 0.984375\n",
      "2017-03-13T14:15:35.204129: step 784, loss 0.00872764, acc 1\n",
      "2017-03-13T14:15:35.374250: step 785, loss 0.00415693, acc 1\n",
      "2017-03-13T14:15:35.505343: step 786, loss 0.0311884, acc 0.96875\n",
      "2017-03-13T14:15:35.666457: step 787, loss 0.0427699, acc 0.984375\n",
      "2017-03-13T14:15:35.805555: step 788, loss 0.00463291, acc 1\n",
      "2017-03-13T14:15:35.974676: step 789, loss 0.00905683, acc 1\n",
      "2017-03-13T14:15:36.141794: step 790, loss 0.00735125, acc 1\n",
      "2017-03-13T14:15:36.290900: step 791, loss 0.0849933, acc 0.984375\n",
      "2017-03-13T14:15:36.453015: step 792, loss 0.0342471, acc 0.984375\n",
      "2017-03-13T14:15:36.595115: step 793, loss 0.0268506, acc 1\n",
      "2017-03-13T14:15:36.746223: step 794, loss 0.00846964, acc 1\n",
      "2017-03-13T14:15:36.793256: step 795, loss 0.000667587, acc 1\n",
      "2017-03-13T14:15:36.928352: step 796, loss 0.00656708, acc 1\n",
      "2017-03-13T14:15:37.101477: step 797, loss 0.00635584, acc 1\n",
      "2017-03-13T14:15:37.240574: step 798, loss 0.0170314, acc 1\n",
      "2017-03-13T14:15:37.402689: step 799, loss 0.0422208, acc 0.984375\n",
      "2017-03-13T14:15:37.564803: step 800, loss 0.0517567, acc 0.96875\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:15:37.649864: step 800, loss 0.455758, acc 0.81\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-800\n",
      "\n",
      "2017-03-13T14:15:38.656304: step 801, loss 0.00957831, acc 1\n",
      "2017-03-13T14:15:38.813416: step 802, loss 0.00734762, acc 1\n",
      "2017-03-13T14:15:39.037575: step 803, loss 0.0215747, acc 1\n",
      "2017-03-13T14:15:39.231713: step 804, loss 0.012446, acc 1\n",
      "2017-03-13T14:15:39.389825: step 805, loss 0.14934, acc 0.953125\n",
      "2017-03-13T14:15:39.556944: step 806, loss 0.0391481, acc 0.96875\n",
      "2017-03-13T14:15:39.709051: step 807, loss 0.00217744, acc 1\n",
      "2017-03-13T14:15:39.872168: step 808, loss 0.044895, acc 0.96875\n",
      "2017-03-13T14:15:40.015269: step 809, loss 0.00425162, acc 1\n",
      "2017-03-13T14:15:40.057298: step 810, loss 0.000111207, acc 1\n",
      "2017-03-13T14:15:40.223419: step 811, loss 0.0109735, acc 1\n",
      "2017-03-13T14:15:40.373523: step 812, loss 0.0989077, acc 0.984375\n",
      "2017-03-13T14:15:40.552651: step 813, loss 0.0106963, acc 1\n",
      "2017-03-13T14:15:40.736781: step 814, loss 0.0131958, acc 1\n",
      "2017-03-13T14:15:40.918912: step 815, loss 0.0183943, acc 0.984375\n",
      "2017-03-13T14:15:41.086029: step 816, loss 0.00735672, acc 1\n",
      "2017-03-13T14:15:41.231134: step 817, loss 0.00357897, acc 1\n",
      "2017-03-13T14:15:41.388243: step 818, loss 0.00539009, acc 1\n",
      "2017-03-13T14:15:41.552361: step 819, loss 0.00230435, acc 1\n",
      "2017-03-13T14:15:41.692458: step 820, loss 0.00420987, acc 1\n",
      "2017-03-13T14:15:41.857576: step 821, loss 0.00729859, acc 1\n",
      "2017-03-13T14:15:42.007683: step 822, loss 0.0499996, acc 0.96875\n",
      "2017-03-13T14:15:42.173801: step 823, loss 0.00332431, acc 1\n",
      "2017-03-13T14:15:42.321905: step 824, loss 0.00361817, acc 1\n",
      "2017-03-13T14:15:42.374942: step 825, loss 0.000805046, acc 1\n",
      "2017-03-13T14:15:42.535056: step 826, loss 0.00269851, acc 1\n",
      "2017-03-13T14:15:42.693169: step 827, loss 0.00844501, acc 1\n",
      "2017-03-13T14:15:42.875297: step 828, loss 0.00119324, acc 1\n",
      "2017-03-13T14:15:43.026405: step 829, loss 0.0064031, acc 1\n",
      "2017-03-13T14:15:43.190522: step 830, loss 0.00778477, acc 1\n",
      "2017-03-13T14:15:43.341628: step 831, loss 0.00621726, acc 1\n",
      "2017-03-13T14:15:43.501742: step 832, loss 0.0297853, acc 0.984375\n",
      "2017-03-13T14:15:43.642842: step 833, loss 0.0126477, acc 1\n",
      "2017-03-13T14:15:43.797952: step 834, loss 0.0121859, acc 1\n",
      "2017-03-13T14:15:43.947058: step 835, loss 0.00409547, acc 1\n",
      "2017-03-13T14:15:44.098165: step 836, loss 0.0113327, acc 1\n",
      "2017-03-13T14:15:44.244269: step 837, loss 0.00930934, acc 1\n",
      "2017-03-13T14:15:44.384368: step 838, loss 0.0216596, acc 0.984375\n",
      "2017-03-13T14:15:44.550489: step 839, loss 0.00982468, acc 1\n",
      "2017-03-13T14:15:44.589513: step 840, loss 0.000102114, acc 1\n",
      "2017-03-13T14:15:44.716604: step 841, loss 0.000880067, acc 1\n",
      "2017-03-13T14:15:44.868714: step 842, loss 0.00574342, acc 1\n",
      "2017-03-13T14:15:45.002807: step 843, loss 0.0104255, acc 1\n",
      "2017-03-13T14:15:45.170926: step 844, loss 0.0417173, acc 0.984375\n",
      "2017-03-13T14:15:45.328037: step 845, loss 0.0124349, acc 1\n",
      "2017-03-13T14:15:45.485149: step 846, loss 0.0374522, acc 0.984375\n",
      "2017-03-13T14:15:45.646264: step 847, loss 0.0172937, acc 0.984375\n",
      "2017-03-13T14:15:45.796370: step 848, loss 0.0746197, acc 0.96875\n",
      "2017-03-13T14:15:45.960486: step 849, loss 0.00320871, acc 1\n",
      "2017-03-13T14:15:46.102587: step 850, loss 0.020005, acc 1\n",
      "2017-03-13T14:15:46.238684: step 851, loss 0.00471658, acc 1\n",
      "2017-03-13T14:15:46.404803: step 852, loss 0.0181948, acc 0.984375\n",
      "2017-03-13T14:15:46.564916: step 853, loss 0.0142522, acc 1\n",
      "2017-03-13T14:15:46.709017: step 854, loss 0.00390055, acc 1\n",
      "2017-03-13T14:15:46.741041: step 855, loss 0.00612903, acc 1\n",
      "2017-03-13T14:15:46.875136: step 856, loss 0.0220605, acc 0.984375\n",
      "2017-03-13T14:15:47.037250: step 857, loss 0.00973146, acc 1\n",
      "2017-03-13T14:15:47.202367: step 858, loss 0.0091448, acc 1\n",
      "2017-03-13T14:15:47.348471: step 859, loss 0.00260716, acc 1\n",
      "2017-03-13T14:15:47.479564: step 860, loss 0.00884591, acc 1\n",
      "2017-03-13T14:15:47.658691: step 861, loss 0.0223033, acc 1\n",
      "2017-03-13T14:15:47.813801: step 862, loss 0.0273264, acc 0.984375\n",
      "2017-03-13T14:15:47.970912: step 863, loss 0.00232223, acc 1\n",
      "2017-03-13T14:15:48.107009: step 864, loss 0.0154136, acc 1\n",
      "2017-03-13T14:15:48.265121: step 865, loss 0.00388792, acc 1\n",
      "2017-03-13T14:15:48.427236: step 866, loss 0.0105291, acc 1\n",
      "2017-03-13T14:15:48.582346: step 867, loss 0.0337768, acc 0.984375\n",
      "2017-03-13T14:15:48.716441: step 868, loss 0.0677156, acc 0.984375\n",
      "2017-03-13T14:15:48.885567: step 869, loss 0.0358684, acc 0.984375\n",
      "2017-03-13T14:15:48.925590: step 870, loss 1.18314e-05, acc 1\n",
      "2017-03-13T14:15:49.050678: step 871, loss 0.0304187, acc 0.984375\n",
      "2017-03-13T14:15:49.217797: step 872, loss 0.0028237, acc 1\n",
      "2017-03-13T14:15:49.367904: step 873, loss 0.00795585, acc 1\n",
      "2017-03-13T14:15:49.535022: step 874, loss 0.00515782, acc 1\n",
      "2017-03-13T14:15:49.715151: step 875, loss 0.0112022, acc 1\n",
      "2017-03-13T14:15:49.896279: step 876, loss 0.0177824, acc 0.984375\n",
      "2017-03-13T14:15:50.057393: step 877, loss 0.00977269, acc 1\n",
      "2017-03-13T14:15:50.229515: step 878, loss 0.00837924, acc 1\n",
      "2017-03-13T14:15:50.381625: step 879, loss 0.00533219, acc 1\n",
      "2017-03-13T14:15:50.547741: step 880, loss 0.0106618, acc 1\n",
      "2017-03-13T14:15:50.729870: step 881, loss 0.01269, acc 1\n",
      "2017-03-13T14:15:50.894987: step 882, loss 0.00878537, acc 1\n",
      "2017-03-13T14:15:51.075117: step 883, loss 0.06006, acc 0.984375\n",
      "2017-03-13T14:15:51.237231: step 884, loss 0.00232932, acc 1\n",
      "2017-03-13T14:15:51.288266: step 885, loss 0.00226633, acc 1\n",
      "2017-03-13T14:15:51.464392: step 886, loss 0.0191361, acc 0.984375\n",
      "2017-03-13T14:15:51.642517: step 887, loss 0.010474, acc 1\n",
      "2017-03-13T14:15:51.802631: step 888, loss 0.0288437, acc 0.984375\n",
      "2017-03-13T14:15:51.948735: step 889, loss 0.00923619, acc 1\n",
      "2017-03-13T14:15:52.123859: step 890, loss 0.0141675, acc 1\n",
      "2017-03-13T14:15:52.275968: step 891, loss 0.022434, acc 1\n",
      "2017-03-13T14:15:52.458099: step 892, loss 0.003621, acc 1\n",
      "2017-03-13T14:15:52.650233: step 893, loss 0.00195681, acc 1\n",
      "2017-03-13T14:15:52.838368: step 894, loss 0.00834929, acc 1\n",
      "2017-03-13T14:15:53.015491: step 895, loss 0.00848044, acc 1\n",
      "2017-03-13T14:15:53.201625: step 896, loss 0.00322581, acc 1\n",
      "2017-03-13T14:15:53.418777: step 897, loss 0.0292817, acc 1\n",
      "2017-03-13T14:15:53.595903: step 898, loss 0.037372, acc 0.96875\n",
      "2017-03-13T14:15:53.774030: step 899, loss 0.00521522, acc 1\n",
      "2017-03-13T14:15:53.824065: step 900, loss 5.51337e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:15:53.911127: step 900, loss 0.452455, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-900\n",
      "\n",
      "2017-03-13T14:15:56.259570: step 901, loss 0.00435823, acc 1\n",
      "2017-03-13T14:15:56.395401: step 902, loss 0.016497, acc 1\n",
      "2017-03-13T14:15:56.526494: step 903, loss 0.00747939, acc 1\n",
      "2017-03-13T14:15:56.674599: step 904, loss 0.0796034, acc 0.984375\n",
      "2017-03-13T14:15:56.831710: step 905, loss 0.0169812, acc 0.984375\n",
      "2017-03-13T14:15:56.971810: step 906, loss 0.00276922, acc 1\n",
      "2017-03-13T14:15:57.149936: step 907, loss 0.0122874, acc 1\n",
      "2017-03-13T14:15:57.283030: step 908, loss 0.0151551, acc 1\n",
      "2017-03-13T14:15:57.456153: step 909, loss 0.00295182, acc 1\n",
      "2017-03-13T14:15:57.603257: step 910, loss 0.0102843, acc 1\n",
      "2017-03-13T14:15:57.763371: step 911, loss 0.00276399, acc 1\n",
      "2017-03-13T14:15:57.905472: step 912, loss 0.00419277, acc 1\n",
      "2017-03-13T14:15:58.070589: step 913, loss 0.00366129, acc 1\n",
      "2017-03-13T14:15:58.212690: step 914, loss 0.00489239, acc 1\n",
      "2017-03-13T14:15:58.248715: step 915, loss 0.00101536, acc 1\n",
      "2017-03-13T14:15:58.418837: step 916, loss 0.00555717, acc 1\n",
      "2017-03-13T14:15:58.554933: step 917, loss 0.00155678, acc 1\n",
      "2017-03-13T14:15:58.722052: step 918, loss 0.00301739, acc 1\n",
      "2017-03-13T14:15:58.862150: step 919, loss 0.023644, acc 0.984375\n",
      "2017-03-13T14:15:59.023265: step 920, loss 0.0129214, acc 1\n",
      "2017-03-13T14:15:59.159361: step 921, loss 0.00692096, acc 1\n",
      "2017-03-13T14:15:59.319475: step 922, loss 0.0137783, acc 1\n",
      "2017-03-13T14:15:59.471586: step 923, loss 0.089085, acc 0.96875\n",
      "2017-03-13T14:15:59.631697: step 924, loss 0.0169771, acc 1\n",
      "2017-03-13T14:15:59.774798: step 925, loss 0.0308029, acc 1\n",
      "2017-03-13T14:15:59.913897: step 926, loss 0.00646539, acc 1\n",
      "2017-03-13T14:16:00.073010: step 927, loss 0.00421867, acc 1\n",
      "2017-03-13T14:16:00.218112: step 928, loss 0.0076691, acc 1\n",
      "2017-03-13T14:16:00.377225: step 929, loss 0.00697089, acc 1\n",
      "2017-03-13T14:16:00.415252: step 930, loss 0.000251943, acc 1\n",
      "2017-03-13T14:16:00.539340: step 931, loss 0.00847375, acc 1\n",
      "2017-03-13T14:16:00.698453: step 932, loss 0.00504824, acc 1\n",
      "2017-03-13T14:16:00.838553: step 933, loss 0.00365428, acc 1\n",
      "2017-03-13T14:16:00.973649: step 934, loss 0.083668, acc 0.953125\n",
      "2017-03-13T14:16:01.141768: step 935, loss 0.00370402, acc 1\n",
      "2017-03-13T14:16:01.276864: step 936, loss 0.00224201, acc 1\n",
      "2017-03-13T14:16:01.427971: step 937, loss 0.00226036, acc 1\n",
      "2017-03-13T14:16:01.583081: step 938, loss 0.0156472, acc 1\n",
      "2017-03-13T14:16:01.724182: step 939, loss 0.0440938, acc 0.984375\n",
      "2017-03-13T14:16:01.875289: step 940, loss 0.0141827, acc 1\n",
      "2017-03-13T14:16:02.025395: step 941, loss 0.00310889, acc 1\n",
      "2017-03-13T14:16:02.195516: step 942, loss 0.0112253, acc 1\n",
      "2017-03-13T14:16:02.358631: step 943, loss 0.0347959, acc 0.984375\n",
      "2017-03-13T14:16:02.530753: step 944, loss 0.00797612, acc 1\n",
      "2017-03-13T14:16:02.579788: step 945, loss 0.0395316, acc 1\n",
      "2017-03-13T14:16:02.717886: step 946, loss 0.00536285, acc 1\n",
      "2017-03-13T14:16:02.860988: step 947, loss 0.00934324, acc 1\n",
      "2017-03-13T14:16:03.009093: step 948, loss 0.0060837, acc 1\n",
      "2017-03-13T14:16:03.139185: step 949, loss 0.00269056, acc 1\n",
      "2017-03-13T14:16:03.289292: step 950, loss 0.0110203, acc 1\n",
      "2017-03-13T14:16:03.447403: step 951, loss 0.0205317, acc 0.984375\n",
      "2017-03-13T14:16:03.607517: step 952, loss 0.0115654, acc 1\n",
      "2017-03-13T14:16:03.750619: step 953, loss 0.0269276, acc 0.984375\n",
      "2017-03-13T14:16:03.880711: step 954, loss 0.0179174, acc 0.984375\n",
      "2017-03-13T14:16:04.044834: step 955, loss 0.00447367, acc 1\n",
      "2017-03-13T14:16:04.188929: step 956, loss 0.00657288, acc 1\n",
      "2017-03-13T14:16:04.356050: step 957, loss 0.0189477, acc 0.984375\n",
      "2017-03-13T14:16:04.497148: step 958, loss 0.00990519, acc 1\n",
      "2017-03-13T14:16:04.648255: step 959, loss 0.0066779, acc 1\n",
      "2017-03-13T14:16:04.695291: step 960, loss 0.000134782, acc 1\n",
      "2017-03-13T14:16:04.823379: step 961, loss 0.00296542, acc 1\n",
      "2017-03-13T14:16:04.960477: step 962, loss 0.00344065, acc 1\n",
      "2017-03-13T14:16:05.121593: step 963, loss 0.00493304, acc 1\n",
      "2017-03-13T14:16:05.261690: step 964, loss 0.00968141, acc 1\n",
      "2017-03-13T14:16:05.426808: step 965, loss 0.00320017, acc 1\n",
      "2017-03-13T14:16:05.570910: step 966, loss 0.0469488, acc 0.96875\n",
      "2017-03-13T14:16:05.725019: step 967, loss 0.00850278, acc 1\n",
      "2017-03-13T14:16:05.872123: step 968, loss 0.00178071, acc 1\n",
      "2017-03-13T14:16:06.035240: step 969, loss 0.0490244, acc 0.984375\n",
      "2017-03-13T14:16:06.181343: step 970, loss 0.0185557, acc 1\n",
      "2017-03-13T14:16:06.347463: step 971, loss 0.00154062, acc 1\n",
      "2017-03-13T14:16:06.484558: step 972, loss 0.0140044, acc 1\n",
      "2017-03-13T14:16:06.635665: step 973, loss 0.0042428, acc 1\n",
      "2017-03-13T14:16:06.785772: step 974, loss 0.0222904, acc 0.984375\n",
      "2017-03-13T14:16:06.829803: step 975, loss 2.59279e-06, acc 1\n",
      "2017-03-13T14:16:06.978909: step 976, loss 0.00451993, acc 1\n",
      "2017-03-13T14:16:07.124011: step 977, loss 0.00542402, acc 1\n",
      "2017-03-13T14:16:07.256105: step 978, loss 0.00943129, acc 1\n",
      "2017-03-13T14:16:07.428227: step 979, loss 0.00401186, acc 1\n",
      "2017-03-13T14:16:07.584338: step 980, loss 0.0114164, acc 1\n",
      "2017-03-13T14:16:07.736446: step 981, loss 0.0145674, acc 1\n",
      "2017-03-13T14:16:07.865538: step 982, loss 0.0105554, acc 1\n",
      "2017-03-13T14:16:08.030656: step 983, loss 0.0058847, acc 1\n",
      "2017-03-13T14:16:08.165750: step 984, loss 0.0224809, acc 0.984375\n",
      "2017-03-13T14:16:08.333870: step 985, loss 0.00960935, acc 1\n",
      "2017-03-13T14:16:08.474970: step 986, loss 0.00479755, acc 1\n",
      "2017-03-13T14:16:08.666106: step 987, loss 0.0100721, acc 1\n",
      "2017-03-13T14:16:08.816212: step 988, loss 0.0204797, acc 0.984375\n",
      "2017-03-13T14:16:08.952310: step 989, loss 0.00562148, acc 1\n",
      "2017-03-13T14:16:09.006347: step 990, loss 0.000518435, acc 1\n",
      "2017-03-13T14:16:09.153451: step 991, loss 0.0120895, acc 1\n",
      "2017-03-13T14:16:09.286546: step 992, loss 0.0123961, acc 1\n",
      "2017-03-13T14:16:09.453664: step 993, loss 0.000942374, acc 1\n",
      "2017-03-13T14:16:09.589761: step 994, loss 0.00276391, acc 1\n",
      "2017-03-13T14:16:09.767887: step 995, loss 0.00112168, acc 1\n",
      "2017-03-13T14:16:09.904985: step 996, loss 0.0388039, acc 0.96875\n",
      "2017-03-13T14:16:10.039081: step 997, loss 0.00497486, acc 1\n",
      "2017-03-13T14:16:10.190187: step 998, loss 0.0402996, acc 0.984375\n",
      "2017-03-13T14:16:10.329286: step 999, loss 0.034735, acc 0.984375\n",
      "2017-03-13T14:16:10.479393: step 1000, loss 0.00668564, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:16:10.566454: step 1000, loss 0.459457, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1000\n",
      "\n",
      "2017-03-13T14:16:11.506966: step 1001, loss 0.010061, acc 1\n",
      "2017-03-13T14:16:11.663794: step 1002, loss 0.0115229, acc 1\n",
      "2017-03-13T14:16:11.849929: step 1003, loss 0.0253857, acc 0.984375\n",
      "2017-03-13T14:16:12.051069: step 1004, loss 0.00108685, acc 1\n",
      "2017-03-13T14:16:12.105107: step 1005, loss 0.000101797, acc 1\n",
      "2017-03-13T14:16:12.257215: step 1006, loss 0.00981637, acc 1\n",
      "2017-03-13T14:16:12.396314: step 1007, loss 0.00218285, acc 1\n",
      "2017-03-13T14:16:12.562432: step 1008, loss 0.00270776, acc 1\n",
      "2017-03-13T14:16:12.701530: step 1009, loss 0.0042786, acc 1\n",
      "2017-03-13T14:16:12.836626: step 1010, loss 0.00591093, acc 1\n",
      "2017-03-13T14:16:12.988735: step 1011, loss 0.00666441, acc 1\n",
      "2017-03-13T14:16:13.132836: step 1012, loss 0.0200089, acc 0.984375\n",
      "2017-03-13T14:16:13.295956: step 1013, loss 0.0573494, acc 0.984375\n",
      "2017-03-13T14:16:13.431048: step 1014, loss 0.0018461, acc 1\n",
      "2017-03-13T14:16:13.602169: step 1015, loss 0.00717327, acc 1\n",
      "2017-03-13T14:16:13.742268: step 1016, loss 0.00240679, acc 1\n",
      "2017-03-13T14:16:13.893376: step 1017, loss 0.00537439, acc 1\n",
      "2017-03-13T14:16:14.042481: step 1018, loss 0.00736467, acc 1\n",
      "2017-03-13T14:16:14.170572: step 1019, loss 0.0190613, acc 1\n",
      "2017-03-13T14:16:14.225611: step 1020, loss 0.000139598, acc 1\n",
      "2017-03-13T14:16:14.378720: step 1021, loss 0.0228426, acc 0.984375\n",
      "2017-03-13T14:16:14.516817: step 1022, loss 0.00225514, acc 1\n",
      "2017-03-13T14:16:14.671928: step 1023, loss 0.00507927, acc 1\n",
      "2017-03-13T14:16:14.815029: step 1024, loss 0.00313515, acc 1\n",
      "2017-03-13T14:16:14.976145: step 1025, loss 0.00216103, acc 1\n",
      "2017-03-13T14:16:15.112240: step 1026, loss 0.0253389, acc 0.984375\n",
      "2017-03-13T14:16:15.261346: step 1027, loss 0.0410797, acc 0.984375\n",
      "2017-03-13T14:16:15.412453: step 1028, loss 0.0049997, acc 1\n",
      "2017-03-13T14:16:15.545548: step 1029, loss 0.00613033, acc 1\n",
      "2017-03-13T14:16:15.714669: step 1030, loss 0.00189785, acc 1\n",
      "2017-03-13T14:16:15.854767: step 1031, loss 0.0469179, acc 0.984375\n",
      "2017-03-13T14:16:16.017882: step 1032, loss 0.00744887, acc 1\n",
      "2017-03-13T14:16:16.155981: step 1033, loss 0.0118933, acc 1\n",
      "2017-03-13T14:16:16.306087: step 1034, loss 0.00765186, acc 1\n",
      "2017-03-13T14:16:16.355122: step 1035, loss 0.000639189, acc 1\n",
      "2017-03-13T14:16:16.478209: step 1036, loss 0.00208945, acc 1\n",
      "2017-03-13T14:16:16.614306: step 1037, loss 0.00134639, acc 1\n",
      "2017-03-13T14:16:16.744398: step 1038, loss 0.0156961, acc 1\n",
      "2017-03-13T14:16:16.878493: step 1039, loss 0.00269121, acc 1\n",
      "2017-03-13T14:16:17.044612: step 1040, loss 0.00618031, acc 1\n",
      "2017-03-13T14:16:17.181708: step 1041, loss 0.00598384, acc 1\n",
      "2017-03-13T14:16:17.314803: step 1042, loss 0.00388061, acc 1\n",
      "2017-03-13T14:16:17.449898: step 1043, loss 0.00341501, acc 1\n",
      "2017-03-13T14:16:17.611013: step 1044, loss 0.00240354, acc 1\n",
      "2017-03-13T14:16:17.750112: step 1045, loss 0.00184655, acc 1\n",
      "2017-03-13T14:16:17.882205: step 1046, loss 0.00277429, acc 1\n",
      "2017-03-13T14:16:18.021304: step 1047, loss 0.00166488, acc 1\n",
      "2017-03-13T14:16:18.159402: step 1048, loss 0.00688815, acc 1\n",
      "2017-03-13T14:16:18.317514: step 1049, loss 0.00310368, acc 1\n",
      "2017-03-13T14:16:18.368550: step 1050, loss 0.00922781, acc 1\n",
      "2017-03-13T14:16:18.502645: step 1051, loss 0.0014486, acc 1\n",
      "2017-03-13T14:16:18.631737: step 1052, loss 0.00319277, acc 1\n",
      "2017-03-13T14:16:18.766833: step 1053, loss 0.0107576, acc 1\n",
      "2017-03-13T14:16:18.902929: step 1054, loss 0.0183403, acc 0.984375\n",
      "2017-03-13T14:16:19.066047: step 1055, loss 0.00399322, acc 1\n",
      "2017-03-13T14:16:19.206144: step 1056, loss 0.0131389, acc 1\n",
      "2017-03-13T14:16:19.356251: step 1057, loss 0.00885333, acc 1\n",
      "2017-03-13T14:16:19.505357: step 1058, loss 0.00100553, acc 1\n",
      "2017-03-13T14:16:19.632447: step 1059, loss 0.00574967, acc 1\n",
      "2017-03-13T14:16:19.791560: step 1060, loss 0.00495008, acc 1\n",
      "2017-03-13T14:16:19.934661: step 1061, loss 0.00275574, acc 1\n",
      "2017-03-13T14:16:20.080766: step 1062, loss 0.0442882, acc 0.984375\n",
      "2017-03-13T14:16:20.228870: step 1063, loss 0.00250556, acc 1\n",
      "2017-03-13T14:16:20.367969: step 1064, loss 0.0243609, acc 0.984375\n",
      "2017-03-13T14:16:20.425009: step 1065, loss 0.00134464, acc 1\n",
      "2017-03-13T14:16:20.568110: step 1066, loss 0.0139377, acc 1\n",
      "2017-03-13T14:16:20.707209: step 1067, loss 0.00811432, acc 1\n",
      "2017-03-13T14:16:20.875329: step 1068, loss 0.0127286, acc 1\n",
      "2017-03-13T14:16:21.015428: step 1069, loss 0.00900125, acc 1\n",
      "2017-03-13T14:16:21.173540: step 1070, loss 0.0018681, acc 1\n",
      "2017-03-13T14:16:21.315641: step 1071, loss 0.00728035, acc 1\n",
      "2017-03-13T14:16:21.469750: step 1072, loss 0.00806075, acc 1\n",
      "2017-03-13T14:16:21.611851: step 1073, loss 0.00265415, acc 1\n",
      "2017-03-13T14:16:21.756954: step 1074, loss 0.00541061, acc 1\n",
      "2017-03-13T14:16:21.903057: step 1075, loss 0.0047002, acc 1\n",
      "2017-03-13T14:16:22.068176: step 1076, loss 0.000654148, acc 1\n",
      "2017-03-13T14:16:22.237296: step 1077, loss 0.0076247, acc 1\n",
      "2017-03-13T14:16:22.401414: step 1078, loss 0.00485005, acc 1\n",
      "2017-03-13T14:16:22.589545: step 1079, loss 0.00365713, acc 1\n",
      "2017-03-13T14:16:22.631577: step 1080, loss 0.00701419, acc 1\n",
      "2017-03-13T14:16:22.806700: step 1081, loss 0.0253212, acc 0.984375\n",
      "2017-03-13T14:16:22.974818: step 1082, loss 0.000586722, acc 1\n",
      "2017-03-13T14:16:23.134935: step 1083, loss 0.00289907, acc 1\n",
      "2017-03-13T14:16:23.316064: step 1084, loss 0.00193028, acc 1\n",
      "2017-03-13T14:16:23.470169: step 1085, loss 0.00160798, acc 1\n",
      "2017-03-13T14:16:23.601575: step 1086, loss 0.00210824, acc 1\n",
      "2017-03-13T14:16:23.724616: step 1087, loss 0.0189572, acc 1\n",
      "2017-03-13T14:16:23.845357: step 1088, loss 0.00224267, acc 1\n",
      "2017-03-13T14:16:23.964462: step 1089, loss 0.00345009, acc 1\n",
      "2017-03-13T14:16:24.096872: step 1090, loss 0.00102504, acc 1\n",
      "2017-03-13T14:16:24.256987: step 1091, loss 0.00184169, acc 1\n",
      "2017-03-13T14:16:24.442121: step 1092, loss 0.00377345, acc 1\n",
      "2017-03-13T14:16:24.621243: step 1093, loss 0.00538584, acc 1\n",
      "2017-03-13T14:16:24.777975: step 1094, loss 0.00155132, acc 1\n",
      "2017-03-13T14:16:24.813000: step 1095, loss 3.23046e-05, acc 1\n",
      "2017-03-13T14:16:24.955101: step 1096, loss 0.00486805, acc 1\n",
      "2017-03-13T14:16:25.107212: step 1097, loss 0.00329503, acc 1\n",
      "2017-03-13T14:16:25.274328: step 1098, loss 0.00292921, acc 1\n",
      "2017-03-13T14:16:25.414882: step 1099, loss 0.00608812, acc 1\n",
      "2017-03-13T14:16:25.562192: step 1100, loss 0.00759152, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:16:25.641250: step 1100, loss 0.476308, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1100\n",
      "\n",
      "2017-03-13T14:16:26.351920: step 1101, loss 0.00147357, acc 1\n",
      "2017-03-13T14:16:26.488012: step 1102, loss 0.00131038, acc 1\n",
      "2017-03-13T14:16:26.628112: step 1103, loss 0.00355628, acc 1\n",
      "2017-03-13T14:16:26.795230: step 1104, loss 0.00111531, acc 1\n",
      "2017-03-13T14:16:27.019393: step 1105, loss 0.0378259, acc 0.96875\n",
      "2017-03-13T14:16:27.270567: step 1106, loss 0.00387053, acc 1\n",
      "2017-03-13T14:16:27.432682: step 1107, loss 0.00365188, acc 1\n",
      "2017-03-13T14:16:27.601804: step 1108, loss 0.00409212, acc 1\n",
      "2017-03-13T14:16:27.761917: step 1109, loss 0.00284131, acc 1\n",
      "2017-03-13T14:16:27.816959: step 1110, loss 0.000935062, acc 1\n",
      "2017-03-13T14:16:27.976068: step 1111, loss 0.00876973, acc 1\n",
      "2017-03-13T14:16:28.126174: step 1112, loss 0.00183371, acc 1\n",
      "2017-03-13T14:16:28.312309: step 1113, loss 0.00335129, acc 1\n",
      "2017-03-13T14:16:28.468416: step 1114, loss 0.00462031, acc 1\n",
      "2017-03-13T14:16:28.645543: step 1115, loss 0.00108094, acc 1\n",
      "2017-03-13T14:16:28.789644: step 1116, loss 0.00255921, acc 1\n",
      "2017-03-13T14:16:28.962767: step 1117, loss 0.00227688, acc 1\n",
      "2017-03-13T14:16:29.114875: step 1118, loss 0.00423867, acc 1\n",
      "2017-03-13T14:16:29.250972: step 1119, loss 0.00532571, acc 1\n",
      "2017-03-13T14:16:29.404080: step 1120, loss 0.0227422, acc 0.984375\n",
      "2017-03-13T14:16:29.558190: step 1121, loss 0.00290444, acc 1\n",
      "2017-03-13T14:16:29.704293: step 1122, loss 0.0135167, acc 1\n",
      "2017-03-13T14:16:29.864407: step 1123, loss 0.00275985, acc 1\n",
      "2017-03-13T14:16:30.011511: step 1124, loss 0.00352888, acc 1\n",
      "2017-03-13T14:16:30.061548: step 1125, loss 0.000506218, acc 1\n",
      "2017-03-13T14:16:30.212654: step 1126, loss 0.00227879, acc 1\n",
      "2017-03-13T14:16:30.350752: step 1127, loss 0.00278998, acc 1\n",
      "2017-03-13T14:16:30.517870: step 1128, loss 0.00441418, acc 1\n",
      "2017-03-13T14:16:30.675983: step 1129, loss 0.00195232, acc 1\n",
      "2017-03-13T14:16:30.826089: step 1130, loss 0.00625915, acc 1\n",
      "2017-03-13T14:16:30.977199: step 1131, loss 0.00878972, acc 1\n",
      "2017-03-13T14:16:31.126302: step 1132, loss 0.00539947, acc 1\n",
      "2017-03-13T14:16:31.252392: step 1133, loss 0.00338145, acc 1\n",
      "2017-03-13T14:16:31.383485: step 1134, loss 0.00904013, acc 1\n",
      "2017-03-13T14:16:31.527587: step 1135, loss 0.00283175, acc 1\n",
      "2017-03-13T14:16:31.662682: step 1136, loss 0.00472768, acc 1\n",
      "2017-03-13T14:16:31.800781: step 1137, loss 0.00250516, acc 1\n",
      "2017-03-13T14:16:31.952888: step 1138, loss 0.00777514, acc 1\n",
      "2017-03-13T14:16:32.082980: step 1139, loss 0.046276, acc 0.984375\n",
      "2017-03-13T14:16:32.115004: step 1140, loss 0.000105161, acc 1\n",
      "2017-03-13T14:16:32.281121: step 1141, loss 0.00282914, acc 1\n",
      "2017-03-13T14:16:32.430227: step 1142, loss 0.00214034, acc 1\n",
      "2017-03-13T14:16:32.590346: step 1143, loss 0.0127353, acc 0.984375\n",
      "2017-03-13T14:16:32.745450: step 1144, loss 0.00112078, acc 1\n",
      "2017-03-13T14:16:32.876543: step 1145, loss 0.0121532, acc 1\n",
      "2017-03-13T14:16:33.018647: step 1146, loss 0.00542226, acc 1\n",
      "2017-03-13T14:16:33.161746: step 1147, loss 0.000898785, acc 1\n",
      "2017-03-13T14:16:33.294840: step 1148, loss 0.00153941, acc 1\n",
      "2017-03-13T14:16:33.440946: step 1149, loss 0.00156041, acc 1\n",
      "2017-03-13T14:16:33.590049: step 1150, loss 0.00550928, acc 1\n",
      "2017-03-13T14:16:33.734153: step 1151, loss 0.0323101, acc 0.984375\n",
      "2017-03-13T14:16:33.873250: step 1152, loss 0.00254951, acc 1\n",
      "2017-03-13T14:16:34.002342: step 1153, loss 0.00504256, acc 1\n",
      "2017-03-13T14:16:34.134436: step 1154, loss 0.00201902, acc 1\n",
      "2017-03-13T14:16:34.180468: step 1155, loss 0.000622345, acc 1\n",
      "2017-03-13T14:16:34.312562: step 1156, loss 0.00463121, acc 1\n",
      "2017-03-13T14:16:34.441653: step 1157, loss 0.00127868, acc 1\n",
      "2017-03-13T14:16:34.592761: step 1158, loss 0.00723553, acc 1\n",
      "2017-03-13T14:16:34.730859: step 1159, loss 0.0110513, acc 1\n",
      "2017-03-13T14:16:34.868957: step 1160, loss 0.00137703, acc 1\n",
      "2017-03-13T14:16:35.006054: step 1161, loss 0.0133452, acc 1\n",
      "2017-03-13T14:16:35.140149: step 1162, loss 0.00335761, acc 1\n",
      "2017-03-13T14:16:35.305267: step 1163, loss 0.0057925, acc 1\n",
      "2017-03-13T14:16:35.462378: step 1164, loss 0.047588, acc 0.984375\n",
      "2017-03-13T14:16:35.592470: step 1165, loss 0.0151582, acc 0.984375\n",
      "2017-03-13T14:16:35.738576: step 1166, loss 0.00284631, acc 1\n",
      "2017-03-13T14:16:35.879674: step 1167, loss 0.00150648, acc 1\n",
      "2017-03-13T14:16:36.010767: step 1168, loss 0.0180036, acc 0.984375\n",
      "2017-03-13T14:16:36.164876: step 1169, loss 0.00129272, acc 1\n",
      "2017-03-13T14:16:36.200901: step 1170, loss 0.000557464, acc 1\n",
      "2017-03-13T14:16:36.335998: step 1171, loss 0.00318349, acc 1\n",
      "2017-03-13T14:16:36.485104: step 1172, loss 0.00141284, acc 1\n",
      "2017-03-13T14:16:36.615195: step 1173, loss 0.0132783, acc 1\n",
      "2017-03-13T14:16:36.776310: step 1174, loss 0.00152777, acc 1\n",
      "2017-03-13T14:16:36.916409: step 1175, loss 0.0447062, acc 0.96875\n",
      "2017-03-13T14:16:37.054507: step 1176, loss 0.00448154, acc 1\n",
      "2017-03-13T14:16:37.210621: step 1177, loss 0.00252149, acc 1\n",
      "2017-03-13T14:16:37.353719: step 1178, loss 0.00208241, acc 1\n",
      "2017-03-13T14:16:37.483812: step 1179, loss 0.00967838, acc 1\n",
      "2017-03-13T14:16:37.634919: step 1180, loss 0.00180247, acc 1\n",
      "2017-03-13T14:16:37.764010: step 1181, loss 0.0354269, acc 0.984375\n",
      "2017-03-13T14:16:37.896104: step 1182, loss 0.00678112, acc 1\n",
      "2017-03-13T14:16:38.039206: step 1183, loss 0.00210228, acc 1\n",
      "2017-03-13T14:16:38.168298: step 1184, loss 0.00171411, acc 1\n",
      "2017-03-13T14:16:38.202323: step 1185, loss 0.005944, acc 1\n",
      "2017-03-13T14:16:38.334415: step 1186, loss 0.00736266, acc 1\n",
      "2017-03-13T14:16:38.474515: step 1187, loss 0.00128191, acc 1\n",
      "2017-03-13T14:16:38.606609: step 1188, loss 0.00614367, acc 1\n",
      "2017-03-13T14:16:38.743706: step 1189, loss 0.00487234, acc 1\n",
      "2017-03-13T14:16:38.875799: step 1190, loss 0.0288238, acc 0.984375\n",
      "2017-03-13T14:16:39.016899: step 1191, loss 0.0015295, acc 1\n",
      "2017-03-13T14:16:39.163003: step 1192, loss 0.00269849, acc 1\n",
      "2017-03-13T14:16:39.305104: step 1193, loss 0.00178075, acc 1\n",
      "2017-03-13T14:16:39.435196: step 1194, loss 0.0169559, acc 0.984375\n",
      "2017-03-13T14:16:39.567290: step 1195, loss 0.0154907, acc 0.984375\n",
      "2017-03-13T14:16:39.706389: step 1196, loss 0.0130273, acc 0.984375\n",
      "2017-03-13T14:16:39.853493: step 1197, loss 0.0016577, acc 1\n",
      "2017-03-13T14:16:39.991591: step 1198, loss 0.0015118, acc 1\n",
      "2017-03-13T14:16:40.138698: step 1199, loss 0.00359019, acc 1\n",
      "2017-03-13T14:16:40.191734: step 1200, loss 0.00260665, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:16:40.264678: step 1200, loss 0.470792, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1200\n",
      "\n",
      "2017-03-13T14:16:42.631449: step 1201, loss 0.00187555, acc 1\n",
      "2017-03-13T14:16:42.763104: step 1202, loss 0.0016562, acc 1\n",
      "2017-03-13T14:16:42.902202: step 1203, loss 0.0146509, acc 1\n",
      "2017-03-13T14:16:43.035297: step 1204, loss 0.00155142, acc 1\n",
      "2017-03-13T14:16:43.186404: step 1205, loss 0.00190134, acc 1\n",
      "2017-03-13T14:16:43.323501: step 1206, loss 0.00825988, acc 1\n",
      "2017-03-13T14:16:43.459597: step 1207, loss 0.0117765, acc 1\n",
      "2017-03-13T14:16:43.611706: step 1208, loss 0.008538, acc 1\n",
      "2017-03-13T14:16:43.742799: step 1209, loss 0.00145154, acc 1\n",
      "2017-03-13T14:16:43.890906: step 1210, loss 0.00580792, acc 1\n",
      "2017-03-13T14:16:44.020997: step 1211, loss 0.0340243, acc 0.984375\n",
      "2017-03-13T14:16:44.161095: step 1212, loss 0.00171237, acc 1\n",
      "2017-03-13T14:16:44.305202: step 1213, loss 0.0008972, acc 1\n",
      "2017-03-13T14:16:44.455304: step 1214, loss 0.0123764, acc 1\n",
      "2017-03-13T14:16:44.489329: step 1215, loss 0.000603467, acc 1\n",
      "2017-03-13T14:16:44.617419: step 1216, loss 0.00142432, acc 1\n",
      "2017-03-13T14:16:44.768527: step 1217, loss 0.0101332, acc 1\n",
      "2017-03-13T14:16:44.926639: step 1218, loss 0.00316486, acc 1\n",
      "2017-03-13T14:16:45.063736: step 1219, loss 0.00725786, acc 1\n",
      "2017-03-13T14:16:45.193828: step 1220, loss 0.00177738, acc 1\n",
      "2017-03-13T14:16:45.320919: step 1221, loss 0.0045766, acc 1\n",
      "2017-03-13T14:16:45.465020: step 1222, loss 0.000774419, acc 1\n",
      "2017-03-13T14:16:45.596113: step 1223, loss 0.00800739, acc 1\n",
      "2017-03-13T14:16:45.776242: step 1224, loss 0.00552266, acc 1\n",
      "2017-03-13T14:16:45.933353: step 1225, loss 0.00190761, acc 1\n",
      "2017-03-13T14:16:46.075453: step 1226, loss 0.00221083, acc 1\n",
      "2017-03-13T14:16:46.204545: step 1227, loss 0.00888713, acc 1\n",
      "2017-03-13T14:16:46.335639: step 1228, loss 0.00294124, acc 1\n",
      "2017-03-13T14:16:46.482742: step 1229, loss 0.00854881, acc 1\n",
      "2017-03-13T14:16:46.514765: step 1230, loss 0.000563774, acc 1\n",
      "2017-03-13T14:16:46.655865: step 1231, loss 0.000643108, acc 1\n",
      "2017-03-13T14:16:46.789960: step 1232, loss 0.00183826, acc 1\n",
      "2017-03-13T14:16:46.927058: step 1233, loss 0.00164876, acc 1\n",
      "2017-03-13T14:16:47.063154: step 1234, loss 0.00127245, acc 1\n",
      "2017-03-13T14:16:47.208257: step 1235, loss 0.00405447, acc 1\n",
      "2017-03-13T14:16:47.365368: step 1236, loss 0.0313907, acc 0.984375\n",
      "2017-03-13T14:16:47.523484: step 1237, loss 0.00453411, acc 1\n",
      "2017-03-13T14:16:47.682594: step 1238, loss 0.00464943, acc 1\n",
      "2017-03-13T14:16:47.820692: step 1239, loss 0.00149274, acc 1\n",
      "2017-03-13T14:16:47.972802: step 1240, loss 0.00721001, acc 1\n",
      "2017-03-13T14:16:48.120904: step 1241, loss 0.00195507, acc 1\n",
      "2017-03-13T14:16:48.258001: step 1242, loss 0.00463377, acc 1\n",
      "2017-03-13T14:16:48.407107: step 1243, loss 0.0143327, acc 0.984375\n",
      "2017-03-13T14:16:48.545206: step 1244, loss 0.00307372, acc 1\n",
      "2017-03-13T14:16:48.578229: step 1245, loss 0.000460267, acc 1\n",
      "2017-03-13T14:16:48.718329: step 1246, loss 0.00428875, acc 1\n",
      "2017-03-13T14:16:48.877043: step 1247, loss 0.00300082, acc 1\n",
      "2017-03-13T14:16:49.005134: step 1248, loss 0.00329375, acc 1\n",
      "2017-03-13T14:16:49.148238: step 1249, loss 0.00407626, acc 1\n",
      "2017-03-13T14:16:49.330364: step 1250, loss 0.00100004, acc 1\n",
      "2017-03-13T14:16:49.479470: step 1251, loss 0.00078588, acc 1\n",
      "2017-03-13T14:16:49.618571: step 1252, loss 0.00367465, acc 1\n",
      "2017-03-13T14:16:49.764672: step 1253, loss 0.00313039, acc 1\n",
      "2017-03-13T14:16:49.895765: step 1254, loss 0.00700736, acc 1\n",
      "2017-03-13T14:16:50.045872: step 1255, loss 0.00461106, acc 1\n",
      "2017-03-13T14:16:50.177966: step 1256, loss 0.00231961, acc 1\n",
      "2017-03-13T14:16:50.311060: step 1257, loss 0.00798231, acc 1\n",
      "2017-03-13T14:16:50.440152: step 1258, loss 0.00394926, acc 1\n",
      "2017-03-13T14:16:50.590258: step 1259, loss 0.00203538, acc 1\n",
      "2017-03-13T14:16:50.633289: step 1260, loss 0.00194136, acc 1\n",
      "2017-03-13T14:16:50.765382: step 1261, loss 0.00473203, acc 1\n",
      "2017-03-13T14:16:50.900478: step 1262, loss 0.00213527, acc 1\n",
      "2017-03-13T14:16:51.052586: step 1263, loss 0.00766854, acc 1\n",
      "2017-03-13T14:16:51.189684: step 1264, loss 0.00225938, acc 1\n",
      "2017-03-13T14:16:51.322778: step 1265, loss 0.00152044, acc 1\n",
      "2017-03-13T14:16:51.469882: step 1266, loss 0.000954421, acc 1\n",
      "2017-03-13T14:16:51.599974: step 1267, loss 0.00432177, acc 1\n",
      "2017-03-13T14:16:51.737072: step 1268, loss 0.00624725, acc 1\n",
      "2017-03-13T14:16:51.877171: step 1269, loss 0.0112965, acc 1\n",
      "2017-03-13T14:16:52.015269: step 1270, loss 0.00142438, acc 1\n",
      "2017-03-13T14:16:52.145361: step 1271, loss 0.00179583, acc 1\n",
      "2017-03-13T14:16:52.292466: step 1272, loss 0.00112867, acc 1\n",
      "2017-03-13T14:16:52.420556: step 1273, loss 0.0309678, acc 0.984375\n",
      "2017-03-13T14:16:52.551649: step 1274, loss 0.00453325, acc 1\n",
      "2017-03-13T14:16:52.594682: step 1275, loss 0.000643673, acc 1\n",
      "2017-03-13T14:16:52.741784: step 1276, loss 0.00235167, acc 1\n",
      "2017-03-13T14:16:52.863871: step 1277, loss 0.000837574, acc 1\n",
      "2017-03-13T14:16:53.008974: step 1278, loss 0.00124161, acc 1\n",
      "2017-03-13T14:16:53.156078: step 1279, loss 0.00197241, acc 1\n",
      "2017-03-13T14:16:53.284169: step 1280, loss 0.00176263, acc 1\n",
      "2017-03-13T14:16:53.432275: step 1281, loss 0.0138359, acc 1\n",
      "2017-03-13T14:16:53.568371: step 1282, loss 0.00189212, acc 1\n",
      "2017-03-13T14:16:53.706469: step 1283, loss 0.0210142, acc 0.984375\n",
      "2017-03-13T14:16:53.845568: step 1284, loss 0.0041878, acc 1\n",
      "2017-03-13T14:16:53.983665: step 1285, loss 0.00392604, acc 1\n",
      "2017-03-13T14:16:54.110756: step 1286, loss 0.00197442, acc 1\n",
      "2017-03-13T14:16:54.247854: step 1287, loss 0.00104334, acc 1\n",
      "2017-03-13T14:16:54.378946: step 1288, loss 0.00286724, acc 1\n",
      "2017-03-13T14:16:54.508037: step 1289, loss 0.000881511, acc 1\n",
      "2017-03-13T14:16:54.539060: step 1290, loss 0.0081476, acc 1\n",
      "2017-03-13T14:16:54.703176: step 1291, loss 0.00329817, acc 1\n",
      "2017-03-13T14:16:54.842274: step 1292, loss 0.0852198, acc 0.984375\n",
      "2017-03-13T14:16:54.973368: step 1293, loss 0.000868127, acc 1\n",
      "2017-03-13T14:16:55.145490: step 1294, loss 0.0026369, acc 1\n",
      "2017-03-13T14:16:55.282587: step 1295, loss 0.00339882, acc 1\n",
      "2017-03-13T14:16:55.426692: step 1296, loss 0.00246225, acc 1\n",
      "2017-03-13T14:16:55.574794: step 1297, loss 0.000586126, acc 1\n",
      "2017-03-13T14:16:55.703886: step 1298, loss 0.00161811, acc 1\n",
      "2017-03-13T14:16:55.835979: step 1299, loss 0.0145391, acc 0.984375\n",
      "2017-03-13T14:16:55.985091: step 1300, loss 0.00266559, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:16:56.082154: step 1300, loss 0.467274, acc 0.85\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1300\n",
      "\n",
      "2017-03-13T14:16:56.964113: step 1301, loss 0.0018701, acc 1\n",
      "2017-03-13T14:16:57.167255: step 1302, loss 0.00073546, acc 1\n",
      "2017-03-13T14:16:57.306354: step 1303, loss 0.00243024, acc 1\n",
      "2017-03-13T14:16:57.438447: step 1304, loss 0.0036433, acc 1\n",
      "2017-03-13T14:16:57.474474: step 1305, loss 0.00070307, acc 1\n",
      "2017-03-13T14:16:57.622578: step 1306, loss 0.0244545, acc 0.984375\n",
      "2017-03-13T14:16:57.764680: step 1307, loss 0.00123884, acc 1\n",
      "2017-03-13T14:16:57.894771: step 1308, loss 0.002981, acc 1\n",
      "2017-03-13T14:16:58.046879: step 1309, loss 0.0224414, acc 0.984375\n",
      "2017-03-13T14:16:58.189981: step 1310, loss 0.00152882, acc 1\n",
      "2017-03-13T14:16:58.322075: step 1311, loss 0.00184029, acc 1\n",
      "2017-03-13T14:16:58.471180: step 1312, loss 0.00873652, acc 1\n",
      "2017-03-13T14:16:58.601272: step 1313, loss 0.00125741, acc 1\n",
      "2017-03-13T14:16:58.740372: step 1314, loss 0.00811898, acc 1\n",
      "2017-03-13T14:16:58.879470: step 1315, loss 0.00390608, acc 1\n",
      "2017-03-13T14:16:59.015566: step 1316, loss 0.00523586, acc 1\n",
      "2017-03-13T14:16:59.157667: step 1317, loss 0.00234475, acc 1\n",
      "2017-03-13T14:16:59.294765: step 1318, loss 0.0236564, acc 0.984375\n",
      "2017-03-13T14:16:59.433864: step 1319, loss 0.00308293, acc 1\n",
      "2017-03-13T14:16:59.474892: step 1320, loss 0.0272068, acc 1\n",
      "2017-03-13T14:16:59.629002: step 1321, loss 0.000665048, acc 1\n",
      "2017-03-13T14:16:59.760095: step 1322, loss 0.000617619, acc 1\n",
      "2017-03-13T14:16:59.914204: step 1323, loss 0.00205509, acc 1\n",
      "2017-03-13T14:17:00.062309: step 1324, loss 0.000839453, acc 1\n",
      "2017-03-13T14:17:00.201408: step 1325, loss 0.00255649, acc 1\n",
      "2017-03-13T14:17:00.350514: step 1326, loss 0.00812461, acc 1\n",
      "2017-03-13T14:17:00.485609: step 1327, loss 0.00319928, acc 1\n",
      "2017-03-13T14:17:00.612700: step 1328, loss 0.00150618, acc 1\n",
      "2017-03-13T14:17:00.758803: step 1329, loss 0.00213403, acc 1\n",
      "2017-03-13T14:17:00.891897: step 1330, loss 0.0014361, acc 1\n",
      "2017-03-13T14:17:01.031997: step 1331, loss 0.00227356, acc 1\n",
      "2017-03-13T14:17:01.202117: step 1332, loss 0.00272362, acc 1\n",
      "2017-03-13T14:17:01.363232: step 1333, loss 0.0013089, acc 1\n",
      "2017-03-13T14:17:01.522345: step 1334, loss 0.0100853, acc 1\n",
      "2017-03-13T14:17:01.569378: step 1335, loss 0.00347745, acc 1\n",
      "2017-03-13T14:17:01.726491: step 1336, loss 0.0110249, acc 1\n",
      "2017-03-13T14:17:01.890606: step 1337, loss 0.00121648, acc 1\n",
      "2017-03-13T14:17:02.044718: step 1338, loss 0.0163483, acc 0.984375\n",
      "2017-03-13T14:17:02.212834: step 1339, loss 0.003402, acc 1\n",
      "2017-03-13T14:17:02.381955: step 1340, loss 0.00194601, acc 1\n",
      "2017-03-13T14:17:02.540067: step 1341, loss 0.00432449, acc 1\n",
      "2017-03-13T14:17:02.705184: step 1342, loss 0.051376, acc 0.96875\n",
      "2017-03-13T14:17:02.863296: step 1343, loss 0.00126327, acc 1\n",
      "2017-03-13T14:17:03.031415: step 1344, loss 0.0026496, acc 1\n",
      "2017-03-13T14:17:03.191529: step 1345, loss 0.00387584, acc 1\n",
      "2017-03-13T14:17:03.339635: step 1346, loss 0.00154728, acc 1\n",
      "2017-03-13T14:17:03.499747: step 1347, loss 0.000564043, acc 1\n",
      "2017-03-13T14:17:03.666866: step 1348, loss 0.00417678, acc 1\n",
      "2017-03-13T14:17:03.814972: step 1349, loss 0.00211785, acc 1\n",
      "2017-03-13T14:17:03.858001: step 1350, loss 0.136757, acc 1\n",
      "2017-03-13T14:17:04.030124: step 1351, loss 0.0114565, acc 1\n",
      "2017-03-13T14:17:04.196243: step 1352, loss 0.000920351, acc 1\n",
      "2017-03-13T14:17:04.342345: step 1353, loss 0.0664788, acc 0.96875\n",
      "2017-03-13T14:17:04.504465: step 1354, loss 0.0892489, acc 0.984375\n",
      "2017-03-13T14:17:04.663573: step 1355, loss 0.00144453, acc 1\n",
      "2017-03-13T14:17:04.821685: step 1356, loss 0.0124476, acc 1\n",
      "2017-03-13T14:17:04.981799: step 1357, loss 0.00483501, acc 1\n",
      "2017-03-13T14:17:05.151920: step 1358, loss 0.00183651, acc 1\n",
      "2017-03-13T14:17:05.307030: step 1359, loss 0.00584712, acc 1\n",
      "2017-03-13T14:17:05.468144: step 1360, loss 0.00497057, acc 1\n",
      "2017-03-13T14:17:05.622253: step 1361, loss 0.00255109, acc 1\n",
      "2017-03-13T14:17:05.785369: step 1362, loss 0.0172757, acc 0.984375\n",
      "2017-03-13T14:17:05.934474: step 1363, loss 0.00555321, acc 1\n",
      "2017-03-13T14:17:06.098592: step 1364, loss 0.00199643, acc 1\n",
      "2017-03-13T14:17:06.144625: step 1365, loss 0.000522742, acc 1\n",
      "2017-03-13T14:17:06.316746: step 1366, loss 0.00126687, acc 1\n",
      "2017-03-13T14:17:06.469854: step 1367, loss 0.0131491, acc 1\n",
      "2017-03-13T14:17:06.626966: step 1368, loss 0.00255417, acc 1\n",
      "2017-03-13T14:17:06.787080: step 1369, loss 0.000874462, acc 1\n",
      "2017-03-13T14:17:06.950195: step 1370, loss 0.00218165, acc 1\n",
      "2017-03-13T14:17:07.123318: step 1371, loss 0.00093731, acc 1\n",
      "2017-03-13T14:17:07.272424: step 1372, loss 0.00162047, acc 1\n",
      "2017-03-13T14:17:07.429535: step 1373, loss 0.0500722, acc 0.96875\n",
      "2017-03-13T14:17:07.595653: step 1374, loss 0.00142101, acc 1\n",
      "2017-03-13T14:17:07.756767: step 1375, loss 0.0039697, acc 1\n",
      "2017-03-13T14:17:07.916881: step 1376, loss 0.00225553, acc 1\n",
      "2017-03-13T14:17:08.068989: step 1377, loss 0.00530822, acc 1\n",
      "2017-03-13T14:17:08.219096: step 1378, loss 0.000570251, acc 1\n",
      "2017-03-13T14:17:08.377208: step 1379, loss 0.00662394, acc 1\n",
      "2017-03-13T14:17:08.428244: step 1380, loss 3.41516e-05, acc 1\n",
      "2017-03-13T14:17:08.591359: step 1381, loss 0.00395222, acc 1\n",
      "2017-03-13T14:17:08.750473: step 1382, loss 0.00225218, acc 1\n",
      "2017-03-13T14:17:08.901583: step 1383, loss 0.000594738, acc 1\n",
      "2017-03-13T14:17:09.051686: step 1384, loss 0.00133461, acc 1\n",
      "2017-03-13T14:17:09.202793: step 1385, loss 0.00575917, acc 1\n",
      "2017-03-13T14:17:09.357904: step 1386, loss 0.00242551, acc 1\n",
      "2017-03-13T14:17:09.508010: step 1387, loss 0.0029042, acc 1\n",
      "2017-03-13T14:17:09.649110: step 1388, loss 0.0041849, acc 1\n",
      "2017-03-13T14:17:09.796215: step 1389, loss 0.00508189, acc 1\n",
      "2017-03-13T14:17:09.959330: step 1390, loss 0.00118453, acc 1\n",
      "2017-03-13T14:17:10.115440: step 1391, loss 0.00295652, acc 1\n",
      "2017-03-13T14:17:10.266548: step 1392, loss 0.00170063, acc 1\n",
      "2017-03-13T14:17:10.421658: step 1393, loss 0.00293292, acc 1\n",
      "2017-03-13T14:17:10.568762: step 1394, loss 0.0064035, acc 1\n",
      "2017-03-13T14:17:10.614803: step 1395, loss 7.55118e-05, acc 1\n",
      "2017-03-13T14:17:10.757897: step 1396, loss 0.00193492, acc 1\n",
      "2017-03-13T14:17:10.902999: step 1397, loss 0.00365738, acc 1\n",
      "2017-03-13T14:17:11.053106: step 1398, loss 0.00047341, acc 1\n",
      "2017-03-13T14:17:11.209218: step 1399, loss 0.021113, acc 0.984375\n",
      "2017-03-13T14:17:11.357321: step 1400, loss 0.0141799, acc 0.984375\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:17:11.438379: step 1400, loss 0.517544, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1400\n",
      "\n",
      "2017-03-13T14:17:15.003775: step 1401, loss 0.00182719, acc 1\n",
      "2017-03-13T14:17:15.177897: step 1402, loss 0.00203046, acc 1\n",
      "2017-03-13T14:17:15.333007: step 1403, loss 0.00453574, acc 1\n",
      "2017-03-13T14:17:15.488117: step 1404, loss 0.00133455, acc 1\n",
      "2017-03-13T14:17:15.652235: step 1405, loss 0.0204638, acc 0.984375\n",
      "2017-03-13T14:17:15.812346: step 1406, loss 0.00575733, acc 1\n",
      "2017-03-13T14:17:15.974461: step 1407, loss 0.00302327, acc 1\n",
      "2017-03-13T14:17:16.140579: step 1408, loss 0.00221667, acc 1\n",
      "2017-03-13T14:17:16.284682: step 1409, loss 0.00206689, acc 1\n",
      "2017-03-13T14:17:16.326713: step 1410, loss 0.00181921, acc 1\n",
      "2017-03-13T14:17:16.507844: step 1411, loss 0.00191016, acc 1\n",
      "2017-03-13T14:17:16.681964: step 1412, loss 0.00420638, acc 1\n",
      "2017-03-13T14:17:16.852084: step 1413, loss 0.00342247, acc 1\n",
      "2017-03-13T14:17:17.012198: step 1414, loss 0.00084806, acc 1\n",
      "2017-03-13T14:17:17.174313: step 1415, loss 0.00148857, acc 1\n",
      "2017-03-13T14:17:17.322418: step 1416, loss 0.00150388, acc 1\n",
      "2017-03-13T14:17:17.489538: step 1417, loss 0.000773661, acc 1\n",
      "2017-03-13T14:17:17.643645: step 1418, loss 0.00263933, acc 1\n",
      "2017-03-13T14:17:17.800757: step 1419, loss 0.00242687, acc 1\n",
      "2017-03-13T14:17:17.950864: step 1420, loss 0.00219759, acc 1\n",
      "2017-03-13T14:17:18.097968: step 1421, loss 0.00825959, acc 1\n",
      "2017-03-13T14:17:18.258082: step 1422, loss 0.00255212, acc 1\n",
      "2017-03-13T14:17:18.413192: step 1423, loss 0.00203752, acc 1\n",
      "2017-03-13T14:17:18.588317: step 1424, loss 0.00319943, acc 1\n",
      "2017-03-13T14:17:18.640353: step 1425, loss 0.0274561, acc 1\n",
      "2017-03-13T14:17:18.805470: step 1426, loss 0.00373032, acc 1\n",
      "2017-03-13T14:17:18.958578: step 1427, loss 0.000880693, acc 1\n",
      "2017-03-13T14:17:19.119693: step 1428, loss 0.00276749, acc 1\n",
      "2017-03-13T14:17:19.281808: step 1429, loss 0.00185375, acc 1\n",
      "2017-03-13T14:17:19.455931: step 1430, loss 0.00152912, acc 1\n",
      "2017-03-13T14:17:19.607039: step 1431, loss 0.00254804, acc 1\n",
      "2017-03-13T14:17:19.760147: step 1432, loss 0.0563196, acc 0.984375\n",
      "2017-03-13T14:17:19.918259: step 1433, loss 0.0153767, acc 1\n",
      "2017-03-13T14:17:20.078373: step 1434, loss 0.0116829, acc 1\n",
      "2017-03-13T14:17:20.234483: step 1435, loss 0.00152136, acc 1\n",
      "2017-03-13T14:17:20.379587: step 1436, loss 0.00206218, acc 1\n",
      "2017-03-13T14:17:20.526691: step 1437, loss 0.0466219, acc 0.984375\n",
      "2017-03-13T14:17:20.677798: step 1438, loss 0.00456398, acc 1\n",
      "2017-03-13T14:17:20.849920: step 1439, loss 0.0042203, acc 1\n",
      "2017-03-13T14:17:20.900956: step 1440, loss 0.000473053, acc 1\n",
      "2017-03-13T14:17:21.033051: step 1441, loss 0.00453708, acc 1\n",
      "2017-03-13T14:17:21.194165: step 1442, loss 0.000901782, acc 1\n",
      "2017-03-13T14:17:21.346272: step 1443, loss 0.00245271, acc 1\n",
      "2017-03-13T14:17:21.503384: step 1444, loss 0.00216899, acc 1\n",
      "2017-03-13T14:17:21.653490: step 1445, loss 0.00912629, acc 1\n",
      "2017-03-13T14:17:21.805598: step 1446, loss 0.00298644, acc 1\n",
      "2017-03-13T14:17:21.955704: step 1447, loss 0.000798425, acc 1\n",
      "2017-03-13T14:17:22.111816: step 1448, loss 0.00162624, acc 1\n",
      "2017-03-13T14:17:22.261922: step 1449, loss 0.00186135, acc 1\n",
      "2017-03-13T14:17:22.426039: step 1450, loss 0.00762082, acc 1\n",
      "2017-03-13T14:17:22.568139: step 1451, loss 0.00198187, acc 1\n",
      "2017-03-13T14:17:22.726252: step 1452, loss 0.000457256, acc 1\n",
      "2017-03-13T14:17:22.874356: step 1453, loss 0.0015561, acc 1\n",
      "2017-03-13T14:17:23.030467: step 1454, loss 0.00157841, acc 1\n",
      "2017-03-13T14:17:23.065492: step 1455, loss 0.0027851, acc 1\n",
      "2017-03-13T14:17:23.197585: step 1456, loss 0.00675712, acc 1\n",
      "2017-03-13T14:17:23.340687: step 1457, loss 0.0134355, acc 0.984375\n",
      "2017-03-13T14:17:23.485790: step 1458, loss 0.0100265, acc 1\n",
      "2017-03-13T14:17:23.652908: step 1459, loss 0.00435485, acc 1\n",
      "2017-03-13T14:17:23.831035: step 1460, loss 0.0042415, acc 1\n",
      "2017-03-13T14:17:23.985144: step 1461, loss 0.00963196, acc 1\n",
      "2017-03-13T14:17:24.151264: step 1462, loss 0.000985864, acc 1\n",
      "2017-03-13T14:17:24.312376: step 1463, loss 0.000808316, acc 1\n",
      "2017-03-13T14:17:24.449474: step 1464, loss 0.00104889, acc 1\n",
      "2017-03-13T14:17:24.601582: step 1465, loss 0.00295849, acc 1\n",
      "2017-03-13T14:17:24.749687: step 1466, loss 0.00106574, acc 1\n",
      "2017-03-13T14:17:24.910801: step 1467, loss 0.00301644, acc 1\n",
      "2017-03-13T14:17:25.080923: step 1468, loss 0.00118739, acc 1\n",
      "2017-03-13T14:17:25.238033: step 1469, loss 0.000845003, acc 1\n",
      "2017-03-13T14:17:25.279063: step 1470, loss 0.0508314, acc 1\n",
      "2017-03-13T14:17:25.441177: step 1471, loss 0.00130782, acc 1\n",
      "2017-03-13T14:17:25.587281: step 1472, loss 0.000719638, acc 1\n",
      "2017-03-13T14:17:25.739389: step 1473, loss 0.00313889, acc 1\n",
      "2017-03-13T14:17:25.905507: step 1474, loss 0.00208054, acc 1\n",
      "2017-03-13T14:17:26.056617: step 1475, loss 0.00207728, acc 1\n",
      "2017-03-13T14:17:26.223733: step 1476, loss 0.00161447, acc 1\n",
      "2017-03-13T14:17:26.380844: step 1477, loss 0.00270392, acc 1\n",
      "2017-03-13T14:17:26.535954: step 1478, loss 0.00169258, acc 1\n",
      "2017-03-13T14:17:26.675052: step 1479, loss 0.0223072, acc 0.984375\n",
      "2017-03-13T14:17:26.822157: step 1480, loss 0.00142967, acc 1\n",
      "2017-03-13T14:17:26.995280: step 1481, loss 0.0029122, acc 1\n",
      "2017-03-13T14:17:27.144385: step 1482, loss 0.00668125, acc 1\n",
      "2017-03-13T14:17:27.302497: step 1483, loss 0.00402946, acc 1\n",
      "2017-03-13T14:17:27.460610: step 1484, loss 0.00608349, acc 1\n",
      "2017-03-13T14:17:27.504641: step 1485, loss 0.00013059, acc 1\n",
      "2017-03-13T14:17:27.644741: step 1486, loss 0.00233831, acc 1\n",
      "2017-03-13T14:17:27.789844: step 1487, loss 0.00128167, acc 1\n",
      "2017-03-13T14:17:27.924939: step 1488, loss 0.000770176, acc 1\n",
      "2017-03-13T14:17:28.069041: step 1489, loss 0.0244039, acc 0.984375\n",
      "2017-03-13T14:17:28.241163: step 1490, loss 0.00127372, acc 1\n",
      "2017-03-13T14:17:28.401278: step 1491, loss 0.00227807, acc 1\n",
      "2017-03-13T14:17:28.546380: step 1492, loss 0.00354749, acc 1\n",
      "2017-03-13T14:17:28.702491: step 1493, loss 0.0069799, acc 1\n",
      "2017-03-13T14:17:28.861604: step 1494, loss 0.00506822, acc 1\n",
      "2017-03-13T14:17:29.027722: step 1495, loss 0.000831757, acc 1\n",
      "2017-03-13T14:17:29.187836: step 1496, loss 0.00132839, acc 1\n",
      "2017-03-13T14:17:29.334940: step 1497, loss 0.00709005, acc 1\n",
      "2017-03-13T14:17:29.468035: step 1498, loss 0.00328245, acc 1\n",
      "2017-03-13T14:17:29.612136: step 1499, loss 0.00334959, acc 1\n",
      "2017-03-13T14:17:29.648162: step 1500, loss 0.00013438, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:17:29.725216: step 1500, loss 0.480698, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1500\n",
      "\n",
      "2017-03-13T14:17:30.521348: step 1501, loss 0.0025615, acc 1\n",
      "2017-03-13T14:17:30.659848: step 1502, loss 0.00248822, acc 1\n",
      "2017-03-13T14:17:30.809954: step 1503, loss 0.002608, acc 1\n",
      "2017-03-13T14:17:31.002094: step 1504, loss 0.000454767, acc 1\n",
      "2017-03-13T14:17:31.205237: step 1505, loss 0.00142481, acc 1\n",
      "2017-03-13T14:17:31.366349: step 1506, loss 0.0136802, acc 1\n",
      "2017-03-13T14:17:31.515455: step 1507, loss 0.0020687, acc 1\n",
      "2017-03-13T14:17:31.662559: step 1508, loss 0.00862644, acc 1\n",
      "2017-03-13T14:17:31.794653: step 1509, loss 0.000455286, acc 1\n",
      "2017-03-13T14:17:31.930750: step 1510, loss 0.0011909, acc 1\n",
      "2017-03-13T14:17:32.063844: step 1511, loss 0.00179329, acc 1\n",
      "2017-03-13T14:17:32.192936: step 1512, loss 0.00930636, acc 1\n",
      "2017-03-13T14:17:32.335036: step 1513, loss 0.0011861, acc 1\n",
      "2017-03-13T14:17:32.491147: step 1514, loss 0.00144007, acc 1\n",
      "2017-03-13T14:17:32.545185: step 1515, loss 0.000299014, acc 1\n",
      "2017-03-13T14:17:32.694291: step 1516, loss 0.00601223, acc 1\n",
      "2017-03-13T14:17:32.827386: step 1517, loss 0.0023928, acc 1\n",
      "2017-03-13T14:17:32.973490: step 1518, loss 0.000952029, acc 1\n",
      "2017-03-13T14:17:33.113588: step 1519, loss 0.0461196, acc 0.984375\n",
      "2017-03-13T14:17:33.251687: step 1520, loss 0.000898522, acc 1\n",
      "2017-03-13T14:17:33.390785: step 1521, loss 0.0105785, acc 1\n",
      "2017-03-13T14:17:33.526881: step 1522, loss 0.000664975, acc 1\n",
      "2017-03-13T14:17:33.662978: step 1523, loss 0.00481631, acc 1\n",
      "2017-03-13T14:17:33.803078: step 1524, loss 0.0022612, acc 1\n",
      "2017-03-13T14:17:33.940175: step 1525, loss 0.0355702, acc 0.984375\n",
      "2017-03-13T14:17:34.079274: step 1526, loss 0.00295045, acc 1\n",
      "2017-03-13T14:17:34.212369: step 1527, loss 0.00168893, acc 1\n",
      "2017-03-13T14:17:34.359473: step 1528, loss 0.0016445, acc 1\n",
      "2017-03-13T14:17:34.493567: step 1529, loss 0.00161738, acc 1\n",
      "2017-03-13T14:17:34.526592: step 1530, loss 0.000341226, acc 1\n",
      "2017-03-13T14:17:34.660686: step 1531, loss 0.0010688, acc 1\n",
      "2017-03-13T14:17:34.791779: step 1532, loss 0.0085394, acc 1\n",
      "2017-03-13T14:17:34.927875: step 1533, loss 0.0196689, acc 0.984375\n",
      "2017-03-13T14:17:35.063972: step 1534, loss 0.00351288, acc 1\n",
      "2017-03-13T14:17:35.212077: step 1535, loss 0.00411878, acc 1\n",
      "2017-03-13T14:17:35.336166: step 1536, loss 0.00835717, acc 1\n",
      "2017-03-13T14:17:35.473263: step 1537, loss 0.0014093, acc 1\n",
      "2017-03-13T14:17:35.609359: step 1538, loss 0.0257085, acc 0.984375\n",
      "2017-03-13T14:17:35.744455: step 1539, loss 0.00186836, acc 1\n",
      "2017-03-13T14:17:35.878550: step 1540, loss 0.0041294, acc 1\n",
      "2017-03-13T14:17:36.011645: step 1541, loss 0.00129852, acc 1\n",
      "2017-03-13T14:17:36.151744: step 1542, loss 0.00271652, acc 1\n",
      "2017-03-13T14:17:36.295846: step 1543, loss 0.00175608, acc 1\n",
      "2017-03-13T14:17:36.435946: step 1544, loss 0.000881538, acc 1\n",
      "2017-03-13T14:17:36.466968: step 1545, loss 0.000308029, acc 1\n",
      "2017-03-13T14:17:36.602063: step 1546, loss 0.00250933, acc 1\n",
      "2017-03-13T14:17:36.735158: step 1547, loss 0.0017053, acc 1\n",
      "2017-03-13T14:17:36.861247: step 1548, loss 0.00111454, acc 1\n",
      "2017-03-13T14:17:37.007351: step 1549, loss 0.00063939, acc 1\n",
      "2017-03-13T14:17:37.142446: step 1550, loss 0.0136953, acc 0.984375\n",
      "2017-03-13T14:17:37.277543: step 1551, loss 0.00111014, acc 1\n",
      "2017-03-13T14:17:37.409636: step 1552, loss 0.00659858, acc 1\n",
      "2017-03-13T14:17:37.543731: step 1553, loss 0.00180242, acc 1\n",
      "2017-03-13T14:17:37.677827: step 1554, loss 0.00168738, acc 1\n",
      "2017-03-13T14:17:37.824930: step 1555, loss 0.000894916, acc 1\n",
      "2017-03-13T14:17:37.973036: step 1556, loss 0.00126569, acc 1\n",
      "2017-03-13T14:17:38.101126: step 1557, loss 0.00460077, acc 1\n",
      "2017-03-13T14:17:38.242227: step 1558, loss 0.00706037, acc 1\n",
      "2017-03-13T14:17:38.380324: step 1559, loss 0.001198, acc 1\n",
      "2017-03-13T14:17:38.411347: step 1560, loss 0.0477647, acc 1\n",
      "2017-03-13T14:17:38.545442: step 1561, loss 0.00209889, acc 1\n",
      "2017-03-13T14:17:38.683540: step 1562, loss 0.000545603, acc 1\n",
      "2017-03-13T14:17:38.823639: step 1563, loss 0.00280118, acc 1\n",
      "2017-03-13T14:17:38.965740: step 1564, loss 0.00102406, acc 1\n",
      "2017-03-13T14:17:39.095833: step 1565, loss 0.00069277, acc 1\n",
      "2017-03-13T14:17:39.231930: step 1566, loss 0.0041112, acc 1\n",
      "2017-03-13T14:17:39.367024: step 1567, loss 0.012172, acc 1\n",
      "2017-03-13T14:17:39.505123: step 1568, loss 0.0104825, acc 1\n",
      "2017-03-13T14:17:39.640219: step 1569, loss 0.00137452, acc 1\n",
      "2017-03-13T14:17:39.777316: step 1570, loss 0.00162645, acc 1\n",
      "2017-03-13T14:17:39.915414: step 1571, loss 0.0157447, acc 0.984375\n",
      "2017-03-13T14:17:40.045506: step 1572, loss 0.0162518, acc 0.984375\n",
      "2017-03-13T14:17:40.176599: step 1573, loss 0.00196694, acc 1\n",
      "2017-03-13T14:17:40.327706: step 1574, loss 0.00269257, acc 1\n",
      "2017-03-13T14:17:40.360730: step 1575, loss 0.00195705, acc 1\n",
      "2017-03-13T14:17:40.491822: step 1576, loss 0.00131775, acc 1\n",
      "2017-03-13T14:17:40.627919: step 1577, loss 0.0295068, acc 0.984375\n",
      "2017-03-13T14:17:40.765017: step 1578, loss 0.00251575, acc 1\n",
      "2017-03-13T14:17:40.899111: step 1579, loss 0.000841686, acc 1\n",
      "2017-03-13T14:17:41.033207: step 1580, loss 0.00161475, acc 1\n",
      "2017-03-13T14:17:41.163299: step 1581, loss 0.00225821, acc 1\n",
      "2017-03-13T14:17:41.309402: step 1582, loss 0.00180007, acc 1\n",
      "2017-03-13T14:17:41.439495: step 1583, loss 0.00284648, acc 1\n",
      "2017-03-13T14:17:41.567586: step 1584, loss 0.00754486, acc 1\n",
      "2017-03-13T14:17:41.709687: step 1585, loss 0.00370622, acc 1\n",
      "2017-03-13T14:17:41.852788: step 1586, loss 0.00599014, acc 1\n",
      "2017-03-13T14:17:41.993888: step 1587, loss 0.00955994, acc 1\n",
      "2017-03-13T14:17:42.126982: step 1588, loss 0.00183935, acc 1\n",
      "2017-03-13T14:17:42.279091: step 1589, loss 0.00142432, acc 1\n",
      "2017-03-13T14:17:42.316117: step 1590, loss 0.00015626, acc 1\n",
      "2017-03-13T14:17:42.443207: step 1591, loss 0.00647623, acc 1\n",
      "2017-03-13T14:17:42.571298: step 1592, loss 0.00426466, acc 1\n",
      "2017-03-13T14:17:42.715400: step 1593, loss 0.000621299, acc 1\n",
      "2017-03-13T14:17:42.854498: step 1594, loss 0.0203181, acc 0.984375\n",
      "2017-03-13T14:17:42.987593: step 1595, loss 0.00726824, acc 1\n",
      "2017-03-13T14:17:43.130696: step 1596, loss 0.000770627, acc 1\n",
      "2017-03-13T14:17:43.271795: step 1597, loss 0.00171416, acc 1\n",
      "2017-03-13T14:17:43.404891: step 1598, loss 0.00444253, acc 1\n",
      "2017-03-13T14:17:43.556998: step 1599, loss 0.00522641, acc 1\n",
      "2017-03-13T14:17:43.695100: step 1600, loss 0.00100859, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:17:43.770148: step 1600, loss 0.49988, acc 0.81\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1600\n",
      "\n",
      "2017-03-13T14:17:44.488072: step 1601, loss 0.000696138, acc 1\n",
      "2017-03-13T14:17:44.608081: step 1602, loss 0.00167193, acc 1\n",
      "2017-03-13T14:17:44.740723: step 1603, loss 0.00242569, acc 1\n",
      "2017-03-13T14:17:44.898835: step 1604, loss 0.000752799, acc 1\n",
      "2017-03-13T14:17:44.951873: step 1605, loss 0.0130202, acc 1\n",
      "2017-03-13T14:17:45.173030: step 1606, loss 0.000842108, acc 1\n",
      "2017-03-13T14:17:45.341151: step 1607, loss 0.00684697, acc 1\n",
      "2017-03-13T14:17:45.495258: step 1608, loss 0.00076262, acc 1\n",
      "2017-03-13T14:17:45.626351: step 1609, loss 0.000535732, acc 1\n",
      "2017-03-13T14:17:45.761447: step 1610, loss 0.00421685, acc 1\n",
      "2017-03-13T14:17:45.907552: step 1611, loss 0.000944163, acc 1\n",
      "2017-03-13T14:17:46.137713: step 1612, loss 0.00367977, acc 1\n",
      "2017-03-13T14:17:46.281816: step 1613, loss 0.00260323, acc 1\n",
      "2017-03-13T14:17:46.422917: step 1614, loss 0.00210124, acc 1\n",
      "2017-03-13T14:17:46.564016: step 1615, loss 0.0160333, acc 0.984375\n",
      "2017-03-13T14:17:46.704116: step 1616, loss 0.00130392, acc 1\n",
      "2017-03-13T14:17:46.848218: step 1617, loss 0.000577419, acc 1\n",
      "2017-03-13T14:17:47.031349: step 1618, loss 0.0014012, acc 1\n",
      "2017-03-13T14:17:47.164442: step 1619, loss 0.000527105, acc 1\n",
      "2017-03-13T14:17:47.198467: step 1620, loss 0.000181699, acc 1\n",
      "2017-03-13T14:17:47.337565: step 1621, loss 0.000891951, acc 1\n",
      "2017-03-13T14:17:47.539940: step 1622, loss 0.00163323, acc 1\n",
      "2017-03-13T14:17:47.673536: step 1623, loss 0.00104474, acc 1\n",
      "2017-03-13T14:17:47.802627: step 1624, loss 0.00064718, acc 1\n",
      "2017-03-13T14:17:47.953734: step 1625, loss 0.000702062, acc 1\n",
      "2017-03-13T14:17:48.103840: step 1626, loss 0.00144277, acc 1\n",
      "2017-03-13T14:17:48.239937: step 1627, loss 0.00761834, acc 1\n",
      "2017-03-13T14:17:48.377035: step 1628, loss 0.00172187, acc 1\n",
      "2017-03-13T14:17:48.501123: step 1629, loss 0.000618409, acc 1\n",
      "2017-03-13T14:17:48.642222: step 1630, loss 0.000608924, acc 1\n",
      "2017-03-13T14:17:48.771314: step 1631, loss 0.00279357, acc 1\n",
      "2017-03-13T14:17:48.910413: step 1632, loss 0.00212844, acc 1\n",
      "2017-03-13T14:17:49.050513: step 1633, loss 0.00047321, acc 1\n",
      "2017-03-13T14:17:49.187609: step 1634, loss 0.000897866, acc 1\n",
      "2017-03-13T14:17:49.222634: step 1635, loss 0.00330604, acc 1\n",
      "2017-03-13T14:17:49.351726: step 1636, loss 0.00139552, acc 1\n",
      "2017-03-13T14:17:49.496830: step 1637, loss 0.00100748, acc 1\n",
      "2017-03-13T14:17:49.632925: step 1638, loss 0.00209528, acc 1\n",
      "2017-03-13T14:17:49.768021: step 1639, loss 0.00207028, acc 1\n",
      "2017-03-13T14:17:49.903117: step 1640, loss 0.00108178, acc 1\n",
      "2017-03-13T14:17:50.044217: step 1641, loss 0.00224389, acc 1\n",
      "2017-03-13T14:17:50.173308: step 1642, loss 0.0291478, acc 0.984375\n",
      "2017-03-13T14:17:50.303401: step 1643, loss 0.00107234, acc 1\n",
      "2017-03-13T14:17:50.434495: step 1644, loss 0.0137608, acc 1\n",
      "2017-03-13T14:17:50.579597: step 1645, loss 0.00224205, acc 1\n",
      "2017-03-13T14:17:50.710691: step 1646, loss 0.00799924, acc 1\n",
      "2017-03-13T14:17:50.849789: step 1647, loss 0.00028271, acc 1\n",
      "2017-03-13T14:17:50.998894: step 1648, loss 0.00479874, acc 1\n",
      "2017-03-13T14:17:51.129987: step 1649, loss 0.00601605, acc 1\n",
      "2017-03-13T14:17:51.166013: step 1650, loss 0.000237765, acc 1\n",
      "2017-03-13T14:17:51.292103: step 1651, loss 0.0310541, acc 0.984375\n",
      "2017-03-13T14:17:51.437205: step 1652, loss 0.00402928, acc 1\n",
      "2017-03-13T14:17:51.566297: step 1653, loss 0.00320373, acc 1\n",
      "2017-03-13T14:17:51.693387: step 1654, loss 0.00036029, acc 1\n",
      "2017-03-13T14:17:51.834487: step 1655, loss 0.00303865, acc 1\n",
      "2017-03-13T14:17:51.965580: step 1656, loss 0.00190754, acc 1\n",
      "2017-03-13T14:17:52.095673: step 1657, loss 0.00183682, acc 1\n",
      "2017-03-13T14:17:52.229767: step 1658, loss 0.0016152, acc 1\n",
      "2017-03-13T14:17:52.368866: step 1659, loss 0.000888555, acc 1\n",
      "2017-03-13T14:17:52.522976: step 1660, loss 0.00165637, acc 1\n",
      "2017-03-13T14:17:52.673090: step 1661, loss 0.00657136, acc 1\n",
      "2017-03-13T14:17:52.823189: step 1662, loss 0.00133102, acc 1\n",
      "2017-03-13T14:17:52.960286: step 1663, loss 0.00114499, acc 1\n",
      "2017-03-13T14:17:53.098384: step 1664, loss 0.00183438, acc 1\n",
      "2017-03-13T14:17:53.132408: step 1665, loss 6.97365e-06, acc 1\n",
      "2017-03-13T14:17:53.269505: step 1666, loss 0.00368227, acc 1\n",
      "2017-03-13T14:17:53.397596: step 1667, loss 0.0246901, acc 0.984375\n",
      "2017-03-13T14:17:53.559712: step 1668, loss 0.00281697, acc 1\n",
      "2017-03-13T14:17:53.695807: step 1669, loss 0.00161266, acc 1\n",
      "2017-03-13T14:17:53.829903: step 1670, loss 0.00226761, acc 1\n",
      "2017-03-13T14:17:53.962997: step 1671, loss 0.00200691, acc 1\n",
      "2017-03-13T14:17:54.099093: step 1672, loss 0.000678551, acc 1\n",
      "2017-03-13T14:17:54.238192: step 1673, loss 0.000603736, acc 1\n",
      "2017-03-13T14:17:54.376290: step 1674, loss 0.00162613, acc 1\n",
      "2017-03-13T14:17:54.513390: step 1675, loss 0.000804519, acc 1\n",
      "2017-03-13T14:17:54.652486: step 1676, loss 0.00297172, acc 1\n",
      "2017-03-13T14:17:54.786581: step 1677, loss 0.00294202, acc 1\n",
      "2017-03-13T14:17:54.927681: step 1678, loss 0.000347793, acc 1\n",
      "2017-03-13T14:17:55.064779: step 1679, loss 0.00151616, acc 1\n",
      "2017-03-13T14:17:55.101805: step 1680, loss 7.98696e-06, acc 1\n",
      "2017-03-13T14:17:55.226893: step 1681, loss 0.000616907, acc 1\n",
      "2017-03-13T14:17:55.362990: step 1682, loss 0.000717276, acc 1\n",
      "2017-03-13T14:17:55.512096: step 1683, loss 0.00122576, acc 1\n",
      "2017-03-13T14:17:55.650194: step 1684, loss 0.00413522, acc 1\n",
      "2017-03-13T14:17:55.784289: step 1685, loss 0.000775776, acc 1\n",
      "2017-03-13T14:17:55.910379: step 1686, loss 0.00384563, acc 1\n",
      "2017-03-13T14:17:56.062488: step 1687, loss 0.0342115, acc 0.984375\n",
      "2017-03-13T14:17:56.199583: step 1688, loss 0.00432411, acc 1\n",
      "2017-03-13T14:17:56.328675: step 1689, loss 0.00144824, acc 1\n",
      "2017-03-13T14:17:56.459769: step 1690, loss 0.00090558, acc 1\n",
      "2017-03-13T14:17:56.601869: step 1691, loss 0.00109056, acc 1\n",
      "2017-03-13T14:17:56.743971: step 1692, loss 0.00116613, acc 1\n",
      "2017-03-13T14:17:56.883068: step 1693, loss 0.000334355, acc 1\n",
      "2017-03-13T14:17:57.019166: step 1694, loss 0.00282979, acc 1\n",
      "2017-03-13T14:17:57.052188: step 1695, loss 0.00260623, acc 1\n",
      "2017-03-13T14:17:57.182281: step 1696, loss 0.00120942, acc 1\n",
      "2017-03-13T14:17:57.322380: step 1697, loss 9.98716e-05, acc 1\n",
      "2017-03-13T14:17:57.462479: step 1698, loss 0.000706298, acc 1\n",
      "2017-03-13T14:17:57.613587: step 1699, loss 0.000377019, acc 1\n",
      "2017-03-13T14:17:57.762693: step 1700, loss 0.000872045, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:17:57.832742: step 1700, loss 0.495308, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1700\n",
      "\n",
      "2017-03-13T14:18:00.172545: step 1701, loss 0.00290588, acc 1\n",
      "2017-03-13T14:18:00.301958: step 1702, loss 0.00848067, acc 1\n",
      "2017-03-13T14:18:00.442057: step 1703, loss 0.00131197, acc 1\n",
      "2017-03-13T14:18:00.590163: step 1704, loss 0.00239984, acc 1\n",
      "2017-03-13T14:18:00.719254: step 1705, loss 0.000607283, acc 1\n",
      "2017-03-13T14:18:00.853349: step 1706, loss 0.0095845, acc 1\n",
      "2017-03-13T14:18:00.998452: step 1707, loss 0.000749117, acc 1\n",
      "2017-03-13T14:18:01.140553: step 1708, loss 0.0349176, acc 0.984375\n",
      "2017-03-13T14:18:01.283655: step 1709, loss 0.00152453, acc 1\n",
      "2017-03-13T14:18:01.323684: step 1710, loss 0.000217553, acc 1\n",
      "2017-03-13T14:18:01.475791: step 1711, loss 0.000455842, acc 1\n",
      "2017-03-13T14:18:01.608885: step 1712, loss 0.000285566, acc 1\n",
      "2017-03-13T14:18:01.743981: step 1713, loss 0.00111428, acc 1\n",
      "2017-03-13T14:18:01.883080: step 1714, loss 0.000258074, acc 1\n",
      "2017-03-13T14:18:02.028183: step 1715, loss 0.000610023, acc 1\n",
      "2017-03-13T14:18:02.200305: step 1716, loss 0.00183595, acc 1\n",
      "2017-03-13T14:18:02.352413: step 1717, loss 0.000585107, acc 1\n",
      "2017-03-13T14:18:02.499517: step 1718, loss 0.000184761, acc 1\n",
      "2017-03-13T14:18:02.654627: step 1719, loss 0.0318261, acc 0.984375\n",
      "2017-03-13T14:18:02.801732: step 1720, loss 0.00149703, acc 1\n",
      "2017-03-13T14:18:02.939829: step 1721, loss 0.00278649, acc 1\n",
      "2017-03-13T14:18:03.069921: step 1722, loss 0.00431633, acc 1\n",
      "2017-03-13T14:18:03.227033: step 1723, loss 0.0124362, acc 0.984375\n",
      "2017-03-13T14:18:03.375138: step 1724, loss 0.000330997, acc 1\n",
      "2017-03-13T14:18:03.407161: step 1725, loss 3.17682e-05, acc 1\n",
      "2017-03-13T14:18:03.543258: step 1726, loss 0.00265885, acc 1\n",
      "2017-03-13T14:18:03.685358: step 1727, loss 0.00228225, acc 1\n",
      "2017-03-13T14:18:03.816451: step 1728, loss 0.000611393, acc 1\n",
      "2017-03-13T14:18:03.960554: step 1729, loss 0.0015707, acc 1\n",
      "2017-03-13T14:18:04.098651: step 1730, loss 0.00576493, acc 1\n",
      "2017-03-13T14:18:04.246756: step 1731, loss 0.000231215, acc 1\n",
      "2017-03-13T14:18:04.384854: step 1732, loss 0.00128603, acc 1\n",
      "2017-03-13T14:18:04.522952: step 1733, loss 0.00178105, acc 1\n",
      "2017-03-13T14:18:04.680064: step 1734, loss 0.000500522, acc 1\n",
      "2017-03-13T14:18:04.819162: step 1735, loss 0.000965975, acc 1\n",
      "2017-03-13T14:18:04.949255: step 1736, loss 0.0232906, acc 0.984375\n",
      "2017-03-13T14:18:05.085351: step 1737, loss 0.0138627, acc 0.984375\n",
      "2017-03-13T14:18:05.228454: step 1738, loss 0.000680865, acc 1\n",
      "2017-03-13T14:18:05.362548: step 1739, loss 0.00230601, acc 1\n",
      "2017-03-13T14:18:05.403577: step 1740, loss 1.78814e-07, acc 1\n",
      "2017-03-13T14:18:05.527665: step 1741, loss 0.000600283, acc 1\n",
      "2017-03-13T14:18:05.659759: step 1742, loss 0.000841072, acc 1\n",
      "2017-03-13T14:18:05.806863: step 1743, loss 0.000560621, acc 1\n",
      "2017-03-13T14:18:05.945963: step 1744, loss 0.00345187, acc 1\n",
      "2017-03-13T14:18:06.080057: step 1745, loss 0.00131296, acc 1\n",
      "2017-03-13T14:18:06.213152: step 1746, loss 0.00369382, acc 1\n",
      "2017-03-13T14:18:06.368261: step 1747, loss 0.0029524, acc 1\n",
      "2017-03-13T14:18:06.507360: step 1748, loss 0.00385427, acc 1\n",
      "2017-03-13T14:18:06.644457: step 1749, loss 0.000669544, acc 1\n",
      "2017-03-13T14:18:06.772548: step 1750, loss 0.00244998, acc 1\n",
      "2017-03-13T14:18:06.925657: step 1751, loss 0.000486858, acc 1\n",
      "2017-03-13T14:18:07.068759: step 1752, loss 0.000886588, acc 1\n",
      "2017-03-13T14:18:07.197850: step 1753, loss 0.00323707, acc 1\n",
      "2017-03-13T14:18:07.341953: step 1754, loss 0.0130514, acc 0.984375\n",
      "2017-03-13T14:18:07.382981: step 1755, loss 0.00221046, acc 1\n",
      "2017-03-13T14:18:07.517076: step 1756, loss 0.000584148, acc 1\n",
      "2017-03-13T14:18:07.647168: step 1757, loss 0.00074647, acc 1\n",
      "2017-03-13T14:18:07.778261: step 1758, loss 0.000845834, acc 1\n",
      "2017-03-13T14:18:07.914358: step 1759, loss 0.000499959, acc 1\n",
      "2017-03-13T14:18:08.051455: step 1760, loss 0.00390632, acc 1\n",
      "2017-03-13T14:18:08.191554: step 1761, loss 0.000624269, acc 1\n",
      "2017-03-13T14:18:08.327651: step 1762, loss 0.00041264, acc 1\n",
      "2017-03-13T14:18:08.466750: step 1763, loss 0.000412438, acc 1\n",
      "2017-03-13T14:18:08.642875: step 1764, loss 0.00127359, acc 1\n",
      "2017-03-13T14:18:08.798987: step 1765, loss 0.000325816, acc 1\n",
      "2017-03-13T14:18:08.951093: step 1766, loss 0.000298188, acc 1\n",
      "2017-03-13T14:18:09.088191: step 1767, loss 0.000953285, acc 1\n",
      "2017-03-13T14:18:09.220284: step 1768, loss 0.0029204, acc 1\n",
      "2017-03-13T14:18:09.355380: step 1769, loss 0.000447407, acc 1\n",
      "2017-03-13T14:18:09.385402: step 1770, loss 4.68766e-05, acc 1\n",
      "2017-03-13T14:18:09.531506: step 1771, loss 0.00917913, acc 1\n",
      "2017-03-13T14:18:09.677609: step 1772, loss 0.00323284, acc 1\n",
      "2017-03-13T14:18:09.817708: step 1773, loss 0.00142873, acc 1\n",
      "2017-03-13T14:18:09.960810: step 1774, loss 0.000938948, acc 1\n",
      "2017-03-13T14:18:10.096907: step 1775, loss 0.000563586, acc 1\n",
      "2017-03-13T14:18:10.244011: step 1776, loss 0.00238935, acc 1\n",
      "2017-03-13T14:18:10.384111: step 1777, loss 0.00147165, acc 1\n",
      "2017-03-13T14:18:10.516204: step 1778, loss 0.00103946, acc 1\n",
      "2017-03-13T14:18:10.655304: step 1779, loss 0.00143063, acc 1\n",
      "2017-03-13T14:18:10.784394: step 1780, loss 0.0020796, acc 1\n",
      "2017-03-13T14:18:10.923493: step 1781, loss 0.00111115, acc 1\n",
      "2017-03-13T14:18:11.060590: step 1782, loss 0.0126627, acc 1\n",
      "2017-03-13T14:18:11.218703: step 1783, loss 0.026344, acc 0.984375\n",
      "2017-03-13T14:18:11.356800: step 1784, loss 0.00086736, acc 1\n",
      "2017-03-13T14:18:11.391825: step 1785, loss 3.08437e-05, acc 1\n",
      "2017-03-13T14:18:11.517914: step 1786, loss 0.00124735, acc 1\n",
      "2017-03-13T14:18:11.660015: step 1787, loss 0.000238245, acc 1\n",
      "2017-03-13T14:18:11.801116: step 1788, loss 0.0023055, acc 1\n",
      "2017-03-13T14:18:11.932208: step 1789, loss 0.00117854, acc 1\n",
      "2017-03-13T14:18:12.065303: step 1790, loss 0.000638184, acc 1\n",
      "2017-03-13T14:18:12.203401: step 1791, loss 0.000827938, acc 1\n",
      "2017-03-13T14:18:12.351507: step 1792, loss 0.0015216, acc 1\n",
      "2017-03-13T14:18:12.483599: step 1793, loss 0.000413545, acc 1\n",
      "2017-03-13T14:18:12.620697: step 1794, loss 0.0047618, acc 1\n",
      "2017-03-13T14:18:12.747787: step 1795, loss 0.00226087, acc 1\n",
      "2017-03-13T14:18:12.877879: step 1796, loss 0.000754876, acc 1\n",
      "2017-03-13T14:18:13.010974: step 1797, loss 0.000773621, acc 1\n",
      "2017-03-13T14:18:13.140066: step 1798, loss 0.00106496, acc 1\n",
      "2017-03-13T14:18:13.268156: step 1799, loss 0.0138361, acc 0.984375\n",
      "2017-03-13T14:18:13.302180: step 1800, loss 0.00316756, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:18:13.382237: step 1800, loss 0.515817, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1800\n",
      "\n",
      "2017-03-13T14:18:14.107828: step 1801, loss 0.000655295, acc 1\n",
      "2017-03-13T14:18:14.235330: step 1802, loss 0.000833022, acc 1\n",
      "2017-03-13T14:18:14.368424: step 1803, loss 0.00515551, acc 1\n",
      "2017-03-13T14:18:14.531541: step 1804, loss 0.00259942, acc 1\n",
      "2017-03-13T14:18:14.771711: step 1805, loss 0.000544327, acc 1\n",
      "2017-03-13T14:18:14.943838: step 1806, loss 0.00135626, acc 1\n",
      "2017-03-13T14:18:15.090937: step 1807, loss 0.00196805, acc 1\n",
      "2017-03-13T14:18:15.231037: step 1808, loss 0.000677361, acc 1\n",
      "2017-03-13T14:18:15.361129: step 1809, loss 0.00242541, acc 1\n",
      "2017-03-13T14:18:15.499227: step 1810, loss 0.00258689, acc 1\n",
      "2017-03-13T14:18:15.626317: step 1811, loss 0.000519992, acc 1\n",
      "2017-03-13T14:18:15.760412: step 1812, loss 0.000411178, acc 1\n",
      "2017-03-13T14:18:15.902513: step 1813, loss 0.00220449, acc 1\n",
      "2017-03-13T14:18:16.039611: step 1814, loss 0.00122966, acc 1\n",
      "2017-03-13T14:18:16.092649: step 1815, loss 3.33176e-05, acc 1\n",
      "2017-03-13T14:18:16.242756: step 1816, loss 0.000924281, acc 1\n",
      "2017-03-13T14:18:16.397864: step 1817, loss 0.000594448, acc 1\n",
      "2017-03-13T14:18:16.531960: step 1818, loss 0.00986502, acc 1\n",
      "2017-03-13T14:18:16.680065: step 1819, loss 0.00343153, acc 1\n",
      "2017-03-13T14:18:16.823166: step 1820, loss 0.000457761, acc 1\n",
      "2017-03-13T14:18:16.960264: step 1821, loss 0.000703317, acc 1\n",
      "2017-03-13T14:18:17.093357: step 1822, loss 0.000986184, acc 1\n",
      "2017-03-13T14:18:17.241464: step 1823, loss 0.000584432, acc 1\n",
      "2017-03-13T14:18:17.386566: step 1824, loss 0.00110122, acc 1\n",
      "2017-03-13T14:18:17.517658: step 1825, loss 0.00120515, acc 1\n",
      "2017-03-13T14:18:17.650753: step 1826, loss 0.00119294, acc 1\n",
      "2017-03-13T14:18:17.794855: step 1827, loss 0.0100785, acc 1\n",
      "2017-03-13T14:18:17.927950: step 1828, loss 0.000483473, acc 1\n",
      "2017-03-13T14:18:18.067048: step 1829, loss 0.00128613, acc 1\n",
      "2017-03-13T14:18:18.100073: step 1830, loss 2.74181e-06, acc 1\n",
      "2017-03-13T14:18:18.240171: step 1831, loss 0.00196261, acc 1\n",
      "2017-03-13T14:18:18.383273: step 1832, loss 0.0018731, acc 1\n",
      "2017-03-13T14:18:18.515366: step 1833, loss 0.00141337, acc 1\n",
      "2017-03-13T14:18:18.642456: step 1834, loss 0.00161498, acc 1\n",
      "2017-03-13T14:18:18.772548: step 1835, loss 0.000856271, acc 1\n",
      "2017-03-13T14:18:18.913650: step 1836, loss 0.0015267, acc 1\n",
      "2017-03-13T14:18:19.069766: step 1837, loss 0.00154376, acc 1\n",
      "2017-03-13T14:18:19.225872: step 1838, loss 0.000862116, acc 1\n",
      "2017-03-13T14:18:19.363969: step 1839, loss 0.00689566, acc 1\n",
      "2017-03-13T14:18:19.489057: step 1840, loss 0.00272642, acc 1\n",
      "2017-03-13T14:18:19.639163: step 1841, loss 0.0371304, acc 0.984375\n",
      "2017-03-13T14:18:19.779263: step 1842, loss 0.00082731, acc 1\n",
      "2017-03-13T14:18:19.912357: step 1843, loss 0.0031557, acc 1\n",
      "2017-03-13T14:18:20.041449: step 1844, loss 0.00163125, acc 1\n",
      "2017-03-13T14:18:20.076474: step 1845, loss 0.000156458, acc 1\n",
      "2017-03-13T14:18:20.216573: step 1846, loss 0.00033937, acc 1\n",
      "2017-03-13T14:18:20.345664: step 1847, loss 0.0014335, acc 1\n",
      "2017-03-13T14:18:20.473756: step 1848, loss 0.000364755, acc 1\n",
      "2017-03-13T14:18:20.606850: step 1849, loss 0.00209044, acc 1\n",
      "2017-03-13T14:18:20.742947: step 1850, loss 0.0028875, acc 1\n",
      "2017-03-13T14:18:20.885047: step 1851, loss 0.00925282, acc 1\n",
      "2017-03-13T14:18:21.018142: step 1852, loss 0.00135913, acc 1\n",
      "2017-03-13T14:18:21.148234: step 1853, loss 0.00610013, acc 1\n",
      "2017-03-13T14:18:21.283330: step 1854, loss 0.00176654, acc 1\n",
      "2017-03-13T14:18:21.443444: step 1855, loss 0.000702803, acc 1\n",
      "2017-03-13T14:18:21.600555: step 1856, loss 0.000911874, acc 1\n",
      "2017-03-13T14:18:21.735651: step 1857, loss 0.0032414, acc 1\n",
      "2017-03-13T14:18:21.879753: step 1858, loss 0.00136771, acc 1\n",
      "2017-03-13T14:18:22.029860: step 1859, loss 0.000921679, acc 1\n",
      "2017-03-13T14:18:22.063884: step 1860, loss 0.000205927, acc 1\n",
      "2017-03-13T14:18:22.188973: step 1861, loss 0.000460344, acc 1\n",
      "2017-03-13T14:18:22.320066: step 1862, loss 0.000857785, acc 1\n",
      "2017-03-13T14:18:22.460165: step 1863, loss 0.000608459, acc 1\n",
      "2017-03-13T14:18:22.603266: step 1864, loss 0.00159733, acc 1\n",
      "2017-03-13T14:18:22.747368: step 1865, loss 0.0119253, acc 0.984375\n",
      "2017-03-13T14:18:22.884466: step 1866, loss 0.000447945, acc 1\n",
      "2017-03-13T14:18:23.020563: step 1867, loss 0.00140155, acc 1\n",
      "2017-03-13T14:18:23.150655: step 1868, loss 0.00399987, acc 1\n",
      "2017-03-13T14:18:23.281748: step 1869, loss 0.00657503, acc 1\n",
      "2017-03-13T14:18:23.418845: step 1870, loss 0.000380086, acc 1\n",
      "2017-03-13T14:18:23.546935: step 1871, loss 0.000579088, acc 1\n",
      "2017-03-13T14:18:23.694041: step 1872, loss 0.00120639, acc 1\n",
      "2017-03-13T14:18:23.845147: step 1873, loss 0.00164139, acc 1\n",
      "2017-03-13T14:18:23.990250: step 1874, loss 0.000645227, acc 1\n",
      "2017-03-13T14:18:24.029278: step 1875, loss 0.000237831, acc 1\n",
      "2017-03-13T14:18:24.162372: step 1876, loss 0.000780452, acc 1\n",
      "2017-03-13T14:18:24.309477: step 1877, loss 0.00185363, acc 1\n",
      "2017-03-13T14:18:24.445573: step 1878, loss 0.00171804, acc 1\n",
      "2017-03-13T14:18:24.571664: step 1879, loss 0.0129775, acc 0.984375\n",
      "2017-03-13T14:18:24.707759: step 1880, loss 0.00129586, acc 1\n",
      "2017-03-13T14:18:24.855867: step 1881, loss 0.000831015, acc 1\n",
      "2017-03-13T14:18:24.990960: step 1882, loss 0.00421866, acc 1\n",
      "2017-03-13T14:18:25.133061: step 1883, loss 0.000522029, acc 1\n",
      "2017-03-13T14:18:25.262153: step 1884, loss 0.000743395, acc 1\n",
      "2017-03-13T14:18:25.399250: step 1885, loss 0.00180065, acc 1\n",
      "2017-03-13T14:18:25.531343: step 1886, loss 0.0133211, acc 0.984375\n",
      "2017-03-13T14:18:25.665438: step 1887, loss 0.00059441, acc 1\n",
      "2017-03-13T14:18:25.799533: step 1888, loss 0.000773546, acc 1\n",
      "2017-03-13T14:18:25.932628: step 1889, loss 0.00100784, acc 1\n",
      "2017-03-13T14:18:25.984666: step 1890, loss 0.00036877, acc 1\n",
      "2017-03-13T14:18:26.143779: step 1891, loss 0.00100913, acc 1\n",
      "2017-03-13T14:18:26.305893: step 1892, loss 0.000775465, acc 1\n",
      "2017-03-13T14:18:26.471010: step 1893, loss 0.00156592, acc 1\n",
      "2017-03-13T14:18:26.629122: step 1894, loss 0.000364026, acc 1\n",
      "2017-03-13T14:18:26.794239: step 1895, loss 0.00202588, acc 1\n",
      "2017-03-13T14:18:26.943345: step 1896, loss 0.0393811, acc 0.984375\n",
      "2017-03-13T14:18:27.095453: step 1897, loss 0.000409598, acc 1\n",
      "2017-03-13T14:18:27.249562: step 1898, loss 0.000663287, acc 1\n",
      "2017-03-13T14:18:27.386660: step 1899, loss 0.000383991, acc 1\n",
      "2017-03-13T14:18:27.535766: step 1900, loss 0.00956721, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:18:27.625829: step 1900, loss 0.515093, acc 0.81\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-1900\n",
      "\n",
      "2017-03-13T14:18:28.599521: step 1901, loss 0.016197, acc 0.984375\n",
      "2017-03-13T14:18:28.785192: step 1902, loss 0.00064183, acc 1\n",
      "2017-03-13T14:18:29.025364: step 1903, loss 0.000674783, acc 1\n",
      "2017-03-13T14:18:29.232509: step 1904, loss 0.000412176, acc 1\n",
      "2017-03-13T14:18:29.278542: step 1905, loss 1.39769e-05, acc 1\n",
      "2017-03-13T14:18:29.431651: step 1906, loss 0.00188645, acc 1\n",
      "2017-03-13T14:18:29.586760: step 1907, loss 0.000646517, acc 1\n",
      "2017-03-13T14:18:29.744872: step 1908, loss 0.00128313, acc 1\n",
      "2017-03-13T14:18:29.888974: step 1909, loss 0.000729671, acc 1\n",
      "2017-03-13T14:18:30.035079: step 1910, loss 0.000324532, acc 1\n",
      "2017-03-13T14:18:30.179181: step 1911, loss 0.000607147, acc 1\n",
      "2017-03-13T14:18:30.331293: step 1912, loss 0.00107489, acc 1\n",
      "2017-03-13T14:18:30.486398: step 1913, loss 0.00216024, acc 1\n",
      "2017-03-13T14:18:30.640507: step 1914, loss 0.000984413, acc 1\n",
      "2017-03-13T14:18:30.785610: step 1915, loss 0.00132373, acc 1\n",
      "2017-03-13T14:18:30.943722: step 1916, loss 0.00102177, acc 1\n",
      "2017-03-13T14:18:31.097832: step 1917, loss 0.00277578, acc 1\n",
      "2017-03-13T14:18:31.240933: step 1918, loss 0.000623575, acc 1\n",
      "2017-03-13T14:18:31.402048: step 1919, loss 0.000711125, acc 1\n",
      "2017-03-13T14:18:31.441076: step 1920, loss 0.000791043, acc 1\n",
      "2017-03-13T14:18:31.579174: step 1921, loss 0.000555805, acc 1\n",
      "2017-03-13T14:18:31.725277: step 1922, loss 0.0018798, acc 1\n",
      "2017-03-13T14:18:31.874383: step 1923, loss 0.000789847, acc 1\n",
      "2017-03-13T14:18:32.035497: step 1924, loss 0.00148884, acc 1\n",
      "2017-03-13T14:18:32.200615: step 1925, loss 0.000745879, acc 1\n",
      "2017-03-13T14:18:32.367733: step 1926, loss 0.000638151, acc 1\n",
      "2017-03-13T14:18:32.520841: step 1927, loss 0.000450844, acc 1\n",
      "2017-03-13T14:18:32.685958: step 1928, loss 0.000887082, acc 1\n",
      "2017-03-13T14:18:32.850075: step 1929, loss 0.00186048, acc 1\n",
      "2017-03-13T14:18:33.010188: step 1930, loss 0.000540821, acc 1\n",
      "2017-03-13T14:18:33.157293: step 1931, loss 0.000205637, acc 1\n",
      "2017-03-13T14:18:33.303396: step 1932, loss 0.00113918, acc 1\n",
      "2017-03-13T14:18:33.452502: step 1933, loss 0.000292646, acc 1\n",
      "2017-03-13T14:18:33.595604: step 1934, loss 0.000539333, acc 1\n",
      "2017-03-13T14:18:33.642638: step 1935, loss 1.31427e-05, acc 1\n",
      "2017-03-13T14:18:33.779735: step 1936, loss 0.0022748, acc 1\n",
      "2017-03-13T14:18:33.940849: step 1937, loss 0.000436663, acc 1\n",
      "2017-03-13T14:18:34.083950: step 1938, loss 0.00234414, acc 1\n",
      "2017-03-13T14:18:34.230054: step 1939, loss 0.000957499, acc 1\n",
      "2017-03-13T14:18:34.379160: step 1940, loss 0.000657255, acc 1\n",
      "2017-03-13T14:18:34.536271: step 1941, loss 0.0115788, acc 0.984375\n",
      "2017-03-13T14:18:34.682375: step 1942, loss 0.000249853, acc 1\n",
      "2017-03-13T14:18:34.831480: step 1943, loss 0.0017709, acc 1\n",
      "2017-03-13T14:18:34.980587: step 1944, loss 0.000920277, acc 1\n",
      "2017-03-13T14:18:35.123689: step 1945, loss 0.00262415, acc 1\n",
      "2017-03-13T14:18:35.287804: step 1946, loss 0.00211609, acc 1\n",
      "2017-03-13T14:18:35.439912: step 1947, loss 0.000100515, acc 1\n",
      "2017-03-13T14:18:35.585015: step 1948, loss 0.00139756, acc 1\n",
      "2017-03-13T14:18:35.747131: step 1949, loss 0.00498052, acc 1\n",
      "2017-03-13T14:18:35.793163: step 1950, loss 5.74253e-05, acc 1\n",
      "2017-03-13T14:18:35.930260: step 1951, loss 0.000294563, acc 1\n",
      "2017-03-13T14:18:36.088372: step 1952, loss 0.00267971, acc 1\n",
      "2017-03-13T14:18:36.234476: step 1953, loss 0.00028808, acc 1\n",
      "2017-03-13T14:18:36.386584: step 1954, loss 0.00115771, acc 1\n",
      "2017-03-13T14:18:36.544696: step 1955, loss 0.00151867, acc 1\n",
      "2017-03-13T14:18:36.700807: step 1956, loss 0.00119421, acc 1\n",
      "2017-03-13T14:18:36.840906: step 1957, loss 0.00219199, acc 1\n",
      "2017-03-13T14:18:36.998018: step 1958, loss 0.00021233, acc 1\n",
      "2017-03-13T14:18:37.148125: step 1959, loss 0.00433407, acc 1\n",
      "2017-03-13T14:18:37.302233: step 1960, loss 0.00439249, acc 1\n",
      "2017-03-13T14:18:37.449338: step 1961, loss 0.00119318, acc 1\n",
      "2017-03-13T14:18:37.596442: step 1962, loss 0.000842003, acc 1\n",
      "2017-03-13T14:18:37.739543: step 1963, loss 0.000330589, acc 1\n",
      "2017-03-13T14:18:37.893653: step 1964, loss 0.00727092, acc 1\n",
      "2017-03-13T14:18:37.934682: step 1965, loss 5.27499e-06, acc 1\n",
      "2017-03-13T14:18:38.087790: step 1966, loss 0.00198729, acc 1\n",
      "2017-03-13T14:18:38.243901: step 1967, loss 0.00480177, acc 1\n",
      "2017-03-13T14:18:38.388003: step 1968, loss 0.000822567, acc 1\n",
      "2017-03-13T14:18:38.550118: step 1969, loss 0.000305924, acc 1\n",
      "2017-03-13T14:18:38.693220: step 1970, loss 0.000262631, acc 1\n",
      "2017-03-13T14:18:38.845328: step 1971, loss 0.027614, acc 0.984375\n",
      "2017-03-13T14:18:39.006443: step 1972, loss 0.00207681, acc 1\n",
      "2017-03-13T14:18:39.159551: step 1973, loss 0.000202217, acc 1\n",
      "2017-03-13T14:18:39.309657: step 1974, loss 0.0369128, acc 0.984375\n",
      "2017-03-13T14:18:39.461765: step 1975, loss 0.00286861, acc 1\n",
      "2017-03-13T14:18:39.610871: step 1976, loss 0.00208156, acc 1\n",
      "2017-03-13T14:18:39.749970: step 1977, loss 0.000488257, acc 1\n",
      "2017-03-13T14:18:39.918089: step 1978, loss 0.000623166, acc 1\n",
      "2017-03-13T14:18:40.070197: step 1979, loss 0.00135034, acc 1\n",
      "2017-03-13T14:18:40.112227: step 1980, loss 4.02615e-05, acc 1\n",
      "2017-03-13T14:18:40.248323: step 1981, loss 0.00170163, acc 1\n",
      "2017-03-13T14:18:40.400431: step 1982, loss 0.0003801, acc 1\n",
      "2017-03-13T14:18:40.561546: step 1983, loss 0.00305519, acc 1\n",
      "2017-03-13T14:18:40.732667: step 1984, loss 0.000886072, acc 1\n",
      "2017-03-13T14:18:40.885776: step 1985, loss 0.00574732, acc 1\n",
      "2017-03-13T14:18:41.041886: step 1986, loss 0.000544293, acc 1\n",
      "2017-03-13T14:18:41.206003: step 1987, loss 0.00162443, acc 1\n",
      "2017-03-13T14:18:41.363114: step 1988, loss 0.00317838, acc 1\n",
      "2017-03-13T14:18:41.517224: step 1989, loss 0.0186297, acc 0.984375\n",
      "2017-03-13T14:18:41.660325: step 1990, loss 0.00105093, acc 1\n",
      "2017-03-13T14:18:41.812433: step 1991, loss 0.000520964, acc 1\n",
      "2017-03-13T14:18:41.973548: step 1992, loss 0.0018312, acc 1\n",
      "2017-03-13T14:18:42.122653: step 1993, loss 0.000432385, acc 1\n",
      "2017-03-13T14:18:42.274761: step 1994, loss 0.000314178, acc 1\n",
      "2017-03-13T14:18:42.314790: step 1995, loss 0.00924228, acc 1\n",
      "2017-03-13T14:18:42.473902: step 1996, loss 0.000416847, acc 1\n",
      "2017-03-13T14:18:42.621006: step 1997, loss 0.00169012, acc 1\n",
      "2017-03-13T14:18:42.777117: step 1998, loss 0.00122394, acc 1\n",
      "2017-03-13T14:18:42.939232: step 1999, loss 0.00217163, acc 1\n",
      "2017-03-13T14:18:43.118359: step 2000, loss 0.000407683, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:18:43.206422: step 2000, loss 0.520139, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2000\n",
      "\n",
      "2017-03-13T14:18:45.517803: step 2001, loss 0.00117598, acc 1\n",
      "2017-03-13T14:18:45.648510: step 2002, loss 0.00192522, acc 1\n",
      "2017-03-13T14:18:45.791612: step 2003, loss 0.00029578, acc 1\n",
      "2017-03-13T14:18:45.929710: step 2004, loss 0.00535746, acc 1\n",
      "2017-03-13T14:18:46.060803: step 2005, loss 0.000714706, acc 1\n",
      "2017-03-13T14:18:46.191896: step 2006, loss 0.00108006, acc 1\n",
      "2017-03-13T14:18:46.331995: step 2007, loss 0.00131827, acc 1\n",
      "2017-03-13T14:18:46.462087: step 2008, loss 0.000364634, acc 1\n",
      "2017-03-13T14:18:46.601186: step 2009, loss 0.000513121, acc 1\n",
      "2017-03-13T14:18:46.636212: step 2010, loss 4.72642e-05, acc 1\n",
      "2017-03-13T14:18:46.774310: step 2011, loss 0.000624948, acc 1\n",
      "2017-03-13T14:18:46.916409: step 2012, loss 0.000199283, acc 1\n",
      "2017-03-13T14:18:47.052506: step 2013, loss 0.00057934, acc 1\n",
      "2017-03-13T14:18:47.185600: step 2014, loss 8.94455e-05, acc 1\n",
      "2017-03-13T14:18:47.323698: step 2015, loss 0.00497226, acc 1\n",
      "2017-03-13T14:18:47.473806: step 2016, loss 0.000172816, acc 1\n",
      "2017-03-13T14:18:47.622911: step 2017, loss 0.000821455, acc 1\n",
      "2017-03-13T14:18:47.755005: step 2018, loss 0.000567811, acc 1\n",
      "2017-03-13T14:18:47.891102: step 2019, loss 0.000686039, acc 1\n",
      "2017-03-13T14:18:48.020193: step 2020, loss 0.00146363, acc 1\n",
      "2017-03-13T14:18:48.170299: step 2021, loss 0.000390481, acc 1\n",
      "2017-03-13T14:18:48.310399: step 2022, loss 0.000291045, acc 1\n",
      "2017-03-13T14:18:48.439490: step 2023, loss 0.00109218, acc 1\n",
      "2017-03-13T14:18:48.581592: step 2024, loss 0.00805707, acc 1\n",
      "2017-03-13T14:18:48.622623: step 2025, loss 0.00151563, acc 1\n",
      "2017-03-13T14:18:48.767723: step 2026, loss 0.000158156, acc 1\n",
      "2017-03-13T14:18:48.899817: step 2027, loss 0.000350152, acc 1\n",
      "2017-03-13T14:18:49.035913: step 2028, loss 0.0647968, acc 0.984375\n",
      "2017-03-13T14:18:49.177013: step 2029, loss 0.000248656, acc 1\n",
      "2017-03-13T14:18:49.314110: step 2030, loss 0.000155342, acc 1\n",
      "2017-03-13T14:18:49.443202: step 2031, loss 0.0118721, acc 0.984375\n",
      "2017-03-13T14:18:49.589307: step 2032, loss 0.00163209, acc 1\n",
      "2017-03-13T14:18:49.720400: step 2033, loss 0.00550681, acc 1\n",
      "2017-03-13T14:18:49.866502: step 2034, loss 0.00156495, acc 1\n",
      "2017-03-13T14:18:50.006602: step 2035, loss 0.000303908, acc 1\n",
      "2017-03-13T14:18:50.137695: step 2036, loss 0.0017558, acc 1\n",
      "2017-03-13T14:18:50.284802: step 2037, loss 0.000879709, acc 1\n",
      "2017-03-13T14:18:50.426901: step 2038, loss 0.00104932, acc 1\n",
      "2017-03-13T14:18:50.557993: step 2039, loss 0.00166924, acc 1\n",
      "2017-03-13T14:18:50.591017: step 2040, loss 2.17556e-06, acc 1\n",
      "2017-03-13T14:18:50.729115: step 2041, loss 0.000901393, acc 1\n",
      "2017-03-13T14:18:50.873216: step 2042, loss 0.000482527, acc 1\n",
      "2017-03-13T14:18:51.004309: step 2043, loss 0.000707454, acc 1\n",
      "2017-03-13T14:18:51.135403: step 2044, loss 0.00108222, acc 1\n",
      "2017-03-13T14:18:51.267497: step 2045, loss 0.00349074, acc 1\n",
      "2017-03-13T14:18:51.400591: step 2046, loss 0.00432114, acc 1\n",
      "2017-03-13T14:18:51.546696: step 2047, loss 0.00189643, acc 1\n",
      "2017-03-13T14:18:51.673785: step 2048, loss 0.00101889, acc 1\n",
      "2017-03-13T14:18:51.805878: step 2049, loss 0.000143199, acc 1\n",
      "2017-03-13T14:18:51.947979: step 2050, loss 0.000702409, acc 1\n",
      "2017-03-13T14:18:52.085077: step 2051, loss 0.000309599, acc 1\n",
      "2017-03-13T14:18:52.216170: step 2052, loss 0.00206054, acc 1\n",
      "2017-03-13T14:18:52.359271: step 2053, loss 0.00132232, acc 1\n",
      "2017-03-13T14:18:52.496368: step 2054, loss 0.000677945, acc 1\n",
      "2017-03-13T14:18:52.528392: step 2055, loss 0.000153425, acc 1\n",
      "2017-03-13T14:18:52.654481: step 2056, loss 0.00171369, acc 1\n",
      "2017-03-13T14:18:52.794580: step 2057, loss 0.000316784, acc 1\n",
      "2017-03-13T14:18:52.923671: step 2058, loss 0.000496692, acc 1\n",
      "2017-03-13T14:18:53.048760: step 2059, loss 0.000621358, acc 1\n",
      "2017-03-13T14:18:53.179853: step 2060, loss 0.000443967, acc 1\n",
      "2017-03-13T14:18:53.318952: step 2061, loss 0.00438885, acc 1\n",
      "2017-03-13T14:18:53.455048: step 2062, loss 0.000490771, acc 1\n",
      "2017-03-13T14:18:53.590144: step 2063, loss 0.000642802, acc 1\n",
      "2017-03-13T14:18:53.721237: step 2064, loss 0.000452578, acc 1\n",
      "2017-03-13T14:18:53.850329: step 2065, loss 0.000604511, acc 1\n",
      "2017-03-13T14:18:54.011444: step 2066, loss 0.00251122, acc 1\n",
      "2017-03-13T14:18:54.157547: step 2067, loss 0.00100382, acc 1\n",
      "2017-03-13T14:18:54.303650: step 2068, loss 0.000258007, acc 1\n",
      "2017-03-13T14:18:54.446752: step 2069, loss 0.00446947, acc 1\n",
      "2017-03-13T14:18:54.480777: step 2070, loss 0.000345441, acc 1\n",
      "2017-03-13T14:18:54.625881: step 2071, loss 0.00446487, acc 1\n",
      "2017-03-13T14:18:54.791996: step 2072, loss 0.0168201, acc 0.984375\n",
      "2017-03-13T14:18:54.931096: step 2073, loss 0.000580781, acc 1\n",
      "2017-03-13T14:18:55.064190: step 2074, loss 0.000527935, acc 1\n",
      "2017-03-13T14:18:55.195283: step 2075, loss 0.00871593, acc 1\n",
      "2017-03-13T14:18:55.346391: step 2076, loss 0.00135698, acc 1\n",
      "2017-03-13T14:18:55.476482: step 2077, loss 0.0130731, acc 0.984375\n",
      "2017-03-13T14:18:55.613580: step 2078, loss 0.00817109, acc 1\n",
      "2017-03-13T14:18:55.771694: step 2079, loss 0.000304672, acc 1\n",
      "2017-03-13T14:18:55.911791: step 2080, loss 0.000738023, acc 1\n",
      "2017-03-13T14:18:56.046887: step 2081, loss 0.000633337, acc 1\n",
      "2017-03-13T14:18:56.197994: step 2082, loss 0.000497403, acc 1\n",
      "2017-03-13T14:18:56.341095: step 2083, loss 0.000306853, acc 1\n",
      "2017-03-13T14:18:56.484198: step 2084, loss 0.0099609, acc 1\n",
      "2017-03-13T14:18:56.521224: step 2085, loss 0.000206446, acc 1\n",
      "2017-03-13T14:18:56.651315: step 2086, loss 0.00609992, acc 1\n",
      "2017-03-13T14:18:56.782409: step 2087, loss 0.00447569, acc 1\n",
      "2017-03-13T14:18:56.916504: step 2088, loss 0.00118427, acc 1\n",
      "2017-03-13T14:18:57.056604: step 2089, loss 0.000671152, acc 1\n",
      "2017-03-13T14:18:57.196703: step 2090, loss 0.000641923, acc 1\n",
      "2017-03-13T14:18:57.323793: step 2091, loss 0.000549421, acc 1\n",
      "2017-03-13T14:18:57.453885: step 2092, loss 0.00430574, acc 1\n",
      "2017-03-13T14:18:57.586979: step 2093, loss 0.0121836, acc 0.984375\n",
      "2017-03-13T14:18:57.738087: step 2094, loss 0.00389984, acc 1\n",
      "2017-03-13T14:18:57.875184: step 2095, loss 0.00281773, acc 1\n",
      "2017-03-13T14:18:58.009279: step 2096, loss 0.000692921, acc 1\n",
      "2017-03-13T14:18:58.143374: step 2097, loss 0.00255829, acc 1\n",
      "2017-03-13T14:18:58.277469: step 2098, loss 0.000942137, acc 1\n",
      "2017-03-13T14:18:58.407561: step 2099, loss 0.000863519, acc 1\n",
      "2017-03-13T14:18:58.441586: step 2100, loss 3.24843e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:18:58.510635: step 2100, loss 0.543152, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2100\n",
      "\n",
      "2017-03-13T14:18:59.311166: step 2101, loss 0.0310761, acc 0.984375\n",
      "2017-03-13T14:18:59.436404: step 2102, loss 0.00377971, acc 1\n",
      "2017-03-13T14:18:59.583509: step 2103, loss 0.000538915, acc 1\n",
      "2017-03-13T14:18:59.758633: step 2104, loss 0.000358473, acc 1\n",
      "2017-03-13T14:18:59.991800: step 2105, loss 0.000247472, acc 1\n",
      "2017-03-13T14:19:00.144908: step 2106, loss 0.000804059, acc 1\n",
      "2017-03-13T14:19:00.299018: step 2107, loss 0.000623754, acc 1\n",
      "2017-03-13T14:19:00.448123: step 2108, loss 0.000802018, acc 1\n",
      "2017-03-13T14:19:00.590224: step 2109, loss 0.000341632, acc 1\n",
      "2017-03-13T14:19:00.724319: step 2110, loss 0.000164224, acc 1\n",
      "2017-03-13T14:19:00.878434: step 2111, loss 0.00143837, acc 1\n",
      "2017-03-13T14:19:01.036541: step 2112, loss 0.000426649, acc 1\n",
      "2017-03-13T14:19:01.183644: step 2113, loss 0.000452713, acc 1\n",
      "2017-03-13T14:19:01.331750: step 2114, loss 0.0001168, acc 1\n",
      "2017-03-13T14:19:01.374780: step 2115, loss 0.0200357, acc 1\n",
      "2017-03-13T14:19:01.526888: step 2116, loss 0.0041029, acc 1\n",
      "2017-03-13T14:19:01.682999: step 2117, loss 0.000496904, acc 1\n",
      "2017-03-13T14:19:01.829102: step 2118, loss 0.00206981, acc 1\n",
      "2017-03-13T14:19:01.976207: step 2119, loss 0.00165526, acc 1\n",
      "2017-03-13T14:19:02.123311: step 2120, loss 0.00279982, acc 1\n",
      "2017-03-13T14:19:02.280422: step 2121, loss 0.00195054, acc 1\n",
      "2017-03-13T14:19:02.432531: step 2122, loss 0.000905787, acc 1\n",
      "2017-03-13T14:19:02.579636: step 2123, loss 0.0285888, acc 0.984375\n",
      "2017-03-13T14:19:02.733744: step 2124, loss 0.000489263, acc 1\n",
      "2017-03-13T14:19:02.883850: step 2125, loss 0.000443166, acc 1\n",
      "2017-03-13T14:19:03.028953: step 2126, loss 0.00022697, acc 1\n",
      "2017-03-13T14:19:03.178060: step 2127, loss 0.00286275, acc 1\n",
      "2017-03-13T14:19:03.320160: step 2128, loss 0.0143591, acc 0.984375\n",
      "2017-03-13T14:19:03.481274: step 2129, loss 0.000849077, acc 1\n",
      "2017-03-13T14:19:03.524304: step 2130, loss 0.00582174, acc 1\n",
      "2017-03-13T14:19:03.683418: step 2131, loss 0.000286746, acc 1\n",
      "2017-03-13T14:19:03.844532: step 2132, loss 0.00073344, acc 1\n",
      "2017-03-13T14:19:04.000643: step 2133, loss 0.000228637, acc 1\n",
      "2017-03-13T14:19:04.159757: step 2134, loss 0.000386833, acc 1\n",
      "2017-03-13T14:19:04.305859: step 2135, loss 0.000188666, acc 1\n",
      "2017-03-13T14:19:04.449961: step 2136, loss 0.00135612, acc 1\n",
      "2017-03-13T14:19:04.600068: step 2137, loss 0.000315169, acc 1\n",
      "2017-03-13T14:19:04.767189: step 2138, loss 0.00049083, acc 1\n",
      "2017-03-13T14:19:04.920296: step 2139, loss 0.00122737, acc 1\n",
      "2017-03-13T14:19:05.073405: step 2140, loss 0.000191848, acc 1\n",
      "2017-03-13T14:19:05.223510: step 2141, loss 0.000160942, acc 1\n",
      "2017-03-13T14:19:05.362609: step 2142, loss 0.00164488, acc 1\n",
      "2017-03-13T14:19:05.515718: step 2143, loss 0.000846702, acc 1\n",
      "2017-03-13T14:19:05.670827: step 2144, loss 0.000642405, acc 1\n",
      "2017-03-13T14:19:05.721864: step 2145, loss 5.81682e-05, acc 1\n",
      "2017-03-13T14:19:05.864965: step 2146, loss 0.0021745, acc 1\n",
      "2017-03-13T14:19:06.018074: step 2147, loss 0.00391247, acc 1\n",
      "2017-03-13T14:19:06.170182: step 2148, loss 0.00257535, acc 1\n",
      "2017-03-13T14:19:06.325292: step 2149, loss 0.000778178, acc 1\n",
      "2017-03-13T14:19:06.471396: step 2150, loss 0.000732094, acc 1\n",
      "2017-03-13T14:19:06.615498: step 2151, loss 0.00505958, acc 1\n",
      "2017-03-13T14:19:06.765605: step 2152, loss 0.00108709, acc 1\n",
      "2017-03-13T14:19:06.917712: step 2153, loss 0.000778409, acc 1\n",
      "2017-03-13T14:19:07.047805: step 2154, loss 0.000357617, acc 1\n",
      "2017-03-13T14:19:07.186903: step 2155, loss 0.000467009, acc 1\n",
      "2017-03-13T14:19:07.324001: step 2156, loss 0.000558835, acc 1\n",
      "2017-03-13T14:19:07.461098: step 2157, loss 0.000925579, acc 1\n",
      "2017-03-13T14:19:07.614208: step 2158, loss 0.000268516, acc 1\n",
      "2017-03-13T14:19:07.772319: step 2159, loss 0.000212394, acc 1\n",
      "2017-03-13T14:19:07.819352: step 2160, loss 0.000357134, acc 1\n",
      "2017-03-13T14:19:07.970459: step 2161, loss 0.00185706, acc 1\n",
      "2017-03-13T14:19:08.112559: step 2162, loss 0.00220152, acc 1\n",
      "2017-03-13T14:19:08.254661: step 2163, loss 0.000482954, acc 1\n",
      "2017-03-13T14:19:08.406768: step 2164, loss 0.00222163, acc 1\n",
      "2017-03-13T14:19:08.556875: step 2165, loss 0.000314649, acc 1\n",
      "2017-03-13T14:19:08.697975: step 2166, loss 0.000151809, acc 1\n",
      "2017-03-13T14:19:08.840080: step 2167, loss 0.000446928, acc 1\n",
      "2017-03-13T14:19:08.993185: step 2168, loss 0.000901098, acc 1\n",
      "2017-03-13T14:19:09.138287: step 2169, loss 0.000534255, acc 1\n",
      "2017-03-13T14:19:09.283390: step 2170, loss 0.00404418, acc 1\n",
      "2017-03-13T14:19:09.429494: step 2171, loss 6.24054e-05, acc 1\n",
      "2017-03-13T14:19:09.570595: step 2172, loss 0.000721325, acc 1\n",
      "2017-03-13T14:19:09.702689: step 2173, loss 0.000145193, acc 1\n",
      "2017-03-13T14:19:09.845790: step 2174, loss 0.000412841, acc 1\n",
      "2017-03-13T14:19:09.884818: step 2175, loss 8.17941e-05, acc 1\n",
      "2017-03-13T14:19:10.020914: step 2176, loss 0.000126273, acc 1\n",
      "2017-03-13T14:19:10.166017: step 2177, loss 0.00016818, acc 1\n",
      "2017-03-13T14:19:10.309118: step 2178, loss 0.000503726, acc 1\n",
      "2017-03-13T14:19:10.451219: step 2179, loss 0.000655622, acc 1\n",
      "2017-03-13T14:19:10.596322: step 2180, loss 0.00213708, acc 1\n",
      "2017-03-13T14:19:10.780453: step 2181, loss 0.000397041, acc 1\n",
      "2017-03-13T14:19:10.925557: step 2182, loss 0.000458714, acc 1\n",
      "2017-03-13T14:19:11.077663: step 2183, loss 0.000583251, acc 1\n",
      "2017-03-13T14:19:11.228771: step 2184, loss 0.000747257, acc 1\n",
      "2017-03-13T14:19:11.386883: step 2185, loss 0.00271885, acc 1\n",
      "2017-03-13T14:19:11.534988: step 2186, loss 0.000624404, acc 1\n",
      "2017-03-13T14:19:11.678090: step 2187, loss 8.02532e-05, acc 1\n",
      "2017-03-13T14:19:11.824193: step 2188, loss 0.00147338, acc 1\n",
      "2017-03-13T14:19:11.984311: step 2189, loss 0.000954665, acc 1\n",
      "2017-03-13T14:19:12.032340: step 2190, loss 1.99675e-06, acc 1\n",
      "2017-03-13T14:19:12.165435: step 2191, loss 0.000159158, acc 1\n",
      "2017-03-13T14:19:12.299530: step 2192, loss 0.00532805, acc 1\n",
      "2017-03-13T14:19:12.446634: step 2193, loss 0.000377782, acc 1\n",
      "2017-03-13T14:19:12.579729: step 2194, loss 0.000191836, acc 1\n",
      "2017-03-13T14:19:12.723832: step 2195, loss 0.000478842, acc 1\n",
      "2017-03-13T14:19:12.864931: step 2196, loss 0.0116048, acc 0.984375\n",
      "2017-03-13T14:19:13.017039: step 2197, loss 0.00470628, acc 1\n",
      "2017-03-13T14:19:13.157139: step 2198, loss 0.000430556, acc 1\n",
      "2017-03-13T14:19:13.295236: step 2199, loss 0.000770273, acc 1\n",
      "2017-03-13T14:19:13.432334: step 2200, loss 0.000256931, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:19:13.511393: step 2200, loss 0.510984, acc 0.8\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2200\n",
      "\n",
      "2017-03-13T14:19:14.174915: step 2201, loss 0.00122155, acc 1\n",
      "2017-03-13T14:19:14.306008: step 2202, loss 0.000142522, acc 1\n",
      "2017-03-13T14:19:14.437101: step 2203, loss 0.024216, acc 0.984375\n",
      "2017-03-13T14:19:14.591210: step 2204, loss 0.0014044, acc 1\n",
      "2017-03-13T14:19:14.632239: step 2205, loss 8.44479e-05, acc 1\n",
      "2017-03-13T14:19:14.864404: step 2206, loss 0.000685626, acc 1\n",
      "2017-03-13T14:19:15.077555: step 2207, loss 0.000500611, acc 1\n",
      "2017-03-13T14:19:15.227661: step 2208, loss 0.000928597, acc 1\n",
      "2017-03-13T14:19:15.360756: step 2209, loss 0.00111996, acc 1\n",
      "2017-03-13T14:19:15.486845: step 2210, loss 0.00027035, acc 1\n",
      "2017-03-13T14:19:15.622942: step 2211, loss 0.000824009, acc 1\n",
      "2017-03-13T14:19:15.769046: step 2212, loss 0.000285759, acc 1\n",
      "2017-03-13T14:19:15.904144: step 2213, loss 0.000690654, acc 1\n",
      "2017-03-13T14:19:16.032042: step 2214, loss 0.000286057, acc 1\n",
      "2017-03-13T14:19:16.152049: step 2215, loss 0.000872955, acc 1\n",
      "2017-03-13T14:19:16.272058: step 2216, loss 0.000578461, acc 1\n",
      "2017-03-13T14:19:16.387857: step 2217, loss 0.000473766, acc 1\n",
      "2017-03-13T14:19:16.519061: step 2218, loss 0.000699586, acc 1\n",
      "2017-03-13T14:19:16.659159: step 2219, loss 0.00122552, acc 1\n",
      "2017-03-13T14:19:16.692183: step 2220, loss 9.41372e-05, acc 1\n",
      "2017-03-13T14:19:16.821275: step 2221, loss 0.000687666, acc 1\n",
      "2017-03-13T14:19:16.963376: step 2222, loss 0.000608542, acc 1\n",
      "2017-03-13T14:19:17.097470: step 2223, loss 0.00081033, acc 1\n",
      "2017-03-13T14:19:17.258586: step 2224, loss 0.000367991, acc 1\n",
      "2017-03-13T14:19:17.395682: step 2225, loss 8.44634e-05, acc 1\n",
      "2017-03-13T14:19:17.539784: step 2226, loss 0.0249605, acc 0.984375\n",
      "2017-03-13T14:19:17.684887: step 2227, loss 0.021267, acc 0.984375\n",
      "2017-03-13T14:19:17.816981: step 2228, loss 0.000105278, acc 1\n",
      "2017-03-13T14:19:17.977096: step 2229, loss 0.000676951, acc 1\n",
      "2017-03-13T14:19:18.117401: step 2230, loss 0.000858645, acc 1\n",
      "2017-03-13T14:19:18.237410: step 2231, loss 0.00239282, acc 1\n",
      "2017-03-13T14:19:18.357418: step 2232, loss 0.000199286, acc 1\n",
      "2017-03-13T14:19:18.476763: step 2233, loss 0.000328151, acc 1\n",
      "2017-03-13T14:19:18.592100: step 2234, loss 0.00318528, acc 1\n",
      "2017-03-13T14:19:18.630258: step 2235, loss 0.000140295, acc 1\n",
      "2017-03-13T14:19:18.759350: step 2236, loss 0.000325778, acc 1\n",
      "2017-03-13T14:19:18.905453: step 2237, loss 0.000458698, acc 1\n",
      "2017-03-13T14:19:19.038547: step 2238, loss 0.000228974, acc 1\n",
      "2017-03-13T14:19:19.178647: step 2239, loss 0.000358004, acc 1\n",
      "2017-03-13T14:19:19.321749: step 2240, loss 0.0012784, acc 1\n",
      "2017-03-13T14:19:19.461848: step 2241, loss 0.00253568, acc 1\n",
      "2017-03-13T14:19:19.598945: step 2242, loss 0.000122025, acc 1\n",
      "2017-03-13T14:19:19.734041: step 2243, loss 0.000275231, acc 1\n",
      "2017-03-13T14:19:19.859130: step 2244, loss 8.76305e-05, acc 1\n",
      "2017-03-13T14:19:19.993226: step 2245, loss 0.000813852, acc 1\n",
      "2017-03-13T14:19:20.152340: step 2246, loss 0.00427733, acc 1\n",
      "2017-03-13T14:19:20.278413: step 2247, loss 0.000188203, acc 1\n",
      "2017-03-13T14:19:20.398421: step 2248, loss 0.000155384, acc 1\n",
      "2017-03-13T14:19:20.518430: step 2249, loss 0.000276724, acc 1\n",
      "2017-03-13T14:19:20.548432: step 2250, loss 0.000282952, acc 1\n",
      "2017-03-13T14:19:20.688175: step 2251, loss 0.000284931, acc 1\n",
      "2017-03-13T14:19:20.820269: step 2252, loss 0.00048735, acc 1\n",
      "2017-03-13T14:19:20.960368: step 2253, loss 0.00106734, acc 1\n",
      "2017-03-13T14:19:21.091461: step 2254, loss 0.000308315, acc 1\n",
      "2017-03-13T14:19:21.229559: step 2255, loss 0.00048075, acc 1\n",
      "2017-03-13T14:19:21.387671: step 2256, loss 0.00055371, acc 1\n",
      "2017-03-13T14:19:21.530773: step 2257, loss 0.000503898, acc 1\n",
      "2017-03-13T14:19:21.657862: step 2258, loss 0.0319891, acc 0.984375\n",
      "2017-03-13T14:19:21.791958: step 2259, loss 0.000368584, acc 1\n",
      "2017-03-13T14:19:21.933058: step 2260, loss 0.000542403, acc 1\n",
      "2017-03-13T14:19:22.060148: step 2261, loss 0.000906022, acc 1\n",
      "2017-03-13T14:19:22.191241: step 2262, loss 0.000172481, acc 1\n",
      "2017-03-13T14:19:22.325336: step 2263, loss 0.000555571, acc 1\n",
      "2017-03-13T14:19:22.453428: step 2264, loss 0.000341269, acc 1\n",
      "2017-03-13T14:19:22.485451: step 2265, loss 1.32618e-05, acc 1\n",
      "2017-03-13T14:19:22.617546: step 2266, loss 0.000372604, acc 1\n",
      "2017-03-13T14:19:22.778830: step 2267, loss 0.000275303, acc 1\n",
      "2017-03-13T14:19:22.898838: step 2268, loss 0.000138208, acc 1\n",
      "2017-03-13T14:19:23.008846: step 2269, loss 0.00079946, acc 1\n",
      "2017-03-13T14:19:23.147430: step 2270, loss 0.021105, acc 0.984375\n",
      "2017-03-13T14:19:23.291532: step 2271, loss 0.000100057, acc 1\n",
      "2017-03-13T14:19:23.422625: step 2272, loss 0.0251938, acc 0.984375\n",
      "2017-03-13T14:19:23.551717: step 2273, loss 0.0248858, acc 0.984375\n",
      "2017-03-13T14:19:23.687814: step 2274, loss 0.000543098, acc 1\n",
      "2017-03-13T14:19:23.829915: step 2275, loss 0.000370223, acc 1\n",
      "2017-03-13T14:19:23.961007: step 2276, loss 0.00087902, acc 1\n",
      "2017-03-13T14:19:24.100106: step 2277, loss 0.0637598, acc 0.984375\n",
      "2017-03-13T14:19:24.235202: step 2278, loss 0.000836714, acc 1\n",
      "2017-03-13T14:19:24.368296: step 2279, loss 0.00219719, acc 1\n",
      "2017-03-13T14:19:24.409326: step 2280, loss 0.000145454, acc 1\n",
      "2017-03-13T14:19:24.576444: step 2281, loss 0.00203786, acc 1\n",
      "2017-03-13T14:19:24.714542: step 2282, loss 0.000870406, acc 1\n",
      "2017-03-13T14:19:24.846635: step 2283, loss 0.000657548, acc 1\n",
      "2017-03-13T14:19:24.977729: step 2284, loss 0.000246285, acc 1\n",
      "2017-03-13T14:19:25.114826: step 2285, loss 0.00104717, acc 1\n",
      "2017-03-13T14:19:25.266933: step 2286, loss 0.000255057, acc 1\n",
      "2017-03-13T14:19:25.401028: step 2287, loss 0.00124516, acc 1\n",
      "2017-03-13T14:19:25.533122: step 2288, loss 0.0053528, acc 1\n",
      "2017-03-13T14:19:25.668218: step 2289, loss 0.0123998, acc 1\n",
      "2017-03-13T14:19:25.801312: step 2290, loss 0.000875578, acc 1\n",
      "2017-03-13T14:19:25.974436: step 2291, loss 0.000453064, acc 1\n",
      "2017-03-13T14:19:26.115535: step 2292, loss 0.00044551, acc 1\n",
      "2017-03-13T14:19:26.251632: step 2293, loss 0.000458403, acc 1\n",
      "2017-03-13T14:19:26.382725: step 2294, loss 0.00110265, acc 1\n",
      "2017-03-13T14:19:26.415749: step 2295, loss 0.0023583, acc 1\n",
      "2017-03-13T14:19:26.574862: step 2296, loss 0.000304332, acc 1\n",
      "2017-03-13T14:19:26.719965: step 2297, loss 0.000690711, acc 1\n",
      "2017-03-13T14:19:26.856061: step 2298, loss 0.00229327, acc 1\n",
      "2017-03-13T14:19:26.984152: step 2299, loss 0.00146909, acc 1\n",
      "2017-03-13T14:19:27.118247: step 2300, loss 0.000538173, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:19:27.194301: step 2300, loss 0.586895, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2300\n",
      "\n",
      "2017-03-13T14:19:30.068465: step 2301, loss 0.000695826, acc 1\n",
      "2017-03-13T14:19:30.198093: step 2302, loss 0.000765453, acc 1\n",
      "2017-03-13T14:19:30.337191: step 2303, loss 0.000989347, acc 1\n",
      "2017-03-13T14:19:30.475289: step 2304, loss 0.000476265, acc 1\n",
      "2017-03-13T14:19:30.615389: step 2305, loss 0.000584589, acc 1\n",
      "2017-03-13T14:19:30.751485: step 2306, loss 0.0010254, acc 1\n",
      "2017-03-13T14:19:30.889584: step 2307, loss 0.000472934, acc 1\n",
      "2017-03-13T14:19:31.079720: step 2308, loss 0.00109167, acc 1\n",
      "2017-03-13T14:19:31.242834: step 2309, loss 0.000184402, acc 1\n",
      "2017-03-13T14:19:31.278860: step 2310, loss 9.11796e-05, acc 1\n",
      "2017-03-13T14:19:31.402947: step 2311, loss 0.000132929, acc 1\n",
      "2017-03-13T14:19:31.537043: step 2312, loss 0.000536184, acc 1\n",
      "2017-03-13T14:19:31.671138: step 2313, loss 0.0181647, acc 1\n",
      "2017-03-13T14:19:31.840260: step 2314, loss 0.024536, acc 0.984375\n",
      "2017-03-13T14:19:31.988364: step 2315, loss 0.000683339, acc 1\n",
      "2017-03-13T14:19:32.126461: step 2316, loss 0.00101274, acc 1\n",
      "2017-03-13T14:19:32.255552: step 2317, loss 0.00228807, acc 1\n",
      "2017-03-13T14:19:32.400655: step 2318, loss 0.000965097, acc 1\n",
      "2017-03-13T14:19:32.558768: step 2319, loss 0.00158882, acc 1\n",
      "2017-03-13T14:19:32.704871: step 2320, loss 0.000394101, acc 1\n",
      "2017-03-13T14:19:32.838966: step 2321, loss 0.013972, acc 0.984375\n",
      "2017-03-13T14:19:32.988072: step 2322, loss 0.000401618, acc 1\n",
      "2017-03-13T14:19:33.130173: step 2323, loss 0.00106795, acc 1\n",
      "2017-03-13T14:19:33.266269: step 2324, loss 0.000126499, acc 1\n",
      "2017-03-13T14:19:33.313303: step 2325, loss 3.48686e-06, acc 1\n",
      "2017-03-13T14:19:33.473417: step 2326, loss 0.00434169, acc 1\n",
      "2017-03-13T14:19:33.647540: step 2327, loss 0.0022064, acc 1\n",
      "2017-03-13T14:19:33.807654: step 2328, loss 0.000491491, acc 1\n",
      "2017-03-13T14:19:33.992785: step 2329, loss 0.000680546, acc 1\n",
      "2017-03-13T14:19:34.153899: step 2330, loss 0.000203608, acc 1\n",
      "2017-03-13T14:19:34.296000: step 2331, loss 0.000387751, acc 1\n",
      "2017-03-13T14:19:34.440102: step 2332, loss 0.0056969, acc 1\n",
      "2017-03-13T14:19:34.579201: step 2333, loss 0.00101262, acc 1\n",
      "2017-03-13T14:19:34.745318: step 2334, loss 0.00388536, acc 1\n",
      "2017-03-13T14:19:34.885418: step 2335, loss 3.78704e-05, acc 1\n",
      "2017-03-13T14:19:35.020514: step 2336, loss 0.000848679, acc 1\n",
      "2017-03-13T14:19:35.160613: step 2337, loss 0.000673625, acc 1\n",
      "2017-03-13T14:19:35.294708: step 2338, loss 0.00101822, acc 1\n",
      "2017-03-13T14:19:35.426802: step 2339, loss 0.000437389, acc 1\n",
      "2017-03-13T14:19:35.460826: step 2340, loss 4.44033e-05, acc 1\n",
      "2017-03-13T14:19:35.588917: step 2341, loss 0.000954389, acc 1\n",
      "2017-03-13T14:19:35.725013: step 2342, loss 0.000636466, acc 1\n",
      "2017-03-13T14:19:35.870116: step 2343, loss 0.000931659, acc 1\n",
      "2017-03-13T14:19:36.023226: step 2344, loss 0.000362003, acc 1\n",
      "2017-03-13T14:19:36.181337: step 2345, loss 0.00338008, acc 1\n",
      "2017-03-13T14:19:36.319435: step 2346, loss 0.000231224, acc 1\n",
      "2017-03-13T14:19:36.450528: step 2347, loss 0.000141866, acc 1\n",
      "2017-03-13T14:19:36.577619: step 2348, loss 0.000582867, acc 1\n",
      "2017-03-13T14:19:36.711714: step 2349, loss 0.000706738, acc 1\n",
      "2017-03-13T14:19:36.849812: step 2350, loss 0.000195152, acc 1\n",
      "2017-03-13T14:19:36.983907: step 2351, loss 0.00715125, acc 1\n",
      "2017-03-13T14:19:37.122004: step 2352, loss 0.0065733, acc 1\n",
      "2017-03-13T14:19:37.267108: step 2353, loss 0.000805569, acc 1\n",
      "2017-03-13T14:19:37.399202: step 2354, loss 0.000555344, acc 1\n",
      "2017-03-13T14:19:37.454241: step 2355, loss 8.55322e-06, acc 1\n",
      "2017-03-13T14:19:37.617356: step 2356, loss 0.000163035, acc 1\n",
      "2017-03-13T14:19:37.762459: step 2357, loss 0.0447984, acc 0.984375\n",
      "2017-03-13T14:19:37.912566: step 2358, loss 0.00035438, acc 1\n",
      "2017-03-13T14:19:38.038655: step 2359, loss 0.000343043, acc 1\n",
      "2017-03-13T14:19:38.170748: step 2360, loss 0.000830511, acc 1\n",
      "2017-03-13T14:19:38.298839: step 2361, loss 0.000275527, acc 1\n",
      "2017-03-13T14:19:38.434936: step 2362, loss 0.000321844, acc 1\n",
      "2017-03-13T14:19:38.562026: step 2363, loss 8.1687e-05, acc 1\n",
      "2017-03-13T14:19:38.707129: step 2364, loss 0.000281831, acc 1\n",
      "2017-03-13T14:19:38.871247: step 2365, loss 0.000258727, acc 1\n",
      "2017-03-13T14:19:39.032360: step 2366, loss 0.00276364, acc 1\n",
      "2017-03-13T14:19:39.173460: step 2367, loss 0.000201635, acc 1\n",
      "2017-03-13T14:19:39.323567: step 2368, loss 0.000134483, acc 1\n",
      "2017-03-13T14:19:39.452658: step 2369, loss 0.000439344, acc 1\n",
      "2017-03-13T14:19:39.488683: step 2370, loss 0.000277768, acc 1\n",
      "2017-03-13T14:19:39.617775: step 2371, loss 0.00638824, acc 1\n",
      "2017-03-13T14:19:39.755873: step 2372, loss 0.0229006, acc 0.984375\n",
      "2017-03-13T14:19:39.888967: step 2373, loss 0.000250761, acc 1\n",
      "2017-03-13T14:19:40.024064: step 2374, loss 0.000147144, acc 1\n",
      "2017-03-13T14:19:40.155157: step 2375, loss 0.000685162, acc 1\n",
      "2017-03-13T14:19:40.306264: step 2376, loss 0.000273524, acc 1\n",
      "2017-03-13T14:19:40.502406: step 2377, loss 0.00197483, acc 1\n",
      "2017-03-13T14:19:40.661516: step 2378, loss 0.000426456, acc 1\n",
      "2017-03-13T14:19:40.807620: step 2379, loss 0.000435943, acc 1\n",
      "2017-03-13T14:19:40.947718: step 2380, loss 0.00066133, acc 1\n",
      "2017-03-13T14:19:41.084816: step 2381, loss 0.00138531, acc 1\n",
      "2017-03-13T14:19:41.218911: step 2382, loss 0.000353136, acc 1\n",
      "2017-03-13T14:19:41.351005: step 2383, loss 0.000368953, acc 1\n",
      "2017-03-13T14:19:41.493106: step 2384, loss 0.000568886, acc 1\n",
      "2017-03-13T14:19:41.535135: step 2385, loss 0.0126986, acc 1\n",
      "2017-03-13T14:19:41.678237: step 2386, loss 0.000123659, acc 1\n",
      "2017-03-13T14:19:41.809330: step 2387, loss 0.000264276, acc 1\n",
      "2017-03-13T14:19:41.940423: step 2388, loss 0.00124713, acc 1\n",
      "2017-03-13T14:19:42.075519: step 2389, loss 0.000209371, acc 1\n",
      "2017-03-13T14:19:42.208613: step 2390, loss 0.000492805, acc 1\n",
      "2017-03-13T14:19:42.343709: step 2391, loss 0.000689635, acc 1\n",
      "2017-03-13T14:19:42.473801: step 2392, loss 7.06736e-05, acc 1\n",
      "2017-03-13T14:19:42.641922: step 2393, loss 0.000516597, acc 1\n",
      "2017-03-13T14:19:42.789025: step 2394, loss 0.00226952, acc 1\n",
      "2017-03-13T14:19:42.934128: step 2395, loss 0.00145636, acc 1\n",
      "2017-03-13T14:19:43.060217: step 2396, loss 0.000937188, acc 1\n",
      "2017-03-13T14:19:43.194313: step 2397, loss 0.000130132, acc 1\n",
      "2017-03-13T14:19:43.324405: step 2398, loss 0.00058769, acc 1\n",
      "2017-03-13T14:19:43.453496: step 2399, loss 0.000506, acc 1\n",
      "2017-03-13T14:19:43.486519: step 2400, loss 3.60606e-06, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:19:43.561573: step 2400, loss 0.565454, acc 0.84\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2400\n",
      "\n",
      "2017-03-13T14:19:44.399167: step 2401, loss 0.000586551, acc 1\n",
      "2017-03-13T14:19:44.577294: step 2402, loss 0.000159438, acc 1\n",
      "2017-03-13T14:19:44.809462: step 2403, loss 0.000404445, acc 1\n",
      "2017-03-13T14:19:45.012604: step 2404, loss 0.00039602, acc 1\n",
      "2017-03-13T14:19:45.184725: step 2405, loss 0.000484743, acc 1\n",
      "2017-03-13T14:19:45.337833: step 2406, loss 0.000232131, acc 1\n",
      "2017-03-13T14:19:45.479934: step 2407, loss 0.000451154, acc 1\n",
      "2017-03-13T14:19:45.614030: step 2408, loss 0.0158379, acc 0.984375\n",
      "2017-03-13T14:19:45.750125: step 2409, loss 0.0098326, acc 1\n",
      "2017-03-13T14:19:45.880218: step 2410, loss 0.0012248, acc 1\n",
      "2017-03-13T14:19:46.018315: step 2411, loss 0.00175828, acc 1\n",
      "2017-03-13T14:19:46.157415: step 2412, loss 0.000360107, acc 1\n",
      "2017-03-13T14:19:46.298515: step 2413, loss 0.000840116, acc 1\n",
      "2017-03-13T14:19:46.442617: step 2414, loss 0.000719645, acc 1\n",
      "2017-03-13T14:19:46.490651: step 2415, loss 8.01327e-05, acc 1\n",
      "2017-03-13T14:19:46.613738: step 2416, loss 0.00168583, acc 1\n",
      "2017-03-13T14:19:46.742830: step 2417, loss 0.00638584, acc 1\n",
      "2017-03-13T14:19:46.876925: step 2418, loss 0.000507418, acc 1\n",
      "2017-03-13T14:19:47.009018: step 2419, loss 0.000265596, acc 1\n",
      "2017-03-13T14:19:47.144114: step 2420, loss 0.00035041, acc 1\n",
      "2017-03-13T14:19:47.276208: step 2421, loss 0.000222912, acc 1\n",
      "2017-03-13T14:19:47.409302: step 2422, loss 0.000579288, acc 1\n",
      "2017-03-13T14:19:47.537393: step 2423, loss 0.000494146, acc 1\n",
      "2017-03-13T14:19:47.678494: step 2424, loss 0.000739217, acc 1\n",
      "2017-03-13T14:19:47.841609: step 2425, loss 0.000758454, acc 1\n",
      "2017-03-13T14:19:47.987713: step 2426, loss 0.000371411, acc 1\n",
      "2017-03-13T14:19:48.117805: step 2427, loss 0.000475691, acc 1\n",
      "2017-03-13T14:19:48.250900: step 2428, loss 0.00109383, acc 1\n",
      "2017-03-13T14:19:48.381992: step 2429, loss 0.000410119, acc 1\n",
      "2017-03-13T14:19:48.414015: step 2430, loss 0.000307311, acc 1\n",
      "2017-03-13T14:19:48.551113: step 2431, loss 0.00218877, acc 1\n",
      "2017-03-13T14:19:48.682205: step 2432, loss 0.000516683, acc 1\n",
      "2017-03-13T14:19:48.817302: step 2433, loss 0.00100011, acc 1\n",
      "2017-03-13T14:19:48.950396: step 2434, loss 5.44279e-05, acc 1\n",
      "2017-03-13T14:19:49.086492: step 2435, loss 0.000274445, acc 1\n",
      "2017-03-13T14:19:49.217585: step 2436, loss 0.000234541, acc 1\n",
      "2017-03-13T14:19:49.389708: step 2437, loss 0.000615435, acc 1\n",
      "2017-03-13T14:19:49.528806: step 2438, loss 0.000826824, acc 1\n",
      "2017-03-13T14:19:49.658898: step 2439, loss 0.000432563, acc 1\n",
      "2017-03-13T14:19:49.789992: step 2440, loss 0.000305143, acc 1\n",
      "2017-03-13T14:19:49.929090: step 2441, loss 0.000360489, acc 1\n",
      "2017-03-13T14:19:50.068189: step 2442, loss 0.000338154, acc 1\n",
      "2017-03-13T14:19:50.204285: step 2443, loss 0.000712284, acc 1\n",
      "2017-03-13T14:19:50.337380: step 2444, loss 0.00021308, acc 1\n",
      "2017-03-13T14:19:50.375406: step 2445, loss 0.000107356, acc 1\n",
      "2017-03-13T14:19:50.510503: step 2446, loss 0.000174831, acc 1\n",
      "2017-03-13T14:19:50.655607: step 2447, loss 0.000417931, acc 1\n",
      "2017-03-13T14:19:50.822725: step 2448, loss 0.000277507, acc 1\n",
      "2017-03-13T14:19:50.962824: step 2449, loss 7.22083e-05, acc 1\n",
      "2017-03-13T14:19:51.096918: step 2450, loss 0.000490636, acc 1\n",
      "2017-03-13T14:19:51.233016: step 2451, loss 7.64888e-05, acc 1\n",
      "2017-03-13T14:19:51.360106: step 2452, loss 0.00131333, acc 1\n",
      "2017-03-13T14:19:51.495201: step 2453, loss 0.000134922, acc 1\n",
      "2017-03-13T14:19:51.635300: step 2454, loss 0.000373261, acc 1\n",
      "2017-03-13T14:19:51.770396: step 2455, loss 9.57997e-05, acc 1\n",
      "2017-03-13T14:19:51.899488: step 2456, loss 0.000342418, acc 1\n",
      "2017-03-13T14:19:52.035585: step 2457, loss 0.000168499, acc 1\n",
      "2017-03-13T14:19:52.198701: step 2458, loss 0.000101161, acc 1\n",
      "2017-03-13T14:19:52.342802: step 2459, loss 0.00034253, acc 1\n",
      "2017-03-13T14:19:52.374825: step 2460, loss 0.000989042, acc 1\n",
      "2017-03-13T14:19:52.508920: step 2461, loss 0.00349184, acc 1\n",
      "2017-03-13T14:19:52.646017: step 2462, loss 0.000553001, acc 1\n",
      "2017-03-13T14:19:52.784115: step 2463, loss 0.000119817, acc 1\n",
      "2017-03-13T14:19:52.924215: step 2464, loss 0.00035568, acc 1\n",
      "2017-03-13T14:19:53.054308: step 2465, loss 0.00129953, acc 1\n",
      "2017-03-13T14:19:53.188402: step 2466, loss 0.000153407, acc 1\n",
      "2017-03-13T14:19:53.323498: step 2467, loss 0.000276651, acc 1\n",
      "2017-03-13T14:19:53.456593: step 2468, loss 0.00104435, acc 1\n",
      "2017-03-13T14:19:53.620709: step 2469, loss 0.000326474, acc 1\n",
      "2017-03-13T14:19:53.769814: step 2470, loss 0.000407448, acc 1\n",
      "2017-03-13T14:19:53.900908: step 2471, loss 0.000474306, acc 1\n",
      "2017-03-13T14:19:54.039006: step 2472, loss 0.000217785, acc 1\n",
      "2017-03-13T14:19:54.181106: step 2473, loss 0.000484933, acc 1\n",
      "2017-03-13T14:19:54.311210: step 2474, loss 0.000262374, acc 1\n",
      "2017-03-13T14:19:54.344222: step 2475, loss 0.000277289, acc 1\n",
      "2017-03-13T14:19:54.471312: step 2476, loss 0.000712517, acc 1\n",
      "2017-03-13T14:19:54.607410: step 2477, loss 8.48239e-05, acc 1\n",
      "2017-03-13T14:19:54.746507: step 2478, loss 5.34582e-05, acc 1\n",
      "2017-03-13T14:19:54.905621: step 2479, loss 0.000401813, acc 1\n",
      "2017-03-13T14:19:55.055727: step 2480, loss 0.000177, acc 1\n",
      "2017-03-13T14:19:55.191823: step 2481, loss 0.000809845, acc 1\n",
      "2017-03-13T14:19:55.321916: step 2482, loss 0.000234626, acc 1\n",
      "2017-03-13T14:19:55.457011: step 2483, loss 0.000186119, acc 1\n",
      "2017-03-13T14:19:55.589106: step 2484, loss 0.000244843, acc 1\n",
      "2017-03-13T14:19:55.731206: step 2485, loss 0.000778042, acc 1\n",
      "2017-03-13T14:19:55.875308: step 2486, loss 0.00067538, acc 1\n",
      "2017-03-13T14:19:56.006401: step 2487, loss 0.000572923, acc 1\n",
      "2017-03-13T14:19:56.140497: step 2488, loss 0.000395348, acc 1\n",
      "2017-03-13T14:19:56.311618: step 2489, loss 0.000818054, acc 1\n",
      "2017-03-13T14:19:56.360653: step 2490, loss 3.49567e-05, acc 1\n",
      "2017-03-13T14:19:56.491746: step 2491, loss 0.000337161, acc 1\n",
      "2017-03-13T14:19:56.628844: step 2492, loss 0.000180524, acc 1\n",
      "2017-03-13T14:19:56.762939: step 2493, loss 0.000459237, acc 1\n",
      "2017-03-13T14:19:56.896032: step 2494, loss 0.000382605, acc 1\n",
      "2017-03-13T14:19:57.022122: step 2495, loss 0.000925668, acc 1\n",
      "2017-03-13T14:19:57.161220: step 2496, loss 0.000286266, acc 1\n",
      "2017-03-13T14:19:57.295315: step 2497, loss 0.000426207, acc 1\n",
      "2017-03-13T14:19:57.428410: step 2498, loss 0.000498333, acc 1\n",
      "2017-03-13T14:19:57.566508: step 2499, loss 0.000226676, acc 1\n",
      "2017-03-13T14:19:57.734628: step 2500, loss 0.000416679, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:19:57.817686: step 2500, loss 0.523066, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2500\n",
      "\n",
      "2017-03-13T14:19:58.499538: step 2501, loss 0.000256657, acc 1\n",
      "2017-03-13T14:19:58.622097: step 2502, loss 0.00104544, acc 1\n",
      "2017-03-13T14:19:58.758193: step 2503, loss 0.000336949, acc 1\n",
      "2017-03-13T14:19:58.913303: step 2504, loss 0.00216465, acc 1\n",
      "2017-03-13T14:19:58.955334: step 2505, loss 0.000935009, acc 1\n",
      "2017-03-13T14:19:59.169487: step 2506, loss 0.000151598, acc 1\n",
      "2017-03-13T14:19:59.359620: step 2507, loss 0.000149687, acc 1\n",
      "2017-03-13T14:19:59.512729: step 2508, loss 0.000603589, acc 1\n",
      "2017-03-13T14:19:59.660834: step 2509, loss 0.000568431, acc 1\n",
      "2017-03-13T14:19:59.813944: step 2510, loss 0.00021663, acc 1\n",
      "2017-03-13T14:19:59.955042: step 2511, loss 0.00041751, acc 1\n",
      "2017-03-13T14:20:00.085134: step 2512, loss 0.00120622, acc 1\n",
      "2017-03-13T14:20:00.243247: step 2513, loss 0.000121727, acc 1\n",
      "2017-03-13T14:20:00.394354: step 2514, loss 0.000420615, acc 1\n",
      "2017-03-13T14:20:00.520444: step 2515, loss 0.00105257, acc 1\n",
      "2017-03-13T14:20:00.652537: step 2516, loss 0.000582592, acc 1\n",
      "2017-03-13T14:20:00.784631: step 2517, loss 5.18467e-05, acc 1\n",
      "2017-03-13T14:20:00.919728: step 2518, loss 0.000509137, acc 1\n",
      "2017-03-13T14:20:01.080842: step 2519, loss 0.00514543, acc 1\n",
      "2017-03-13T14:20:01.126875: step 2520, loss 0.000707234, acc 1\n",
      "2017-03-13T14:20:01.267974: step 2521, loss 0.000202464, acc 1\n",
      "2017-03-13T14:20:01.415078: step 2522, loss 0.000506584, acc 1\n",
      "2017-03-13T14:20:01.543169: step 2523, loss 0.000439486, acc 1\n",
      "2017-03-13T14:20:01.680267: step 2524, loss 0.00098615, acc 1\n",
      "2017-03-13T14:20:01.848388: step 2525, loss 0.000349324, acc 1\n",
      "2017-03-13T14:20:01.987484: step 2526, loss 0.000607695, acc 1\n",
      "2017-03-13T14:20:02.117577: step 2527, loss 0.00633957, acc 1\n",
      "2017-03-13T14:20:02.260679: step 2528, loss 0.000111532, acc 1\n",
      "2017-03-13T14:20:02.402780: step 2529, loss 0.000179883, acc 1\n",
      "2017-03-13T14:20:02.570901: step 2530, loss 0.000207715, acc 1\n",
      "2017-03-13T14:20:02.715000: step 2531, loss 0.000576843, acc 1\n",
      "2017-03-13T14:20:02.855100: step 2532, loss 0.000163715, acc 1\n",
      "2017-03-13T14:20:02.983191: step 2533, loss 0.000393825, acc 1\n",
      "2017-03-13T14:20:03.110281: step 2534, loss 0.000111788, acc 1\n",
      "2017-03-13T14:20:03.147307: step 2535, loss 0.000110363, acc 1\n",
      "2017-03-13T14:20:03.279401: step 2536, loss 0.000144319, acc 1\n",
      "2017-03-13T14:20:03.443520: step 2537, loss 0.000159393, acc 1\n",
      "2017-03-13T14:20:03.585618: step 2538, loss 0.000877992, acc 1\n",
      "2017-03-13T14:20:03.715710: step 2539, loss 0.000691896, acc 1\n",
      "2017-03-13T14:20:03.848805: step 2540, loss 0.000828404, acc 1\n",
      "2017-03-13T14:20:03.987904: step 2541, loss 0.000294464, acc 1\n",
      "2017-03-13T14:20:04.149018: step 2542, loss 0.000566062, acc 1\n",
      "2017-03-13T14:20:04.294121: step 2543, loss 0.00027915, acc 1\n",
      "2017-03-13T14:20:04.422212: step 2544, loss 0.000353191, acc 1\n",
      "2017-03-13T14:20:04.560309: step 2545, loss 6.44206e-05, acc 1\n",
      "2017-03-13T14:20:04.700409: step 2546, loss 0.000424797, acc 1\n",
      "2017-03-13T14:20:04.863525: step 2547, loss 0.000208684, acc 1\n",
      "2017-03-13T14:20:05.002623: step 2548, loss 0.000704815, acc 1\n",
      "2017-03-13T14:20:05.130715: step 2549, loss 0.000938611, acc 1\n",
      "2017-03-13T14:20:05.161736: step 2550, loss 0.000506975, acc 1\n",
      "2017-03-13T14:20:05.288826: step 2551, loss 0.000213363, acc 1\n",
      "2017-03-13T14:20:05.428926: step 2552, loss 0.000212387, acc 1\n",
      "2017-03-13T14:20:05.603049: step 2553, loss 0.0051076, acc 1\n",
      "2017-03-13T14:20:05.742148: step 2554, loss 0.000183665, acc 1\n",
      "2017-03-13T14:20:05.874241: step 2555, loss 0.000229526, acc 1\n",
      "2017-03-13T14:20:06.005335: step 2556, loss 0.000519834, acc 1\n",
      "2017-03-13T14:20:06.136428: step 2557, loss 0.00806425, acc 1\n",
      "2017-03-13T14:20:06.275527: step 2558, loss 0.000134097, acc 1\n",
      "2017-03-13T14:20:06.418629: step 2559, loss 0.000221205, acc 1\n",
      "2017-03-13T14:20:06.556726: step 2560, loss 0.000723405, acc 1\n",
      "2017-03-13T14:20:06.693823: step 2561, loss 0.0153755, acc 0.984375\n",
      "2017-03-13T14:20:06.835924: step 2562, loss 0.0111581, acc 0.984375\n",
      "2017-03-13T14:20:06.975022: step 2563, loss 0.00149197, acc 1\n",
      "2017-03-13T14:20:07.143143: step 2564, loss 0.00139872, acc 1\n",
      "2017-03-13T14:20:07.184171: step 2565, loss 3.88294e-05, acc 1\n",
      "2017-03-13T14:20:07.313263: step 2566, loss 0.00242721, acc 1\n",
      "2017-03-13T14:20:07.442354: step 2567, loss 0.000290687, acc 1\n",
      "2017-03-13T14:20:07.580452: step 2568, loss 0.00283372, acc 1\n",
      "2017-03-13T14:20:07.718550: step 2569, loss 0.000171877, acc 1\n",
      "2017-03-13T14:20:07.887671: step 2570, loss 0.00180969, acc 1\n",
      "2017-03-13T14:20:08.028770: step 2571, loss 0.000678314, acc 1\n",
      "2017-03-13T14:20:08.159863: step 2572, loss 0.000299952, acc 1\n",
      "2017-03-13T14:20:08.287953: step 2573, loss 0.00024229, acc 1\n",
      "2017-03-13T14:20:08.420048: step 2574, loss 0.00013346, acc 1\n",
      "2017-03-13T14:20:08.576159: step 2575, loss 0.0131637, acc 0.984375\n",
      "2017-03-13T14:20:08.728266: step 2576, loss 0.000845857, acc 1\n",
      "2017-03-13T14:20:08.862361: step 2577, loss 0.00130194, acc 1\n",
      "2017-03-13T14:20:08.994455: step 2578, loss 8.3429e-05, acc 1\n",
      "2017-03-13T14:20:09.127550: step 2579, loss 0.000327643, acc 1\n",
      "2017-03-13T14:20:09.163575: step 2580, loss 0.000232632, acc 1\n",
      "2017-03-13T14:20:09.315683: step 2581, loss 0.000190178, acc 1\n",
      "2017-03-13T14:20:09.463788: step 2582, loss 0.000411238, acc 1\n",
      "2017-03-13T14:20:09.592880: step 2583, loss 0.000564378, acc 1\n",
      "2017-03-13T14:20:09.721971: step 2584, loss 0.000417348, acc 1\n",
      "2017-03-13T14:20:09.858068: step 2585, loss 0.000170238, acc 1\n",
      "2017-03-13T14:20:09.990162: step 2586, loss 0.000762008, acc 1\n",
      "2017-03-13T14:20:10.154279: step 2587, loss 0.000102112, acc 1\n",
      "2017-03-13T14:20:10.292376: step 2588, loss 0.000192649, acc 1\n",
      "2017-03-13T14:20:10.426471: step 2589, loss 0.000420579, acc 1\n",
      "2017-03-13T14:20:10.555562: step 2590, loss 0.00210375, acc 1\n",
      "2017-03-13T14:20:10.699665: step 2591, loss 0.000258115, acc 1\n",
      "2017-03-13T14:20:10.840765: step 2592, loss 0.000252051, acc 1\n",
      "2017-03-13T14:20:11.005884: step 2593, loss 0.020336, acc 0.984375\n",
      "2017-03-13T14:20:11.137975: step 2594, loss 0.000585074, acc 1\n",
      "2017-03-13T14:20:11.169999: step 2595, loss 1.1265e-05, acc 1\n",
      "2017-03-13T14:20:11.305095: step 2596, loss 9.5721e-05, acc 1\n",
      "2017-03-13T14:20:11.435186: step 2597, loss 0.000192958, acc 1\n",
      "2017-03-13T14:20:11.572284: step 2598, loss 0.000151825, acc 1\n",
      "2017-03-13T14:20:11.736402: step 2599, loss 0.000263283, acc 1\n",
      "2017-03-13T14:20:11.875499: step 2600, loss 0.00235671, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:20:11.945549: step 2600, loss 0.549465, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2600\n",
      "\n",
      "2017-03-13T14:20:14.177913: step 2601, loss 0.000598003, acc 1\n",
      "2017-03-13T14:20:14.306417: step 2602, loss 0.000220381, acc 1\n",
      "2017-03-13T14:20:14.447517: step 2603, loss 0.0011063, acc 1\n",
      "2017-03-13T14:20:14.578610: step 2604, loss 0.000338074, acc 1\n",
      "2017-03-13T14:20:14.704699: step 2605, loss 0.000384631, acc 1\n",
      "2017-03-13T14:20:14.841797: step 2606, loss 0.0221744, acc 0.984375\n",
      "2017-03-13T14:20:14.973890: step 2607, loss 9.7835e-05, acc 1\n",
      "2017-03-13T14:20:15.138007: step 2608, loss 0.000917648, acc 1\n",
      "2017-03-13T14:20:15.283109: step 2609, loss 0.000468719, acc 1\n",
      "2017-03-13T14:20:15.318135: step 2610, loss 0.00167327, acc 1\n",
      "2017-03-13T14:20:15.445224: step 2611, loss 0.000100996, acc 1\n",
      "2017-03-13T14:20:15.580320: step 2612, loss 0.000223354, acc 1\n",
      "2017-03-13T14:20:15.712414: step 2613, loss 9.5663e-05, acc 1\n",
      "2017-03-13T14:20:15.861522: step 2614, loss 0.000633939, acc 1\n",
      "2017-03-13T14:20:16.023635: step 2615, loss 0.000458131, acc 1\n",
      "2017-03-13T14:20:16.159731: step 2616, loss 0.000305592, acc 1\n",
      "2017-03-13T14:20:16.298830: step 2617, loss 0.00102875, acc 1\n",
      "2017-03-13T14:20:16.437929: step 2618, loss 0.000892143, acc 1\n",
      "2017-03-13T14:20:16.564018: step 2619, loss 0.000393673, acc 1\n",
      "2017-03-13T14:20:16.719129: step 2620, loss 0.000123573, acc 1\n",
      "2017-03-13T14:20:16.857226: step 2621, loss 0.000121689, acc 1\n",
      "2017-03-13T14:20:16.987319: step 2622, loss 0.000228511, acc 1\n",
      "2017-03-13T14:20:17.118412: step 2623, loss 0.000101584, acc 1\n",
      "2017-03-13T14:20:17.257510: step 2624, loss 0.00013898, acc 1\n",
      "2017-03-13T14:20:17.288533: step 2625, loss 0.000909073, acc 1\n",
      "2017-03-13T14:20:17.442642: step 2626, loss 0.00148026, acc 1\n",
      "2017-03-13T14:20:17.585743: step 2627, loss 8.57583e-05, acc 1\n",
      "2017-03-13T14:20:17.718837: step 2628, loss 0.000601342, acc 1\n",
      "2017-03-13T14:20:17.853934: step 2629, loss 5.78156e-05, acc 1\n",
      "2017-03-13T14:20:17.985026: step 2630, loss 0.00130886, acc 1\n",
      "2017-03-13T14:20:18.114118: step 2631, loss 4.99753e-05, acc 1\n",
      "2017-03-13T14:20:18.285240: step 2632, loss 5.04813e-05, acc 1\n",
      "2017-03-13T14:20:18.423337: step 2633, loss 0.000277334, acc 1\n",
      "2017-03-13T14:20:18.551428: step 2634, loss 0.000233078, acc 1\n",
      "2017-03-13T14:20:18.689526: step 2635, loss 0.000170802, acc 1\n",
      "2017-03-13T14:20:18.822620: step 2636, loss 0.000533383, acc 1\n",
      "2017-03-13T14:20:18.989739: step 2637, loss 0.000312067, acc 1\n",
      "2017-03-13T14:20:19.125835: step 2638, loss 0.00102317, acc 1\n",
      "2017-03-13T14:20:19.256929: step 2639, loss 0.00335058, acc 1\n",
      "2017-03-13T14:20:19.288952: step 2640, loss 6.92836e-05, acc 1\n",
      "2017-03-13T14:20:19.428050: step 2641, loss 0.00021652, acc 1\n",
      "2017-03-13T14:20:19.556141: step 2642, loss 0.000471741, acc 1\n",
      "2017-03-13T14:20:19.707249: step 2643, loss 0.000168334, acc 1\n",
      "2017-03-13T14:20:19.850349: step 2644, loss 0.000609439, acc 1\n",
      "2017-03-13T14:20:19.993451: step 2645, loss 0.000187522, acc 1\n",
      "2017-03-13T14:20:20.126545: step 2646, loss 0.000361103, acc 1\n",
      "2017-03-13T14:20:20.262642: step 2647, loss 0.000660032, acc 1\n",
      "2017-03-13T14:20:20.394736: step 2648, loss 5.54153e-05, acc 1\n",
      "2017-03-13T14:20:20.550847: step 2649, loss 0.000372535, acc 1\n",
      "2017-03-13T14:20:20.694949: step 2650, loss 0.000415572, acc 1\n",
      "2017-03-13T14:20:20.835048: step 2651, loss 0.000358557, acc 1\n",
      "2017-03-13T14:20:20.964140: step 2652, loss 0.000453894, acc 1\n",
      "2017-03-13T14:20:21.094232: step 2653, loss 0.000254924, acc 1\n",
      "2017-03-13T14:20:21.222323: step 2654, loss 0.000314976, acc 1\n",
      "2017-03-13T14:20:21.254346: step 2655, loss 0.000171129, acc 1\n",
      "2017-03-13T14:20:21.415462: step 2656, loss 0.000650037, acc 1\n",
      "2017-03-13T14:20:21.552557: step 2657, loss 0.000177162, acc 1\n",
      "2017-03-13T14:20:21.691656: step 2658, loss 0.000412537, acc 1\n",
      "2017-03-13T14:20:21.826752: step 2659, loss 0.0025246, acc 1\n",
      "2017-03-13T14:20:21.972855: step 2660, loss 0.000537156, acc 1\n",
      "2017-03-13T14:20:22.124966: step 2661, loss 0.00120925, acc 1\n",
      "2017-03-13T14:20:22.261060: step 2662, loss 0.0233652, acc 0.984375\n",
      "2017-03-13T14:20:22.394154: step 2663, loss 7.48424e-05, acc 1\n",
      "2017-03-13T14:20:22.525247: step 2664, loss 0.00196413, acc 1\n",
      "2017-03-13T14:20:22.663345: step 2665, loss 0.000419185, acc 1\n",
      "2017-03-13T14:20:22.802445: step 2666, loss 0.000448688, acc 1\n",
      "2017-03-13T14:20:22.963559: step 2667, loss 0.000141006, acc 1\n",
      "2017-03-13T14:20:23.108661: step 2668, loss 0.000411241, acc 1\n",
      "2017-03-13T14:20:23.235751: step 2669, loss 0.000691321, acc 1\n",
      "2017-03-13T14:20:23.266773: step 2670, loss 0.000957819, acc 1\n",
      "2017-03-13T14:20:23.397866: step 2671, loss 0.000165565, acc 1\n",
      "2017-03-13T14:20:23.534964: step 2672, loss 0.000499656, acc 1\n",
      "2017-03-13T14:20:23.702084: step 2673, loss 0.000204406, acc 1\n",
      "2017-03-13T14:20:23.835176: step 2674, loss 0.00191285, acc 1\n",
      "2017-03-13T14:20:23.973274: step 2675, loss 0.000582529, acc 1\n",
      "2017-03-13T14:20:24.107370: step 2676, loss 0.0211665, acc 0.984375\n",
      "2017-03-13T14:20:24.246468: step 2677, loss 0.000176891, acc 1\n",
      "2017-03-13T14:20:24.411586: step 2678, loss 0.00301301, acc 1\n",
      "2017-03-13T14:20:24.559690: step 2679, loss 0.00180051, acc 1\n",
      "2017-03-13T14:20:24.688782: step 2680, loss 0.000267912, acc 1\n",
      "2017-03-13T14:20:24.820875: step 2681, loss 0.000585527, acc 1\n",
      "2017-03-13T14:20:24.961976: step 2682, loss 0.00184461, acc 1\n",
      "2017-03-13T14:20:25.131097: step 2683, loss 0.000209653, acc 1\n",
      "2017-03-13T14:20:25.267192: step 2684, loss 0.00112244, acc 1\n",
      "2017-03-13T14:20:25.300216: step 2685, loss 0.000109143, acc 1\n",
      "2017-03-13T14:20:25.432309: step 2686, loss 0.00330087, acc 1\n",
      "2017-03-13T14:20:25.572409: step 2687, loss 0.000214221, acc 1\n",
      "2017-03-13T14:20:25.705504: step 2688, loss 0.00104401, acc 1\n",
      "2017-03-13T14:20:25.830592: step 2689, loss 7.94792e-05, acc 1\n",
      "2017-03-13T14:20:25.964687: step 2690, loss 0.0196263, acc 0.984375\n",
      "2017-03-13T14:20:26.096781: step 2691, loss 0.000658329, acc 1\n",
      "2017-03-13T14:20:26.231876: step 2692, loss 0.000123281, acc 1\n",
      "2017-03-13T14:20:26.367973: step 2693, loss 0.000124211, acc 1\n",
      "2017-03-13T14:20:26.497065: step 2694, loss 0.000225152, acc 1\n",
      "2017-03-13T14:20:26.665184: step 2695, loss 0.000456564, acc 1\n",
      "2017-03-13T14:20:26.808285: step 2696, loss 0.000180541, acc 1\n",
      "2017-03-13T14:20:26.943381: step 2697, loss 0.000330708, acc 1\n",
      "2017-03-13T14:20:27.077476: step 2698, loss 0.000170851, acc 1\n",
      "2017-03-13T14:20:27.203566: step 2699, loss 0.000221688, acc 1\n",
      "2017-03-13T14:20:27.235588: step 2700, loss 0.0147133, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:20:27.309641: step 2700, loss 0.579204, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2700\n",
      "\n",
      "2017-03-13T14:20:28.030153: step 2701, loss 0.000526802, acc 1\n",
      "2017-03-13T14:20:28.184134: step 2702, loss 0.000479553, acc 1\n",
      "2017-03-13T14:20:28.318229: step 2703, loss 0.0312511, acc 0.984375\n",
      "2017-03-13T14:20:28.478342: step 2704, loss 0.00790228, acc 1\n",
      "2017-03-13T14:20:28.710510: step 2705, loss 0.00172551, acc 1\n",
      "2017-03-13T14:20:28.872623: step 2706, loss 9.77146e-05, acc 1\n",
      "2017-03-13T14:20:29.011722: step 2707, loss 0.000704081, acc 1\n",
      "2017-03-13T14:20:29.174837: step 2708, loss 0.00285437, acc 1\n",
      "2017-03-13T14:20:29.320940: step 2709, loss 0.000590883, acc 1\n",
      "2017-03-13T14:20:29.464042: step 2710, loss 0.000129153, acc 1\n",
      "2017-03-13T14:20:29.606143: step 2711, loss 0.000207674, acc 1\n",
      "2017-03-13T14:20:29.738236: step 2712, loss 0.000250994, acc 1\n",
      "2017-03-13T14:20:29.904354: step 2713, loss 0.000332679, acc 1\n",
      "2017-03-13T14:20:30.056462: step 2714, loss 0.000334696, acc 1\n",
      "2017-03-13T14:20:30.094489: step 2715, loss 0.000110077, acc 1\n",
      "2017-03-13T14:20:30.229585: step 2716, loss 0.000553239, acc 1\n",
      "2017-03-13T14:20:30.364681: step 2717, loss 0.000165843, acc 1\n",
      "2017-03-13T14:20:30.502778: step 2718, loss 0.000440695, acc 1\n",
      "2017-03-13T14:20:30.638875: step 2719, loss 0.00124163, acc 1\n",
      "2017-03-13T14:20:30.773971: step 2720, loss 0.000206431, acc 1\n",
      "2017-03-13T14:20:30.916072: step 2721, loss 0.000476155, acc 1\n",
      "2017-03-13T14:20:31.051167: step 2722, loss 0.000222448, acc 1\n",
      "2017-03-13T14:20:31.189265: step 2723, loss 0.00534886, acc 1\n",
      "2017-03-13T14:20:31.318357: step 2724, loss 0.0169376, acc 0.984375\n",
      "2017-03-13T14:20:31.483474: step 2725, loss 0.00595209, acc 1\n",
      "2017-03-13T14:20:31.645589: step 2726, loss 0.00105311, acc 1\n",
      "2017-03-13T14:20:31.772679: step 2727, loss 0.000149805, acc 1\n",
      "2017-03-13T14:20:31.910778: step 2728, loss 0.00705846, acc 1\n",
      "2017-03-13T14:20:32.044872: step 2729, loss 0.000102133, acc 1\n",
      "2017-03-13T14:20:32.076896: step 2730, loss 0.000562668, acc 1\n",
      "2017-03-13T14:20:32.211992: step 2731, loss 0.000381792, acc 1\n",
      "2017-03-13T14:20:32.369107: step 2732, loss 0.0275456, acc 0.984375\n",
      "2017-03-13T14:20:32.518209: step 2733, loss 0.000220599, acc 1\n",
      "2017-03-13T14:20:32.647300: step 2734, loss 0.000324242, acc 1\n",
      "2017-03-13T14:20:32.789401: step 2735, loss 7.92977e-05, acc 1\n",
      "2017-03-13T14:20:32.926498: step 2736, loss 0.00139541, acc 1\n",
      "2017-03-13T14:20:33.095618: step 2737, loss 0.000357187, acc 1\n",
      "2017-03-13T14:20:33.233716: step 2738, loss 0.00202884, acc 1\n",
      "2017-03-13T14:20:33.363809: step 2739, loss 0.00260661, acc 1\n",
      "2017-03-13T14:20:33.502907: step 2740, loss 0.000166593, acc 1\n",
      "2017-03-13T14:20:33.637002: step 2741, loss 0.000697307, acc 1\n",
      "2017-03-13T14:20:33.768095: step 2742, loss 7.1876e-05, acc 1\n",
      "2017-03-13T14:20:33.908194: step 2743, loss 0.00012565, acc 1\n",
      "2017-03-13T14:20:34.041289: step 2744, loss 0.000365378, acc 1\n",
      "2017-03-13T14:20:34.073311: step 2745, loss 8.91074e-06, acc 1\n",
      "2017-03-13T14:20:34.207407: step 2746, loss 5.72461e-05, acc 1\n",
      "2017-03-13T14:20:34.341502: step 2747, loss 0.000458665, acc 1\n",
      "2017-03-13T14:20:34.477598: step 2748, loss 0.000685952, acc 1\n",
      "2017-03-13T14:20:34.641715: step 2749, loss 5.58958e-05, acc 1\n",
      "2017-03-13T14:20:34.790852: step 2750, loss 0.000625091, acc 1\n",
      "2017-03-13T14:20:34.920862: step 2751, loss 0.000228924, acc 1\n",
      "2017-03-13T14:20:35.040870: step 2752, loss 0.000137715, acc 1\n",
      "2017-03-13T14:20:35.160878: step 2753, loss 0.000684953, acc 1\n",
      "2017-03-13T14:20:35.280886: step 2754, loss 0.000341182, acc 1\n",
      "2017-03-13T14:20:35.399595: step 2755, loss 0.000707163, acc 1\n",
      "2017-03-13T14:20:35.519606: step 2756, loss 0.000563501, acc 1\n",
      "2017-03-13T14:20:35.651127: step 2757, loss 0.000177335, acc 1\n",
      "2017-03-13T14:20:35.763001: step 2758, loss 0.000675784, acc 1\n",
      "2017-03-13T14:20:35.893009: step 2759, loss 0.000117652, acc 1\n",
      "2017-03-13T14:20:35.913011: step 2760, loss 0.000112281, acc 1\n",
      "2017-03-13T14:20:36.051658: step 2761, loss 0.00025252, acc 1\n",
      "2017-03-13T14:20:36.180750: step 2762, loss 0.000628667, acc 1\n",
      "2017-03-13T14:20:36.318848: step 2763, loss 0.000155508, acc 1\n",
      "2017-03-13T14:20:36.456946: step 2764, loss 0.00325821, acc 1\n",
      "2017-03-13T14:20:36.597045: step 2765, loss 0.000605995, acc 1\n",
      "2017-03-13T14:20:36.728138: step 2766, loss 0.000258923, acc 1\n",
      "2017-03-13T14:20:36.894257: step 2767, loss 0.000854927, acc 1\n",
      "2017-03-13T14:20:37.040360: step 2768, loss 0.000314599, acc 1\n",
      "2017-03-13T14:20:37.179458: step 2769, loss 0.000194376, acc 1\n",
      "2017-03-13T14:20:37.313554: step 2770, loss 0.000165213, acc 1\n",
      "2017-03-13T14:20:37.450651: step 2771, loss 0.000216456, acc 1\n",
      "2017-03-13T14:20:37.579742: step 2772, loss 0.000269792, acc 1\n",
      "2017-03-13T14:20:37.708834: step 2773, loss 0.000100749, acc 1\n",
      "2017-03-13T14:20:37.843929: step 2774, loss 0.00145377, acc 1\n",
      "2017-03-13T14:20:37.878955: step 2775, loss 0.000407913, acc 1\n",
      "2017-03-13T14:20:38.004043: step 2776, loss 9.16615e-05, acc 1\n",
      "2017-03-13T14:20:38.135136: step 2777, loss 0.000946313, acc 1\n",
      "2017-03-13T14:20:38.280239: step 2778, loss 0.00036351, acc 1\n",
      "2017-03-13T14:20:38.405328: step 2779, loss 9.57685e-05, acc 1\n",
      "2017-03-13T14:20:38.552433: step 2780, loss 0.00116192, acc 1\n",
      "2017-03-13T14:20:38.735563: step 2781, loss 0.000731687, acc 1\n",
      "2017-03-13T14:20:38.892674: step 2782, loss 0.000247507, acc 1\n",
      "2017-03-13T14:20:39.042780: step 2783, loss 0.000477469, acc 1\n",
      "2017-03-13T14:20:39.186883: step 2784, loss 0.000973233, acc 1\n",
      "2017-03-13T14:20:39.329985: step 2785, loss 0.00154951, acc 1\n",
      "2017-03-13T14:20:39.478089: step 2786, loss 0.00106955, acc 1\n",
      "2017-03-13T14:20:39.620190: step 2787, loss 0.00152927, acc 1\n",
      "2017-03-13T14:20:39.771297: step 2788, loss 0.000655245, acc 1\n",
      "2017-03-13T14:20:39.912397: step 2789, loss 0.000201236, acc 1\n",
      "2017-03-13T14:20:39.954427: step 2790, loss 0.0369995, acc 1\n",
      "2017-03-13T14:20:40.096528: step 2791, loss 0.000539534, acc 1\n",
      "2017-03-13T14:20:40.264648: step 2792, loss 0.0278858, acc 0.984375\n",
      "2017-03-13T14:20:40.424762: step 2793, loss 6.01551e-05, acc 1\n",
      "2017-03-13T14:20:40.567862: step 2794, loss 0.00728024, acc 1\n",
      "2017-03-13T14:20:40.716968: step 2795, loss 0.00177, acc 1\n",
      "2017-03-13T14:20:40.863071: step 2796, loss 0.00154116, acc 1\n",
      "2017-03-13T14:20:41.011177: step 2797, loss 0.000136903, acc 1\n",
      "2017-03-13T14:20:41.155279: step 2798, loss 0.00145452, acc 1\n",
      "2017-03-13T14:20:41.305385: step 2799, loss 0.00100757, acc 1\n",
      "2017-03-13T14:20:41.450488: step 2800, loss 0.00074899, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:20:41.528544: step 2800, loss 0.660447, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2800\n",
      "\n",
      "2017-03-13T14:20:42.263065: step 2801, loss 0.000201307, acc 1\n",
      "2017-03-13T14:20:42.400162: step 2802, loss 0.00163981, acc 1\n",
      "2017-03-13T14:20:42.558274: step 2803, loss 0.000393905, acc 1\n",
      "2017-03-13T14:20:42.755415: step 2804, loss 8.74084e-05, acc 1\n",
      "2017-03-13T14:20:42.803449: step 2805, loss 1.85066e-05, acc 1\n",
      "2017-03-13T14:20:43.022604: step 2806, loss 0.034057, acc 0.984375\n",
      "2017-03-13T14:20:43.166706: step 2807, loss 0.000988532, acc 1\n",
      "2017-03-13T14:20:43.307806: step 2808, loss 0.000371797, acc 1\n",
      "2017-03-13T14:20:43.441901: step 2809, loss 0.0479463, acc 0.984375\n",
      "2017-03-13T14:20:43.609020: step 2810, loss 9.22257e-05, acc 1\n",
      "2017-03-13T14:20:43.748118: step 2811, loss 0.00322064, acc 1\n",
      "2017-03-13T14:20:43.881213: step 2812, loss 0.000628332, acc 1\n",
      "2017-03-13T14:20:44.016308: step 2813, loss 0.000186034, acc 1\n",
      "2017-03-13T14:20:44.156408: step 2814, loss 0.00268474, acc 1\n",
      "2017-03-13T14:20:44.285499: step 2815, loss 0.00196462, acc 1\n",
      "2017-03-13T14:20:44.454625: step 2816, loss 0.00292126, acc 1\n",
      "2017-03-13T14:20:44.587714: step 2817, loss 0.000357576, acc 1\n",
      "2017-03-13T14:20:44.724812: step 2818, loss 0.000769814, acc 1\n",
      "2017-03-13T14:20:44.857906: step 2819, loss 0.00112276, acc 1\n",
      "2017-03-13T14:20:44.889928: step 2820, loss 4.76837e-07, acc 1\n",
      "2017-03-13T14:20:45.015017: step 2821, loss 0.000593107, acc 1\n",
      "2017-03-13T14:20:45.139105: step 2822, loss 0.00220212, acc 1\n",
      "2017-03-13T14:20:45.305223: step 2823, loss 0.000942473, acc 1\n",
      "2017-03-13T14:20:45.438317: step 2824, loss 0.000382235, acc 1\n",
      "2017-03-13T14:20:45.565407: step 2825, loss 0.000132016, acc 1\n",
      "2017-03-13T14:20:45.703505: step 2826, loss 0.0016509, acc 1\n",
      "2017-03-13T14:20:45.834598: step 2827, loss 0.000844771, acc 1\n",
      "2017-03-13T14:20:45.969694: step 2828, loss 8.99937e-05, acc 1\n",
      "2017-03-13T14:20:46.130809: step 2829, loss 0.000107987, acc 1\n",
      "2017-03-13T14:20:46.278914: step 2830, loss 0.000186487, acc 1\n",
      "2017-03-13T14:20:46.409006: step 2831, loss 0.00196473, acc 1\n",
      "2017-03-13T14:20:46.545102: step 2832, loss 0.000197164, acc 1\n",
      "2017-03-13T14:20:46.681199: step 2833, loss 7.22542e-05, acc 1\n",
      "2017-03-13T14:20:46.809290: step 2834, loss 0.000511901, acc 1\n",
      "2017-03-13T14:20:46.841313: step 2835, loss 0, acc 1\n",
      "2017-03-13T14:20:46.999428: step 2836, loss 0.000347146, acc 1\n",
      "2017-03-13T14:20:47.145529: step 2837, loss 0.000228337, acc 1\n",
      "2017-03-13T14:20:47.277622: step 2838, loss 0.00719028, acc 1\n",
      "2017-03-13T14:20:47.415720: step 2839, loss 0.000883695, acc 1\n",
      "2017-03-13T14:20:47.545812: step 2840, loss 0.00200428, acc 1\n",
      "2017-03-13T14:20:47.677906: step 2841, loss 0.00223055, acc 1\n",
      "2017-03-13T14:20:47.840021: step 2842, loss 0.00109363, acc 1\n",
      "2017-03-13T14:20:47.980120: step 2843, loss 0.000419115, acc 1\n",
      "2017-03-13T14:20:48.114216: step 2844, loss 0.00160495, acc 1\n",
      "2017-03-13T14:20:48.248311: step 2845, loss 0.00029243, acc 1\n",
      "2017-03-13T14:20:48.387409: step 2846, loss 0.000167867, acc 1\n",
      "2017-03-13T14:20:48.517502: step 2847, loss 0.000364945, acc 1\n",
      "2017-03-13T14:20:48.678616: step 2848, loss 0.000452709, acc 1\n",
      "2017-03-13T14:20:48.817715: step 2849, loss 0.000127853, acc 1\n",
      "2017-03-13T14:20:48.856742: step 2850, loss 2.80141e-06, acc 1\n",
      "2017-03-13T14:20:48.980830: step 2851, loss 0.000339823, acc 1\n",
      "2017-03-13T14:20:49.110923: step 2852, loss 0.00616966, acc 1\n",
      "2017-03-13T14:20:49.243016: step 2853, loss 0.000175653, acc 1\n",
      "2017-03-13T14:20:49.380114: step 2854, loss 0.00023428, acc 1\n",
      "2017-03-13T14:20:49.529220: step 2855, loss 9.31836e-05, acc 1\n",
      "2017-03-13T14:20:49.672321: step 2856, loss 0.00036725, acc 1\n",
      "2017-03-13T14:20:49.802413: step 2857, loss 0.000953775, acc 1\n",
      "2017-03-13T14:20:49.934507: step 2858, loss 0.0010392, acc 1\n",
      "2017-03-13T14:20:50.062598: step 2859, loss 0.00118971, acc 1\n",
      "2017-03-13T14:20:50.196693: step 2860, loss 0.000153119, acc 1\n",
      "2017-03-13T14:20:50.332790: step 2861, loss 0.000832674, acc 1\n",
      "2017-03-13T14:20:50.484898: step 2862, loss 0.000118918, acc 1\n",
      "2017-03-13T14:20:50.619993: step 2863, loss 0.00022498, acc 1\n",
      "2017-03-13T14:20:50.750085: step 2864, loss 0.00042617, acc 1\n",
      "2017-03-13T14:20:50.786111: step 2865, loss 1.94009e-05, acc 1\n",
      "2017-03-13T14:20:50.912201: step 2866, loss 0.000119589, acc 1\n",
      "2017-03-13T14:20:51.038290: step 2867, loss 0.000171912, acc 1\n",
      "2017-03-13T14:20:51.177389: step 2868, loss 0.000356606, acc 1\n",
      "2017-03-13T14:20:51.338503: step 2869, loss 0.000181525, acc 1\n",
      "2017-03-13T14:20:51.482606: step 2870, loss 0.000257643, acc 1\n",
      "2017-03-13T14:20:51.614698: step 2871, loss 0.000146536, acc 1\n",
      "2017-03-13T14:20:51.747793: step 2872, loss 0.0007457, acc 1\n",
      "2017-03-13T14:20:51.876885: step 2873, loss 0.000654518, acc 1\n",
      "2017-03-13T14:20:52.007978: step 2874, loss 0.000250527, acc 1\n",
      "2017-03-13T14:20:52.171094: step 2875, loss 0.000275026, acc 1\n",
      "2017-03-13T14:20:52.321200: step 2876, loss 0.00016192, acc 1\n",
      "2017-03-13T14:20:52.459298: step 2877, loss 0.000191169, acc 1\n",
      "2017-03-13T14:20:52.590391: step 2878, loss 0.000777145, acc 1\n",
      "2017-03-13T14:20:52.730491: step 2879, loss 0.000690374, acc 1\n",
      "2017-03-13T14:20:52.762514: step 2880, loss 6.92815e-05, acc 1\n",
      "2017-03-13T14:20:52.895608: step 2881, loss 0.000564341, acc 1\n",
      "2017-03-13T14:20:53.059725: step 2882, loss 0.000732165, acc 1\n",
      "2017-03-13T14:20:53.191817: step 2883, loss 0.000966952, acc 1\n",
      "2017-03-13T14:20:53.327914: step 2884, loss 0.00038104, acc 1\n",
      "2017-03-13T14:20:53.463011: step 2885, loss 0.00125263, acc 1\n",
      "2017-03-13T14:20:53.599107: step 2886, loss 0.000146539, acc 1\n",
      "2017-03-13T14:20:53.730200: step 2887, loss 0.00127509, acc 1\n",
      "2017-03-13T14:20:53.895318: step 2888, loss 0.000106839, acc 1\n",
      "2017-03-13T14:20:54.036417: step 2889, loss 0.00121938, acc 1\n",
      "2017-03-13T14:20:54.168510: step 2890, loss 0.000510302, acc 1\n",
      "2017-03-13T14:20:54.303606: step 2891, loss 0.00142011, acc 1\n",
      "2017-03-13T14:20:54.439703: step 2892, loss 0.000784221, acc 1\n",
      "2017-03-13T14:20:54.566794: step 2893, loss 0.000684522, acc 1\n",
      "2017-03-13T14:20:54.747922: step 2894, loss 0.000204165, acc 1\n",
      "2017-03-13T14:20:54.796957: step 2895, loss 2.49433e-05, acc 1\n",
      "2017-03-13T14:20:54.931052: step 2896, loss 0.000122255, acc 1\n",
      "2017-03-13T14:20:55.061144: step 2897, loss 9.7327e-05, acc 1\n",
      "2017-03-13T14:20:55.190235: step 2898, loss 8.52057e-05, acc 1\n",
      "2017-03-13T14:20:55.319327: step 2899, loss 0.000155573, acc 1\n",
      "2017-03-13T14:20:55.459426: step 2900, loss 6.61342e-05, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:20:55.531478: step 2900, loss 0.625174, acc 0.83\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-2900\n",
      "\n",
      "2017-03-13T14:20:57.938522: step 2901, loss 7.81611e-05, acc 1\n",
      "2017-03-13T14:20:58.063091: step 2902, loss 0.00202374, acc 1\n",
      "2017-03-13T14:20:58.191181: step 2903, loss 4.06322e-05, acc 1\n",
      "2017-03-13T14:20:58.328279: step 2904, loss 0.000144807, acc 1\n",
      "2017-03-13T14:20:58.463375: step 2905, loss 0.000234919, acc 1\n",
      "2017-03-13T14:20:58.593466: step 2906, loss 0.000307135, acc 1\n",
      "2017-03-13T14:20:58.722558: step 2907, loss 0.000117072, acc 1\n",
      "2017-03-13T14:20:58.865661: step 2908, loss 2.92466e-05, acc 1\n",
      "2017-03-13T14:20:59.031779: step 2909, loss 0.000159722, acc 1\n",
      "2017-03-13T14:20:59.070805: step 2910, loss 7.38714e-05, acc 1\n",
      "2017-03-13T14:20:59.208903: step 2911, loss 0.000294781, acc 1\n",
      "2017-03-13T14:20:59.344999: step 2912, loss 0.000253282, acc 1\n",
      "2017-03-13T14:20:59.473091: step 2913, loss 0.000175215, acc 1\n",
      "2017-03-13T14:20:59.607186: step 2914, loss 0.00460482, acc 1\n",
      "2017-03-13T14:20:59.740281: step 2915, loss 0.000289534, acc 1\n",
      "2017-03-13T14:20:59.894390: step 2916, loss 0.000114644, acc 1\n",
      "2017-03-13T14:21:00.040493: step 2917, loss 3.14958e-05, acc 1\n",
      "2017-03-13T14:21:00.172587: step 2918, loss 0.000159445, acc 1\n",
      "2017-03-13T14:21:00.299677: step 2919, loss 0.000136006, acc 1\n",
      "2017-03-13T14:21:00.432772: step 2920, loss 0.000243907, acc 1\n",
      "2017-03-13T14:21:00.569869: step 2921, loss 0.000101439, acc 1\n",
      "2017-03-13T14:21:00.735988: step 2922, loss 0.000129971, acc 1\n",
      "2017-03-13T14:21:00.872083: step 2923, loss 0.000105042, acc 1\n",
      "2017-03-13T14:21:01.005178: step 2924, loss 4.04442e-05, acc 1\n",
      "2017-03-13T14:21:01.040202: step 2925, loss 2.62259e-06, acc 1\n",
      "2017-03-13T14:21:01.179301: step 2926, loss 0.000395502, acc 1\n",
      "2017-03-13T14:21:01.324404: step 2927, loss 0.000145681, acc 1\n",
      "2017-03-13T14:21:01.460501: step 2928, loss 0.000230781, acc 1\n",
      "2017-03-13T14:21:01.630623: step 2929, loss 0.000118332, acc 1\n",
      "2017-03-13T14:21:01.765717: step 2930, loss 0.000208621, acc 1\n",
      "2017-03-13T14:21:01.895809: step 2931, loss 0.000831508, acc 1\n",
      "2017-03-13T14:21:02.034908: step 2932, loss 0.000114037, acc 1\n",
      "2017-03-13T14:21:02.180011: step 2933, loss 0.000509896, acc 1\n",
      "2017-03-13T14:21:02.324113: step 2934, loss 0.000186426, acc 1\n",
      "2017-03-13T14:21:02.493234: step 2935, loss 0.000196774, acc 1\n",
      "2017-03-13T14:21:02.635334: step 2936, loss 0.000730001, acc 1\n",
      "2017-03-13T14:21:02.775433: step 2937, loss 8.50916e-05, acc 1\n",
      "2017-03-13T14:21:02.911530: step 2938, loss 0.000615225, acc 1\n",
      "2017-03-13T14:21:03.047626: step 2939, loss 0.000612394, acc 1\n",
      "2017-03-13T14:21:03.080650: step 2940, loss 1.78814e-07, acc 1\n",
      "2017-03-13T14:21:03.204738: step 2941, loss 0.000227508, acc 1\n",
      "2017-03-13T14:21:03.370857: step 2942, loss 0.00067978, acc 1\n",
      "2017-03-13T14:21:03.501949: step 2943, loss 0.000236426, acc 1\n",
      "2017-03-13T14:21:03.638045: step 2944, loss 0.000903328, acc 1\n",
      "2017-03-13T14:21:03.767137: step 2945, loss 0.000157477, acc 1\n",
      "2017-03-13T14:21:03.899230: step 2946, loss 8.4043e-05, acc 1\n",
      "2017-03-13T14:21:04.032325: step 2947, loss 0.00029604, acc 1\n",
      "2017-03-13T14:21:04.183433: step 2948, loss 0.00079507, acc 1\n",
      "2017-03-13T14:21:04.328535: step 2949, loss 0.00084214, acc 1\n",
      "2017-03-13T14:21:04.459628: step 2950, loss 0.000213092, acc 1\n",
      "2017-03-13T14:21:04.602729: step 2951, loss 0.000274313, acc 1\n",
      "2017-03-13T14:21:04.743830: step 2952, loss 7.5548e-05, acc 1\n",
      "2017-03-13T14:21:04.871920: step 2953, loss 0.000196305, acc 1\n",
      "2017-03-13T14:21:05.025029: step 2954, loss 0.000143003, acc 1\n",
      "2017-03-13T14:21:05.072064: step 2955, loss 6.85453e-07, acc 1\n",
      "2017-03-13T14:21:05.199153: step 2956, loss 0.00149306, acc 1\n",
      "2017-03-13T14:21:05.330245: step 2957, loss 0.000570446, acc 1\n",
      "2017-03-13T14:21:05.460339: step 2958, loss 0.000118566, acc 1\n",
      "2017-03-13T14:21:05.599436: step 2959, loss 6.59544e-05, acc 1\n",
      "2017-03-13T14:21:05.738535: step 2960, loss 0.000106826, acc 1\n",
      "2017-03-13T14:21:05.862623: step 2961, loss 0.000369145, acc 1\n",
      "2017-03-13T14:21:06.024739: step 2962, loss 0.000102383, acc 1\n",
      "2017-03-13T14:21:06.167840: step 2963, loss 0.000101995, acc 1\n",
      "2017-03-13T14:21:06.293929: step 2964, loss 7.85527e-05, acc 1\n",
      "2017-03-13T14:21:06.423021: step 2965, loss 0.000232073, acc 1\n",
      "2017-03-13T14:21:06.555114: step 2966, loss 0.000388633, acc 1\n",
      "2017-03-13T14:21:06.688209: step 2967, loss 0.000328508, acc 1\n",
      "2017-03-13T14:21:06.841318: step 2968, loss 0.00022672, acc 1\n",
      "2017-03-13T14:21:06.989422: step 2969, loss 0.000108799, acc 1\n",
      "2017-03-13T14:21:07.023447: step 2970, loss 9.49964e-05, acc 1\n",
      "2017-03-13T14:21:07.148535: step 2971, loss 0.000456633, acc 1\n",
      "2017-03-13T14:21:07.285633: step 2972, loss 0.000158496, acc 1\n",
      "2017-03-13T14:21:07.423730: step 2973, loss 0.000636752, acc 1\n",
      "2017-03-13T14:21:07.557826: step 2974, loss 9.11262e-05, acc 1\n",
      "2017-03-13T14:21:07.698926: step 2975, loss 0.000183268, acc 1\n",
      "2017-03-13T14:21:07.853040: step 2976, loss 0.000549841, acc 1\n",
      "2017-03-13T14:21:07.990132: step 2977, loss 0.00222323, acc 1\n",
      "2017-03-13T14:21:08.126229: step 2978, loss 0.000236046, acc 1\n",
      "2017-03-13T14:21:08.263326: step 2979, loss 0.000210447, acc 1\n",
      "2017-03-13T14:21:08.388416: step 2980, loss 0.00131534, acc 1\n",
      "2017-03-13T14:21:08.520509: step 2981, loss 0.00159681, acc 1\n",
      "2017-03-13T14:21:08.686627: step 2982, loss 0.00122351, acc 1\n",
      "2017-03-13T14:21:08.828728: step 2983, loss 0.000218997, acc 1\n",
      "2017-03-13T14:21:08.960821: step 2984, loss 0.000559718, acc 1\n",
      "2017-03-13T14:21:08.996847: step 2985, loss 1.19209e-07, acc 1\n",
      "2017-03-13T14:21:09.129941: step 2986, loss 0.00439308, acc 1\n",
      "2017-03-13T14:21:09.260033: step 2987, loss 0.000518353, acc 1\n",
      "2017-03-13T14:21:09.398132: step 2988, loss 0.000100761, acc 1\n",
      "2017-03-13T14:21:09.551241: step 2989, loss 0.000250711, acc 1\n",
      "2017-03-13T14:21:09.695342: step 2990, loss 0.000100925, acc 1\n",
      "2017-03-13T14:21:09.834441: step 2991, loss 0.000136535, acc 1\n",
      "2017-03-13T14:21:09.963533: step 2992, loss 0.000105994, acc 1\n",
      "2017-03-13T14:21:10.100630: step 2993, loss 0.00017984, acc 1\n",
      "2017-03-13T14:21:10.228720: step 2994, loss 6.5538e-05, acc 1\n",
      "2017-03-13T14:21:10.392838: step 2995, loss 5.97613e-05, acc 1\n",
      "2017-03-13T14:21:10.533937: step 2996, loss 0.000465919, acc 1\n",
      "2017-03-13T14:21:10.661027: step 2997, loss 4.95946e-05, acc 1\n",
      "2017-03-13T14:21:10.791119: step 2998, loss 8.32521e-05, acc 1\n",
      "2017-03-13T14:21:10.931219: step 2999, loss 0.000231147, acc 1\n",
      "2017-03-13T14:21:10.962241: step 3000, loss 2.68221e-07, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2017-03-13T14:21:11.033292: step 3000, loss 0.615194, acc 0.82\n",
      "\n",
      "Saved model checkpoint to C:\\Users\\Gavin\\Desktop\\Side_Projects\\Stats_Project\\Learning\\runs\\1489439596\\checkpoints\\model-3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "'''\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", .1, \"Percentage of the training data to use for validation\")\n",
    "#tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "#tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the negative data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.0, \"L2 regularization lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 200, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"num_checkpoints\", 5, \"Number of checkpoints to store (default: 5)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "'''\n",
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Data Preparation\n",
    "# ==================================================\n",
    "\n",
    "# Load data\n",
    "print(\"Loading data...\")\n",
    "x_text, y = load_data_and_labels(train_pos, train_neg)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "\n",
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "\n",
    "# Training\n",
    "# ==================================================\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "      log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=y_train.shape[1],\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=FLAGS.embedding_dim,\n",
    "            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "            num_filters=FLAGS.num_filters,\n",
    "            l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.summary.histogram(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.summary.scalar(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.summary.merge(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=FLAGS.num_checkpoints)\n",
    "\n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "\n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "\n",
    "        # Generate batches\n",
    "        batches = batch_iter(\n",
    "            list(zip(x_train, y_train)), FLAGS.batch_size, FLAGS.num_epochs)\n",
    "        # Training loop. For each batch...\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
