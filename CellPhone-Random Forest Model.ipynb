{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All credits to https://github.com/jorpro/workshops/blob/patch-1/NLP/NLP_Demo.ipynb\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Parse data downloaded from http://jmcauley.ucsd.edu/data/amazon/\n",
    "#import 'reviews_Cell_Phones_and_Accessories_5.json.gz'\n",
    "\n",
    "import pandas as pd \n",
    "import gzip \n",
    "\n",
    "def parse(path): \n",
    "    g = gzip.open(path, 'rb') \n",
    "    for l in g: \n",
    "        yield eval(l) \n",
    "        \n",
    "def getDF(path): \n",
    "    i = 0 \n",
    "    df = {}\n",
    "    for d in parse(path): \n",
    "        df[i] = d \n",
    "        i += 1 \n",
    "    return pd.DataFrame.from_dict(df, orient='index') \n",
    "\n",
    "df = getDF('reviews_Cell_Phones_and_Accessories_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They look good and stick good! I just don't li...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These stickers work like the review says they ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>These are awesome and make my phone look so st...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Item arrived in great time and was in perfect ...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>awesome! stays on, and looks great. can be use...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  overall\n",
       "0  They look good and stick good! I just don't li...      4.0\n",
       "1  These stickers work like the review says they ...      5.0\n",
       "2  These are awesome and make my phone look so st...      5.0\n",
       "3  Item arrived in great time and was in perfect ...      4.0\n",
       "4  awesome! stays on, and looks great. can be use...      5.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select only review texts and ratings\n",
    "df = df[['reviewText','overall']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set reviews of rating 1 as negative, 5 as positive.\n",
    "from sklearn.utils import shuffle\n",
    "df = df[(df['overall'] == 5.0) | (df['overall'] == 1.0)]\n",
    "df['sentiment'] = 0 + (df['overall'] > 3.0)\n",
    "negative_reviews = df[df['sentiment'] == 0]\n",
    "positive_reviews = df[df['sentiment'] == 1]\n",
    "positive_reviews = shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[0:negative_reviews.shape[0]]\n",
    "reviews = [positive_reviews,negative_reviews]\n",
    "df = pd.concat(reviews)\n",
    "df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26558, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Default English stopwords (Credit: http://www.ranks.nl/stopwords)\n",
    "#Words with negative implications are removed from the list i.e. 'against','mustn','needn','shan','shouldn','wasn','weren','won','wouldn'.\n",
    "stop_words = {'i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he',\n",
    "              'him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs',\n",
    "              'themselves','what','which','who','whom','this','that', 'these','those','am','is','are','was', 'were',\n",
    "              'be','been','being', 'has','had','having','do','does','did', 'doing','a','an','the','and','but',\n",
    "              'if','or','because','as','until','while','of','at','by','for','with','about','between','into','through',\n",
    "              'during','before','after','to','from','in','on', 'again','further','then','once','here','there','when',\n",
    "              'where','why', 'how','all','any','both','each','other','some','such','own','same','too','s','t','can',\n",
    "              'will','just','now','d','ll','m','o','re','ve','y','ma'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Credit: http://www.nltk.org/howto/stem.html\n",
    "#import snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\",ignore_stopwords=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Clean reviews\n",
    "#Map numbers to generic \"NUMBER\"\n",
    "#Punctuations (including ,.?1) are replaced by \"SYMBOL\"\n",
    "def clean_str(string):\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\w*[0-9]+\",\" NUMBER \",string)\n",
    "#Replace Exclamation and Question marks as symbols \n",
    "    string = re.sub(r\"[?!]+\", \" SYMBOL \", string)\n",
    "    string = re.sub(r\"[,.]\",\" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \", string)\n",
    "    string = re.sub(r\"\\'m\", \" \", string)\n",
    "    string = re.sub(r\"\\'re\", \" \", string)\n",
    "    string = re.sub(r\"\\'d\", \" \", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \", string)\n",
    "    string = re.sub(r\"\\(\", \" \", string)\n",
    "    string = re.sub(r\"\\)\", \" \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    words = string.lower().split()\n",
    "    meaningful_words = [stemmer.stem(w) for w in words if w not in stop_words]\n",
    "    return ( \" \".join(meaningful_words)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Clean each review\n",
    "#Append each review (in string) to an array\n",
    "clean_train_reviews = []\n",
    "\n",
    "for string in df['reviewText']:\n",
    "    clean_train_reviews.append(clean_str(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \n",
    "                                 tokenizer = None,  \n",
    "                                 preprocessor = None, \n",
    "                                 stop_words = None,   \n",
    "                                 max_features = 5000)\n",
    "\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
    "\n",
    "# Numpy arrays are easy to work with, so convert the result to an array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26558, 5000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) \n",
    "\n",
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "\n",
    "forest = forest.fit(train_data_features[0:23900,:], df[\"sentiment\"][0:23900] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Validation accuracy \n",
    "valid_predictions = forest.predict(train_data_features[23900:,:])\n",
    "train_predictions = forest.predict(train_data_features[0:23900,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2658"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_predictions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23900"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_predictions.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training accuracy is:  0.999581589958 \n",
      " The validation accuracy is:  0.890895410083\n"
     ]
    }
   ],
   "source": [
    "train_acc = np.mean(train_predictions == df[\"sentiment\"][0:23900])\n",
    "valid_acc = np.mean(valid_predictions == df[\"sentiment\"][23900:])\n",
    "print(\"The training accuracy is: \", train_acc, \"\\n\", \"The validation accuracy is: \", valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50263355906696761"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(valid_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49937238493723851"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(train_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>So there is no way for me to plug it in here i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Good case, Excellent value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Great for the jawbone.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tied to charger for conversations lasting more...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The mic is great.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                               text\n",
       "0       0  So there is no way for me to plug it in here i...\n",
       "1       1                        Good case, Excellent value.\n",
       "2       1                             Great for the jawbone.\n",
       "3       0  Tied to charger for conversations lasting more...\n",
       "4       1                                  The mic is great."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use \"amazon_cells_labelled.txt\" as test set.\n",
    "text_file = open(\"amazon_cells_labelled.txt\", 'r')\n",
    "text_file = text_file.read()\n",
    "text_file = re.split(\"\\t|\\n\", text_file)\n",
    "text_file = text_file[:len(text_file)-1]\n",
    "test = pd.DataFrame({\"text\":text_file[:len(text_file):2], \"rating\":[int(x) for x in text_file[1:len(text_file):2]]})\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00780324,  0.00860791,  0.01048021,  0.01154504,  0.01214058,\n",
       "        0.01445949,  0.01792511,  0.03024891,  0.03376804,  0.03713392])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = forest.feature_importances_\n",
    "indices = np.argsort(importances)[-10:]\n",
    "importances[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Credit: http://www.agcross.com/2015/02/random-forests-in-python-with-scikit-learn/\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), np.asarray(vocab)[indices])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_test_reviews = [] \n",
    "\n",
    "for string in test['text']:\n",
    "    clean_test_reviews.append(clean_str(string))\n",
    "\n",
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()\n",
    "\n",
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame(data={\"text\":test[\"text\"],\"prediction\":result ,\"truth\":test['rating']})\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_accuracy = np.mean(result == test['rating'])\n",
    "print(\"The test_accuracy is: \", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wrong_predictions = output[output['prediction'] != output['truth']]\n",
    "wrong_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Percentage of negative review identified as negative\n",
    "sensitivity = (output[(output['prediction'] == 0) & (output['truth'] == 0)].shape[0]) / (output[output['truth'] == 0].shape[0])\n",
    "# Percentage of positive review identified as positive\n",
    "specifity = output[(output['prediction'] == 1) & (output['truth'] == 1)].shape[0] / output[output['truth'] == 0].shape[0]\n",
    "print(\"The sensitivity is: \", sensitivity, \"\\n\", \"The specifity is: \", specifity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
